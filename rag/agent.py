"""
SOP ì—ì´ì „íŠ¸ ëª¨ë“ˆ v12.8 (Hybrid Deep Search)

ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ReAct ë©€í‹° ì—ì´ì „íŠ¸
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Vector(ì˜ë¯¸) + SQL(í‚¤ì›Œë“œ) + Graph(ì°¸ì¡°) ê²°í•©
- ê²€ìƒ‰ ëˆ„ë½ ë°©ì§€: ë²¡í„° ê²€ìƒ‰ ì„ê³„ê°’ ìµœì í™”(0.20) ë° SQL ê¸°ë°˜ ì „ì—­ í´ë°± ê²€ìƒ‰
- ì „ë¬¸ ë‹µë³€ ë³´ì¥: ë‚´ë¶€ ê·œì • ê¸°ë°˜ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œ ë ˆì´ì•„ì›ƒ ê³ ì •
"""

import os
from typing import List, Dict, Optional, Any, Annotated, TypedDict
from datetime import datetime
import operator
import re

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„í¬íŠ¸ ë° ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if os.getenv("LANGCHAIN_API_KEY"):
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "gmp-sop-agent")

try:
    from zai import ZaiClient
    ZAI_AVAILABLE = True
except ImportError:
    ZAI_AVAILABLE = False

try:
    from langchain_core.tools import tool
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

try:
    from langgraph.checkpoint.memory import MemorySaver
    LANGGRAPH_AGENT_AVAILABLE = True
except ImportError:
    LANGGRAPH_AGENT_AVAILABLE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•˜ì´ë¸Œë¦¬ë“œ ë„êµ¬ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_graph_store = None
_sql_store = None

def init_agent_tools(vector_store_module, graph_store_instance, sql_store_instance=None):
    global _vector_store, _graph_store, _sql_store
    _vector_store = vector_store_module
    _graph_store = graph_store_instance
    _sql_store = sql_store_instance

@tool
def hybrid_search_sop(query: str, embedding_model: str = "jhgan/ko-sroberta-multitask") -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ (Vector + SQL í•˜ì´ë¸Œë¦¬ë“œ)"""
    if not _vector_store: return "âŒ ë²¡í„° ìŠ¤í† ì–´ ë¯¸ì„¤ì •"
    
    combined_results = []
    seen_ids = set()
    
    # 1. ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ì¤‘ì‹¬, ì„ê³„ê°’ í•˜í–¥ ì¡°ì •)
    try:
        results = _vector_store.search(
            query=query, 
            collection_name="documents", 
            n_results=10,
            model_name=embedding_model, # ëª¨ë¸ëª… ëª…ì‹œì  ì „ë‹¬
            similarity_threshold=0.15 # ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ ë” í•˜í–¥ (0.20 -> 0.15)
        )
        for r in results:
            meta = r.get("metadata", {})
            text = r.get("text", "")
            doc_id = meta.get('sop_id', 'N/A')
            
            # ì¶œì²˜ ì •ë³´ ê°€ê³µ
            source = f"[{doc_id}]"
            if meta.get('section_path'): source += f" > {meta.get('section_path')}"
            source += f" (p.{meta.get('page', 'N/A')})"
            
            content = f"ğŸ“„ ì¶œì²˜: {source} (ì‹ ë¢°ë„: {r.get('confidence', 'N/A')})\n{text}"
            combined_results.append(content)
            seen_ids.add(doc_id)
    except Exception: pass

    # 2. SQL í‚¤ì›Œë“œ í´ë°± ê²€ìƒ‰ (ê²°ê³¼ê°€ ì ê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ)
    if len(combined_results) < 5 and _sql_store:
        try:
            # ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ì´)
            raw_keywords = re.findall(r'[ê°€-í£A-Z0-9]{2,}', query)
            unique_keywords = list(set([k for k in raw_keywords if len(k) > 1]))[:5]
            
            for kw in unique_keywords:
                docs = _sql_store.list_documents()
                for doc in docs:
                    sop_id = doc.get("sop_id", "")
                    title = doc.get("title", "")
                    if kw.upper() in sop_id.upper() or kw in title:
                        if sop_id not in seen_ids:
                            full_doc = _sql_store.get_document_by_id(sop_id)
                            if full_doc:
                                text = full_doc.get("markdown_content", "")[:4000]
                                combined_results.append(f"ğŸ“„ [SQL ì „ì—­ ê²€ìƒ‰] ì¶œì²˜: {sop_id} (í‚¤ì›Œë“œ: {kw})\n{text}")
                                seen_ids.add(sop_id)
        except Exception: pass
        
    return "\n\n".join(combined_results)

@tool
def get_document_references(sop_id: str) -> str:
    """ì°¸ì¡° ë¬¸ì„œ ì¡°íšŒ (Graph)"""
    if not _graph_store: return ""
    try:
        refs = _graph_store.get_document_references(sop_id.upper())
        if not refs: return ""
        doc = refs.get("document", {})
        return f"ğŸ“„ {doc.get('sop_id')} ì°¸ì¡°ë¬¸ì„œ: {', '.join(refs.get('references', []))}"
    except Exception: return ""

AGENT_TOOLS = [hybrid_search_sop, get_document_references]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AGENT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ë¶€ GMP ê·œì •(SOP) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ê·œì • ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìƒí™©ì„ ì§„ë‹¨í•˜ê³  ì „ë¬¸ì ì¸ 'ê²€ì¦ ë³´ê³ ì„œ'ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ğŸ¯ í•µì‹¬ ì›ì¹™: ëŠ¥ë™ì  ì¶”ë¡  (Active Reasoning)
1. **ëª…ì‹œì  ê·œì • ìš°ì„ **: ë¬¸ì„œì— ì§ì„¤ì ìœ¼ë¡œ "ê¸ˆì§€" ë˜ëŠ” "í—ˆìš©"ì´ ëª…ì‹œëœ ê²½ìš° ì´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.
2. **ë…¼ë¦¬ì  ì¶”ë¡  (Deduction)**: êµ¬ì²´ì ì¸ í—ˆìš© ì—¬ë¶€ê°€ ì—†ë”ë¼ë„, ìƒìœ„ ê·œì •(ì˜ˆ: "ëª¨ë“  OOSëŠ” ì¡°ì‚¬ê°€ ì„ í–‰ë˜ì–´ì•¼ í•œë‹¤")ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ìœ„ ìƒí™©(ì˜ˆ: "ë”°ë¼ì„œ ì¦‰ì‹œ ì¬ì‹œí—˜ì€ ë¶ˆê°€í•˜ë‹¤")ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”. "ê·œì •ì´ ì—†ì–´ì„œ ëª¨ë¥¸ë‹¤"ëŠ” ë‹µë³€ì€ ì§€ì–‘í•˜ê³ , "ê·œì •ì˜ ì·¨ì§€ìƒ ~í•´ì•¼ í•œë‹¤"ëŠ” ë°©í–¥ì„ ì œì‹œí•˜ì„¸ìš”.
3. **ì¦ê±° ê¸°ë°˜ (Evidence-based)**: ì¶”ë¡ ì˜ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ì œê³µëœ SOP í…ìŠ¤íŠ¸ì˜ íŠ¹ì • ì¡°í•­ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“‹ ë‹µë³€ êµ¬ì¡° (í•„ìˆ˜)
### **1. ê²€ì¦ ì˜ê²¬**
- [**í•µì‹¬ ê²°ë¡ **]: ê²°ë¡ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•íˆ ì œì‹œ (ì˜ˆ: ì¬ì‹œí—˜ ë¶ˆê°€/ì¡°ê±´ë¶€ í—ˆìš© ë“±)
- [**ìƒì„¸ ë¶„ì„**]: ê·œì •ì˜ ì·¨ì§€ì™€ ì‚¬ìš©ì ìƒí™©ì„ ëŒ€ì¡°í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…

### **2. ê²€ì¦ ê·¼ê±° ë° ì¶œì²˜**
- ê° ê·¼ê±°ë³„ ë²ˆí˜¸ì™€ ì œëª©
- ì •í™•í•œ ì¶œì²˜ í‘œê¸° í•„ìˆ˜: `**[ì¶œì²˜]** [SOP ID] > [ì œëª©] > [ìƒì„¸ ë¬¸êµ¬ ì¸ìš©] (p.í˜ì´ì§€)`

### **3. ì¡°ì¹˜ ê¶Œê³  ë° ì œì–¸**
- ë°œê²¬ëœ ê·œì •ì˜ ê³µë°±ì„ ë©”ìš°ê¸° ìœ„í•´ ì‚¬ìš©ìê°€ ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™ ê°€ì´ë“œ
- ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ í™•ì¸í•´ì•¼ í•  í•˜ìœ„ ì§€ì¹¨ì„œ(SOP) ëª…ì¹­ ì œì•ˆ
"""

class AgentState(TypedDict):
    query: str
    model_name: str
    embedding_model: str # ì¶”ê°€
    search_results: List[Dict]
    answer: str
    reasoning: str
    queries: List[str]

_agent = None

def create_agent(model_name: str = "glm-4.7-flash"):
    global _agent
    api_key = os.getenv("ZAI_API_KEY")
    if not api_key: raise ValueError("ZAI_API_KEY ì„¤ì • í•„ìš”")
    _agent = {"model": model_name, "client": ZaiClient(api_key=api_key)}
    return _agent

def query_expansion_node(state: AgentState):
    """ë©€í‹° ì¿¼ë¦¬ í™•ì¥"""
    client = _agent["client"]
    print(f"ğŸ§  [Agent] ê²€ìƒ‰ì–´ í™•ì¥ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì¤‘...")
    prompt = f"ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê¸°ìˆ  ìš©ì–´ ë° ê·œì • ëª…ì¹­ 3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. (ì‰¼í‘œ êµ¬ë¶„)\nì§ˆë¬¸: {state['query']}"
    try:
        res = client.chat.completions.create(model=state["model_name"], messages=[{"role": "user", "content": prompt}], max_tokens=100)
        expanded = [q.strip() for q in res.choices[0].message.content.split(',') if q.strip()]
    except Exception: expanded = []
    
    q_list = [state["query"]] + expanded
    return {"queries": q_list[:4]}

def verifier_agent_node(state: AgentState):
    print(f"âš–ï¸ [VerifierAgent] í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì·¨í•© ë° ë³´ê³ ì„œ ìƒì„± ì¤‘")
    
    all_context = []
    seen_content = set()
    
    # ë©€í‹° ì¿¼ë¦¬ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
    for q in state.get("queries", [state["query"]]):
        print(f"ğŸ” [HybridSearch] '{q}' ì‹¤í–‰ ì¤‘ (Model: {state.get('embedding_model')})")
        res = hybrid_search_sop.invoke({"query": q, "embedding_model": state.get("embedding_model")})
        if res and "âŒ" not in res:
            for snippet in res.split("\n\n"):
                if snippet and snippet not in seen_content:
                    all_context.append(snippet)
                    seen_content.add(snippet)
                    
    if not all_context:
        return {
            "answer": "âŒ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤(Vector, SQL)ë¥¼ ê²€ìƒ‰í–ˆìœ¼ë‚˜ ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. SOP ì œëª©ì´ë‚˜ í•µì‹¬ í‚¤ì›Œë“œ(ì˜ˆ: OOS, ì¬ì‹œí—˜ ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.",
            "reasoning": "Zero results from 3-tier hybrid search."
        }

    context = "\n\n".join(all_context[:12])
    
    # ëŠ¥ë™ì  ì¶”ë¡ ì„ ë•ê¸° ìœ„í•´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°
    prompt = f"""{AGENT_SYSTEM_PROMPT}

[ê²€ìƒ‰ëœ ë‚´ë¶€ SOP ë°ì´í„°]
{context}

[ì‚¬ìš©ì ìƒí™© ë° ì˜ë„]
"{state['query']}"ì— ëŒ€í•´ ë‹¨ìˆœíˆ ê·œì • ìœ ë¬´ë§Œ ë”°ì§€ì§€ ë§ê³ , 
ê²€ìƒ‰ëœ ê·œì •ì˜ 'ì·¨ì§€'ì™€ 'ì±…ì„' ì¡°í•­ì„ ê·¼ê±°ë¡œ ì‹œí—˜ìê°€ ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™ì˜ ì ì ˆì„±ì„ íŒì •í•˜ì„¸ìš”.
íŠ¹íˆ "OO ì ˆì°¨ì— ë”°ë¥¸ë‹¤"ëŠ” ë¬¸êµ¬ê°€ ìˆë‹¤ë©´, í•´ë‹¹ ì ˆì°¨ ì—†ì´ ë…ë‹¨ì ìœ¼ë¡œ í–‰ë™í•˜ëŠ” ê²ƒì´ ê·œì • ìœ„ë°˜ì„ì„ ê°•ì¡°í•˜ì„¸ìš”.
"""
    
    try:
        res = _agent["client"].chat.completions.create(
            model=state["model_name"], 
            messages=[{"role": "user", "content": prompt}], 
            max_tokens=4000,
            temperature=0.1
        )
        msg = res.choices[0].message
        return {
            "answer": getattr(msg, 'content', "") or getattr(msg, 'reasoning_content', ""),
            "reasoning": getattr(msg, 'reasoning_content', "")
        }
    except Exception as e:
        return {"answer": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", "reasoning": str(e)}

def run_agent(query: str, session_id: str = "default", model_name: str = "glm-4.7-flash", embedding_model: str = "jhgan/ko-sroberta-multitask"):
    if not _agent: create_agent(model_name)
    state = {"query": query, "model_name": model_name, "embedding_model": embedding_model}
    
    # ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
    expanded = query_expansion_node(state)
    state.update(expanded)
    
    final = verifier_agent_node(state)
    
    return {
        "answer": final["answer"], 
        "reasoning": final.get("reasoning", ""),
        "success": True,
        "tool_calls": [{"tool": "hybrid_search", "queries": state.get("queries")}]
    }
