"""
SOP ì—ì´ì „íŠ¸ ëª¨ë“ˆ v12.8 (Hybrid Deep Search)

ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ReAct ë©€í‹° ì—ì´ì „íŠ¸
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Vector(ì˜ë¯¸) + SQL(í‚¤ì›Œë“œ) + Graph(ì°¸ì¡°) ê²°í•©
- ê²€ìƒ‰ ëˆ„ë½ ë°©ì§€: ë²¡í„° ê²€ìƒ‰ ì„ê³„ê°’ ìµœì í™”(0.20) ë° SQL ê¸°ë°˜ ì „ì—­ í´ë°± ê²€ìƒ‰
- ì „ë¬¸ ë‹µë³€ ë³´ì¥: ë‚´ë¶€ ê·œì • ê¸°ë°˜ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œ ë ˆì´ì•„ì›ƒ ê³ ì •
"""

import os
from typing import List, Dict, Optional, Any, Annotated, TypedDict
from datetime import datetime
import operator
import re
import json

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„í¬íŠ¸ ë° ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if os.getenv("LANGCHAIN_API_KEY"):
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "gmp-sop-agent")
    # ğŸ”¥ LangSmithì—ì„œ ìµœìƒìœ„ í”„ë¡œì íŠ¸ ë° ëŸ° ë„¤ì„ ê°•ì œ ì§€ì •
    from langchain_core.tracers.context import tracing_v2_enabled

try:
    from zai import ZaiClient
    ZAI_AVAILABLE = True
except ImportError:
    ZAI_AVAILABLE = False

try:
    from langchain_core.tools import tool
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

try:
    from langgraph.graph import StateGraph, START, END
    LANGGRAPH_AGENT_AVAILABLE = True
except ImportError:
    LANGGRAPH_AGENT_AVAILABLE = False

# ğŸ”¥ LangSmith ê°€ì‹œì„± ê°•í™”ë¥¼ ìœ„í•œ ì¶”ì ê¸° ì„í¬íŠ¸
try:
    from langsmith import traceable
except ImportError:
    import functools
    def traceable(name=None, run_type=None, **kwargs):
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs): return func(*args, **kwargs)
            return wrapper
        return decorator

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•˜ì´ë¸Œë¦¬ë“œ ë„êµ¬ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_graph_store = None
_sql_store = None

def init_agent_tools(vector_store_module, graph_store_instance, sql_store_instance=None):
    global _vector_store, _graph_store, _sql_store
    _vector_store = vector_store_module
    _graph_store = graph_store_instance
    _sql_store = sql_store_instance

@tool
def hybrid_search_sop(query: str, keywords: List[str] = None, embedding_model: str = "intfloat/multilingual-e5-small") -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ (Vector + SQL í•˜ì´ë¸Œë¦¬ë“œ)
    keywords: SQL ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ë‹¨ì–´ ëª©ë¡ (LLMì´ ë¯¸ë¦¬ ì •ì œí•œ ê°’)
    """
    global _sql_store
    if not _vector_store: return "âŒ ë²¡í„° ìŠ¤í† ì–´ ë¯¸ì„¤ì •"
    
    combined_results = []
    seen_ids = set()
    
    # 1. ì§€ëŠ¥í˜• í‚¤ì›Œë“œ ë° ìˆ«ì íŒŒí¸ ì¶”ì¶œ (ê°•í™”ëœ Regex)
    search_terms = keywords or []
    # ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œì—ì„œ ëª¨ë“  ìˆ«ì ë©ì–´ë¦¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ì ì¬ì  IDë¡œ ì·¨ê¸‰
    all_numbers = re.findall(r'\d+', f"{query} {' '.join(search_terms)}")
    search_terms.extend(all_numbers)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”
    unique_terms = list(set([str(k).upper().strip() for k in search_terms if k]))
    is_summary_request = any(word in query for word in ["ìš”ì•½", "ì •ë¦¬", "summary", "ì „ì²´", "ë¦¬ë·°", "ë³¸ë¬¸"])

    print(f"ğŸ“¡ [HybridSearch] SQL ì €ì¥ì†Œ ìƒíƒœ: {'Connected' if _sql_store else 'Disconnected'}")
    print(f"ğŸ“¡ [HybridSearch] ì¶”ì¶œëœ ì§€ëŠ¥í˜• í‚¤ì›Œë“œ: {unique_terms}")

    # [ìš°ì„ ìˆœìœ„ 1] SQL ì €ì¥ì†Œ ì „ìˆ˜ ì¡°ì‚¬ (ì „ì²´ ë³¸ë¬¸ ë° ê³ ì • ID ë§¤ì¹­)
    if _sql_store:
        try:
            all_docs = _sql_store.list_documents()
            for doc in all_docs:
                sop_id = doc.get("sop_id", "")
                if not sop_id: continue
                
                match_found = False
                # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ IDì˜ ì¼ë¶€ì´ê±°ë‚˜, IDê°€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                for kw in unique_terms:
                    if kw in sop_id.upper() or sop_id.upper() in kw:
                        match_found = True
                        break
                
                if match_found and sop_id not in seen_ids:
                    full_doc = _sql_store.get_document_by_id(sop_id)
                    if full_doc:
                        content = full_doc.get("markdown_content", "")
                        if content and len(content.strip()) > 50:
                            # ìš”ì•½ ì‹œì—ëŠ” ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ (ìµœëŒ€ 15000ì) ì œê³µ
                            limit = 15000 if is_summary_request else 4000
                            combined_results.append(f"ğŸ“„ [ì „ì—­ ê²€ìƒ‰/ì›ë³¸ ë³¸ë¬¸] ì¶œì²˜: {sop_id}\n{content[:limit]}")
                            seen_ids.add(sop_id)
                            print(f"âœ… [HybridSearch] SQL ë§¤ì¹­ ì„±ê³µ: {sop_id}")
        except Exception as e:
            print(f"âš ï¸ [SQL Search Error] {e}")

    # [ìš°ì„ ìˆœìœ„ 2] ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ì¤‘ì‹¬ íŒŒí¸ ì°¾ê¸°)
    # ìš”ì•½ ìš”ì²­ ì‹œ ê²€ìƒ‰ëœ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ íŒŒí¸ ì •ë³´ëŠ” ìƒëµí•˜ì—¬ í† í° ì ˆì•½
    if not is_summary_request or len(combined_results) == 0:
        try:
            results = _vector_store.search(
                query=query, 
                collection_name="documents", 
                n_results=15,
                model_name=embedding_model,
                similarity_threshold=0.12 # ë²¡í„° ê²€ìƒ‰ ì„ê³„ê°’ ëŒ€í­ ì™„í™”
            )
            for r in results:
                meta = r.get("metadata", {})
                text = r.get("text", "")
                doc_id = meta.get('sop_id', 'N/A')
                
                # ì´ë¯¸ ì „ì²´ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¨ ë¬¸ì„œì˜ íŒŒí¸ì€ ìŠ¤í‚µ
                if doc_id in seen_ids: continue
                
                source = f"[{doc_id}] > {meta.get('section_path', '')} (p.{meta.get('page', 'N/A')})"
                combined_results.append(f"ğŸ“„ ì¶œì²˜: {source}\n{text}")
        except Exception: pass

    # ë§Œì•½ ì•„ë¬´ê²ƒë„ ëª» ì°¾ì•˜ë‹¤ë©´, ëª¨ë“  ë¬¸ì„œ ìš”ì•½ ì‹œë„ ì˜ˆì™¸ ì²˜ë¦¬
    if not combined_results and is_summary_request and _sql_store:
        try:
            # ì§ˆë¬¸ì—ì„œ ì œëª©ì´ë‚˜ IDë¥¼ ìœ ì¶”í•˜ì§€ ëª»í–ˆì„ ë•Œ ë§ˆì§€ë§‰ ì‹œë„ë¡œ ëª©ë¡ì˜ ì²« ë²ˆì§¸ ë¬¸ì„œë¼ë„ ê°€ì ¸ì˜´
            all_docs = _sql_store.list_documents()
            if all_docs:
                 doc = all_docs[0] # ì„ì‹œ: ì²« ë²ˆì§¸ ë¬¸ì„œ
                 full_doc = _sql_store.get_document_by_id(doc['sop_id'])
                 combined_results.append(f"ğŸ“„ [ì „ì—­ ê²€ìƒ‰/í´ë°±] ì¶œì²˜: {doc['sop_id']}\n{full_doc.get('markdown_content', '')[:10000]}")
        except: pass

    return "\n\n".join(combined_results)

@tool
def get_document_references(sop_id: str) -> str:
    """ì°¸ì¡° ë¬¸ì„œ ì¡°íšŒ (Graph)"""
    if not _graph_store: return ""
    try:
        refs = _graph_store.get_document_references(sop_id.upper())
        if not refs: return ""
        doc = refs.get("document", {})
        return f"ğŸ“„ {doc.get('sop_id')} ì°¸ì¡°ë¬¸ì„œ: {', '.join(refs.get('references', []))}"
    except Exception: return ""

AGENT_TOOLS = [hybrid_search_sop, get_document_references]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ê³µí†µ ì—­í•  ì§€ì •
BASE_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ë¶€ GMP ê·œì •(SOP) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ëª¨ë“  ë‹µë³€ì€ **ì˜¤ì§ ì œê³µëœ ê·œì • ë°ì´í„°(Tool Observation)**ì—ë§Œ ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸš« ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­ (Hallucination Warning)
1. **ì™¸ë¶€ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€**: ë‹¹ì‹ ì´ ì´ë¯¸ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ GMP ì§€ì‹(ì˜ˆ: ALCOA+, ì¼ë°˜ì ì¸ ë¡œê·¸ë¶ ì •ì˜ ë“±)ì´ë¼ë„ ê²€ìƒ‰ëœ ë°ì´í„°ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ì ˆëŒ€ë¡œ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
2. **ì¶”ì¸¡ ê¸ˆì§€**: ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì„ "ì¼ë°˜ì ìœ¼ë¡œ ~í•˜ë‹¤"ë¼ê³  ì¶”ì¸¡í•˜ì—¬ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
3. **ì¦ê±° ì—†ëŠ” ë‹µë³€ ê¸ˆì§€**: ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ì¡°í•­ì—ì„œ ì§ì ‘ì ì¸ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ ëˆ„ë½ì‹œí‚¤ì„¸ìš”.

## ğŸ“‹ ë‹µë³€ ì›ì¹™
1. **ì¦ê±° ê¸°ë°˜ (Strictly Evidence-based)**: ë‹µë³€ì˜ ëª¨ë“  ë¬¸ì¥ì€ ì œê³µëœ SOP í…ìŠ¤íŠ¸ì˜ íŠ¹ì • ì¡°í•­ì—ì„œ ê¸°ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
2. **ì •í™•í•œ ì¸ìš©**: ë‹µë³€ ì‹œ ë°˜ë“œì‹œ `[SOP ID] > [ì„¹ì…˜ëª…] (p.í˜ì´ì§€)` í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
3. **ë°ì´í„° ëˆ„ë½ ì‹œ**: ê´€ë ¨ ë‚´ìš©ì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´ "ì œê³µëœ ê·œì •ì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

# ì „ë¬¸ ì§„ë‹¨ìš© ì¶”ê°€ ì§€ì¹¨ (OOS, ì¼íƒˆ, ì¬ì‹œí—˜ ë“± ë³µì¡í•œ ìƒí™©)
VERIFICATION_INSTRUCTIONS = """
## ğŸ¯ í•µì‹¬ ì›ì¹™: ëŠ¥ë™ì  ì¶”ë¡  (Active Reasoning)
1. **ëª…ì‹œì  ê·œì • ìš°ì„ **: ë¬¸ì„œì— ì§ì„¤ì ìœ¼ë¡œ "ê¸ˆì§€" ë˜ëŠ” "í—ˆìš©"ì´ ëª…ì‹œëœ ê²½ìš° ì´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.
2. **ë…¼ë¦¬ì  ì¶”ë¡  (Deduction)**: êµ¬ì²´ì ì¸ í—ˆìš© ì—¬ë¶€ê°€ ì—†ë”ë¼ë„, ìƒìœ„ ê·œì •ì˜ ì·¨ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ìœ„ ìƒí™©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”. "ê·œì •ì˜ ì·¨ì§€ìƒ ~í•´ì•¼ í•œë‹¤"ëŠ” ë°©í–¥ì„ ì œì‹œí•˜ì„¸ìš”.

## ğŸ“‹ ë‹µë³€ êµ¬ì¡° (í•„ìˆ˜: ë³´ê³ ì„œ í˜•ì‹)
### **1. ê²€ì¦ ì˜ê²¬**
- [**í•µì‹¬ ê²°ë¡ **]: ê²°ë¡ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•íˆ ì œì‹œ
- [**ìƒì„¸ ë¶„ì„**]: ê·œì •ì˜ ì·¨ì§€ì™€ ì‚¬ìš©ì ìƒí™©ì„ ëŒ€ì¡°í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…

### **2. ê²€ì¦ ê·¼ê±° ë° ì¶œì²˜**
- ê° ê·¼ê±°ë³„ ë²ˆí˜¸ì™€ ì œëª©
- ì •í™•í•œ ì¶œì²˜ í‘œê¸° í•„ìˆ˜: `**[ì¶œì²˜]** [SOP ID] > [ì œëª©] > [ìƒì„¸ ë¬¸êµ¬ ì¸ìš©] (p.í˜ì´ì§€)`

### **3. ì¡°ì¹˜ ê¶Œê³  ë° ì œì–¸**
- ì‚¬ìš©ìê°€ ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™ ê°€ì´ë“œ ë° ê´€ë ¨ í•˜ìœ„ ì§€ì¹¨ì„œ ì œì•ˆ
"""

# ë‹¨ìˆœ ì •ë³´ ì œê³µìš© ì¶”ê°€ ì§€ì¹¨ (ìš”ì•½, ì„¤ëª… ë“±)
INFO_INSTRUCTIONS = """
## ğŸ“‹ ë‹µë³€ êµ¬ì¡°
- ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ **ë¬¸ì„œì— ëª…ì‹œëœ í…ìŠ¤íŠ¸ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜**í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
- AIì˜ ìˆ˜ë ¤í•œ ìš”ì•½ë³´ë‹¤ **ë¬¸ì„œìƒì˜ ì •í™•í•œ ì •ì˜ì™€ ìš”ê±´**ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•©ë‹ˆë‹¤.
- ë‹µë³€ì˜ ê° ì£¼ìš” í•­ëª© ëì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ê·¼ê±° ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.
- **ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°**: ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ê°œë…(ì˜ˆ: ALCOA+ ë“±)ì„ ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ ë³´ì¶©í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ "ë¬¸ì„œ ì•ˆì—ì„œ ì°¾ì€ ê²°ê³¼"ë§Œ ë³´ì—¬ì£¼ì„¸ìš”.
"""

class AgentState(TypedDict):
    query: str
    model_name: str
    embedding_model: str
    search_results: List[Dict]
    answer: str
    reasoning: str
    queries: List[str]
    keywords: List[str]
    is_verification: bool # ì¶”ê°€: ê²€ì¦ ì„±ê²© ì§ˆë¬¸ ì—¬ë¶€
    # ğŸ”¥ ReAct ë£¨í”„ë¥¼ ìœ„í•œ ë©”ì‹œì§€ ê¸°ë¡ ì¶”ê°€
    messages: Annotated[List[Any], operator.add]

_agent = None

def create_agent(model_name: str = "glm-4.7-flash"):
    global _agent
    api_key = os.getenv("ZAI_API_KEY")
    if not api_key: raise ValueError("ZAI_API_KEY ì„¤ì • í•„ìš”")
    _agent = {"model": model_name, "client": ZaiClient(api_key=api_key)}
    return _agent

@traceable(run_type="llm", name="Z.AI-LLM-Completion")
def _llm_chat_completion(messages: List[Dict], model: str, tools: Optional[List] = None, tool_choice: str = "auto"):
    """LangSmithì—ì„œ LLM ë…¸ë“œë¡œ í‘œì‹œë˜ë„ë¡ í•˜ëŠ” ì¶”ì ìš© ë˜í¼"""
    return _agent["client"].chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice=tool_choice,
        temperature=0.1
    )

def query_expansion_node(state: AgentState):
    """ë©€í‹° ì¿¼ë¦¬ ë° ì§€ëŠ¥í˜• í‚¤ì›Œë“œ í™•ì¥ (Regex ë°©ì§€)"""
    client = _agent["client"]
    print(f"ğŸ§  [Agent] ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ ì¤‘ (Query: {state['query']})")
    
    prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê·œì • ê²€ìƒ‰ì„ ìœ„í•œ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
íŠ¹íˆ "10ë²ˆ ë¬¸ì„œ"ì™€ ê°™ì´ ìˆ«ìê°€ ì–¸ê¸‰ë˜ë©´ ì´ëŠ” "EQ-SOP-00010"ê³¼ ê°™ì€ ê´€ë¦¬ ë²ˆí˜¸ì˜ íŒŒí¸ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìœ¼ë¯€ë¡œ, í•´ë‹¹ ìˆ«ìë¥¼ í‚¤ì›Œë“œì— í¬í•¨í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{state['query']}

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
{{
  "expanded_queries": ["ê²€ìƒ‰ìš©ìœ¼ë¡œ í™•ì¥ëœ ë¬¸ì¥ 2ê°œ"],
  "keywords": ["SQL ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ë‹¨ì–´/ìˆ«ì 5ê°œ (ID í¬í•¨)"]
}}"""
    
    try:
        res = _llm_chat_completion(
            model=state["model_name"], 
            messages=[{"role": "user", "content": prompt}]
        )
        content = res.choices[0].message.content
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        import json
        if json_match:
            data = json.loads(json_match.group(0))
            return {
                "queries": [state["query"]] + data.get("expanded_queries", []),
                "keywords": data.get("keywords", [])
            }
    except Exception as e:
        print(f"âš ï¸ [Expansion] ì‹¤íŒ¨: {e}")
    
    return {"queries": [state["query"]], "keywords": []}

def reasoner_node(state: AgentState):
    """ì‚¬ê³ (Reasoning) ë° í–‰ë™(Acting) ê²°ì • ë…¸ë“œ"""
    print(f"âš–ï¸ [Reasoner] ì‚¬ê³  ì¤‘... (Message Count: {len(state['messages'])})")
    
    # ì§ˆë¬¸ ì„±ê²©ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    is_verification = any(kw in state['query'] for kw in ["ë˜ë‚˜ìš”", "ê°€ëŠ¥í•œê°€ìš”", "ìœ„ë°˜", "ì ì ˆ", "íŒë‹¨", "í—ˆìš©", "ê¸ˆì§€", "ì í•©"])
    instructions = VERIFICATION_INSTRUCTIONS if is_verification else INFO_INSTRUCTIONS
    
    system_prompt = f"{BASE_SYSTEM_PROMPT}\n{instructions}"
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸ìš© ì‚¬ì „ ì •ë³´
    state_update = {"is_verification": is_verification}
    
    # ë„êµ¬ ì •ì˜ ì „ë‹¬ (LLMì´ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •)
    tools = [
        {
            "type": "function",
            "function": {
                "name": "hybrid_search_sop",
                "description": "SOP ë¬¸ì„œ ê²€ìƒ‰ (Vector + SQL í•˜ì´ë¸Œë¦¬ë“œ). ìš”ì•½ì´ í•„ìš”í•˜ê±°ë‚˜ íŠ¹ì • ë²ˆí˜¸ ì¡°íšŒê°€ í•„ìš”í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "ê²€ìƒ‰ì–´"},
                        "keywords": {"type": "array", "items": {"type": "string"}, "description": "SQL ë§¤ì¹­ìš© í•µì‹¬ í‚¤ì›Œë“œ ëª©ë¡"}
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_document_references",
                "description": "íŠ¹ì • SOP ë¬¸ì„œê°€ ì°¸ì¡°í•˜ê³  ìˆëŠ” ë‹¤ë¥¸ ì—°ê´€ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤ (Graph DB ê¸°ë°˜).",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "sop_id": {"type": "string", "description": "ì¡°íšŒí•  SOP ID (ì˜ˆ: EQ-SOP-00010)"}
                    },
                    "required": ["sop_id"]
                }
            }
        }
    ]

    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    
    try:
        res = _llm_chat_completion(
            model=state["model_name"], 
            messages=messages, 
            tools=tools,
            tool_choice="auto"
        )
        msg = res.choices[0].message
        
        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
        if msg.tool_calls:
            return {"messages": [msg]}
        
        # ìµœì¢… ë‹µë³€ì¸ ê²½ìš°
        return {
            **state_update,
            "messages": [msg],
            "answer": msg.content or "",
            "reasoning": getattr(msg, 'reasoning_content', "")
        }
    except Exception as e:
        print(f"âš ï¸ [Reasoner Error] {e}")
        return {"messages": [{"role": "assistant", "content": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"}]}

def tool_node(state: AgentState):
    """ë„êµ¬ ì‹¤í–‰(Acting) ë° ê²°ê³¼(Observation) ë°˜í™˜ ë…¸ë“œ"""
    last_msg = state["messages"][-1]
    new_messages = []
    
    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
        for tc in last_msg.tool_calls:
            tool_name = tc.function.name
            args = json.loads(tc.function.arguments)
            
            print(f"ğŸ› ï¸ [Tool] {tool_name} ì‹¤í–‰ ì¤‘... ({args})")
            
            obs = ""
            if tool_name == "hybrid_search_sop":
                kw = args.get("keywords") or state.get("keywords", [])
                obs = hybrid_search_sop.invoke({
                    "query": args["query"], 
                    "keywords": kw,
                    "embedding_model": state.get("embedding_model")
                })
            elif tool_name == "get_document_references":
                obs = get_document_references.invoke({
                    "sop_id": args["sop_id"]
                })
            
            new_messages.append({
                "role": "tool", 
                "tool_call_id": tc.id, 
                "name": tool_name, 
                "content": obs or "âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            })
    
    return {"messages": new_messages}

def verifier_node(state: AgentState):
    """ìµœì¢… ê·œì • ê²€ì¦ ë° ë¬´ê²°ì„± ì²´í¬ ë…¸ë“œ"""
    print(f"âš–ï¸ [Verifier] ìµœì¢… ê·œì • ì í•©ì„± íŒë‹¨ ë° ê²€ì¦ ì¤‘")
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œí¸: ì •ë‹µ ìš°ì„  ì›ì¹™
    is_v = state.get("is_verification", False)
    
    if is_v:
        # ê²€ì¦ ëª¨ë“œ: ì‹¬ì¸µ ë³´ê³ ì„œ + ì •ë‹µ
        verification_prompt = f"""ë‹¹ì‹ ì€ í’ˆì§ˆë³´ì¦(QA) ë¶€ì„œì˜ ìµœì¢… ìŠ¹ì¸ê¶Œìì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê·œì • ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì í•©ì„± íŒì • ë° ìµœì¢… ë‹µë³€**ì„ ì‘ì„±í•˜ì„¸ìš”.

## ğŸ¯ ë‹µë³€ ì‘ì„± ê°€ì´ë“œ (ê²€ì¦ ëª¨ë“œ)
1. **ê²°ë¡  (Conclusion)**: ì§ˆë¬¸ì— ëŒ€í•œ ì í•©ì„± ì—¬ë¶€(ì˜ˆ: í—ˆìš©/ê¸ˆì§€/ìœ„ë°˜ ë“±)ë¥¼ ìµœìƒë‹¨ì— ëª…í™•íˆ ê¸°ì¬í•˜ì„¸ìš”.
2. **ìƒì„¸ ê·¼ê±°**: ê²€ìƒ‰ëœ SOPì˜ ì¡°í•­ë“¤ì„ ì¸ìš©í•˜ì—¬ ì™œ ê·¸ëŸ° ê²°ë¡ ì´ ë‚˜ì™”ëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3. **QA ê²€í†  ë³´ê³ ì„œ**: ë‹µë³€ í•˜ë‹¨ì— [ìƒì¶©/ëˆ„ë½/ì¤€ìˆ˜] ì—¬ë¶€ë¥¼ í¬í•¨í•œ ë³´ê³ ì„œ ì„¹ì…˜ì„ êµ¬ì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {state.get('query')}"""
    else:
        # ì •ë³´ ê²€ìƒ‰ ëª¨ë“œ: ì •ë‹µ ì¤‘ì‹¬ + ìµœì†Œí•œì˜ ê²€ì¦
        verification_prompt = f"""ë‹¹ì‹ ì€ GMP ê·œì •(SOP) ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì°¾ëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰ëœ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•˜ì—¬ **ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.

## ğŸ¯ ë‹µë³€ ì‘ì„± ê°€ì´ë“œ (ì •ë³´ ê²€ìƒ‰ ëª¨ë“œ)
1. **ì§ì ‘ì ì¸ ì •ë‹µ**: ì‚¬ìš©ìê°€ ë¬»ëŠ” ì •ë³´(ì˜ˆ: íŠ¹ì • ì¡°í•­ì˜ ë‚´ìš©, ë¬¸ì„œ ë²ˆí˜¸ ë“±)ë¥¼ ê°€ì¥ ë¨¼ì €, ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. 
2. **ë¶ˆí•„ìš”í•œ í˜•ì‹ ì§€ì–‘**: "QA ê²€í†  ê²°ê³¼"ì™€ ê°™ì€ ë”±ë”±í•œ ë³´ê³ ì„œ í˜•ì‹ì„ ìµœìƒë‹¨ì— ë‘ì§€ ë§ˆì„¸ìš”. 
3. **ê·¼ê±° í‘œê¸°**: ë‹µë³€ ë‚´ìš© ëì— ì¶œì²˜(ì¡°í•­ ë²ˆí˜¸)ë§Œ ì§§ê²Œ ë§ë¶™ì´ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {state.get('query')}"""

    messages = [{"role": "system", "content": verification_prompt}] + state["messages"]
    
    try:
        # ê²€ì¦ ê²°ê³¼ ìƒì„± (LLM í˜¸ì¶œ)
        res = _llm_chat_completion(
            model=state["model_name"], 
            messages=messages,
            tool_choice="none" # ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì—ëŠ” ë„êµ¬ ì‚¬ìš© ì•ˆ í•¨
        )
        msg = res.choices[0].message
        return {
            "answer": msg.content or "",
            "reasoning": getattr(msg, 'reasoning_content', ""),
            "messages": [msg]
        }
    except Exception as e:
        print(f"âš ï¸ [Verifier Error] {e}")
        return {}

def should_continue(state: AgentState):
    """ë£¨í”„ ì¢…ë£Œ ì—¬ë¶€ ê²°ì •"""
    last_msg = state["messages"][-1]
    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
        return "tools"
    return END

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_workflow():
    if not LANGGRAPH_AGENT_AVAILABLE:
        return None
    
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("expansion", query_expansion_node)
    workflow.add_node("reasoner", reasoner_node)
    workflow.add_node("tools", tool_node)
    workflow.add_node("verifier", verifier_node) # ê²€ì¦ ë…¸ë“œ ì¶”ê°€
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "expansion")
    workflow.add_edge("expansion", "reasoner")
    
    # ğŸ”¥ ReAct ë£¨í”„: Reasoner -> (Tools -> Reasoner) -> Verifier -> End
    workflow.add_conditional_edges(
        "reasoner",
        should_continue,
        {
            "tools": "tools",
            # ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰ì¸ ê²½ìš° ê²€ì¦ ë…¸ë“œë¥¼ ê±´ë„ˆë›°ê±°ë‚˜ ë°”ë¡œ ì—”ë“œë¡œ ê°ˆ ìˆ˜ë„ ìˆì§€ë§Œ,
            # í˜„ì¬ëŠ” ëª¨ë“  ìµœì¢… ë‹µë³€ì˜ í’ˆì§ˆì„ ìœ„í•´ verifierë¥¼ ê±°ì¹˜ë˜ í”„ë¡¬í”„íŠ¸ë¡œ ì œì–´í•¨
            END: "verifier" 
        }
    )
    workflow.add_edge("tools", "reasoner")
    workflow.add_edge("verifier", END)
    
    return workflow.compile()

# ì „ì—­ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤
_workflow_app = None

def run_agent(query: str, session_id: str = "default", model_name: str = "glm-4.7-flash", embedding_model: str = "intfloat/multilingual-e5-small"):
    global _workflow_app
    if not _agent: create_agent(model_name)
    
    # ğŸ”¥ LangSmithì—ì„œ "Input" ì»¬ëŸ¼ì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ê²Œ ë‚˜ì˜¤ë„ë¡ ë˜í•‘ í•¨ìˆ˜ ì •ì˜
    @traceable(name="GMP-SOP-Orchestrator", run_type="chain")
    def _orchestrated_run(user_input: str, state: dict, runner_config: dict):
        return _workflow_app.invoke(state, config=runner_config)

    initial_state = {
        "query": query, 
        "model_name": model_name, 
        "embedding_model": embedding_model,
        "queries": [],
        "keywords": [],
        "search_results": [],
        "messages": [{"role": "user", "content": query}] # ë©”ì‹œì§€ ì´ˆê¸°í™”
    }
    
    # LangGraphë¥¼ í†µí•œ ì‹¤í–‰
    if LANGGRAPH_AGENT_AVAILABLE:
        if not _workflow_app:
            _workflow_app = create_workflow()
        
        # ëª…ì‹œì ì¸ run_name ë¶€ì—¬ë¡œ LangSmith ê°€ì‹œì„± í™•ë³´
        config = {
            "configurable": {"thread_id": session_id},
            "run_name": "GMP-SOP-Orchestrator",
            "metadata": {"session_id": session_id, "model": model_name}
        }
        
        # user_inputì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ LangSmith ê°€ì‹œì„± ê³ ì •
        final_state = _orchestrated_run(user_input=query, state=initial_state, runner_config=config)
    else:
        # í´ë°±: ìˆ˜ë™ ë…¸ë“œ í˜¸ì¶œ
        expanded = query_expansion_node(initial_state)
        initial_state.update(expanded)
        final_state = verifier_agent_node(initial_state)
    
    return {
        "answer": final_state.get("answer", "âŒ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."), 
        "reasoning": final_state.get("reasoning", ""),
        "success": True,
        "tool_calls": [{"tool": "hybrid_search", "queries": final_state.get("queries")}]
    }
