"""
GMP/SOP ì—ì´ì „íŠ¸ ëª¨ë“ˆ v1.0

ğŸ¤– ReAct ì—ì´ì „íŠ¸ + LangSmith ì¶”ì 
- ë„êµ¬: ChromaDB ê²€ìƒ‰, Neo4j ê·¸ë˜í”„ ê²€ìƒ‰
- LLMì´ ìƒí™©ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰
- LangSmithë¡œ ì‹¤í–‰ ê³¼ì • ëª¨ë‹ˆí„°ë§
"""

import os
from typing import List, Dict, Optional, Any, Annotated, TypedDict
from datetime import datetime
import operator

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LangSmith ì„¤ì • (ë§¨ ìœ„ì—ì„œ ì„¤ì •í•´ì•¼ í•¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥
LANGSMITH_API_KEY = os.getenv("LANGCHAIN_API_KEY", "")
LANGSMITH_PROJECT = os.getenv("LANGCHAIN_PROJECT", "gmp-sop-agent")

if LANGSMITH_API_KEY:
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_API_KEY
    os.environ["LANGCHAIN_PROJECT"] = LANGSMITH_PROJECT
    print(f"âœ… LangSmith ì—°ë™ í™œì„±í™”: {LANGSMITH_PROJECT}")
else:
    print("âš ï¸ LangSmith API í‚¤ ì—†ìŒ - ë¡œì»¬ ëª¨ë“œë¡œ ì‹¤í–‰")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì˜ì¡´ì„± ì„í¬íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from langchain_core.tools import tool
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    from langchain_core.language_models.chat_models import BaseChatModel
    from langchain_core.outputs import ChatResult, ChatGeneration
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ LangChain íŒ¨í‚¤ì§€ í•„ìš”: pip install langchain langchain-core")
    LANGCHAIN_AVAILABLE = False

try:
    from langgraph.prebuilt import create_react_agent
    from langgraph.checkpoint.memory import MemorySaver
    LANGGRAPH_AGENT_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ LangGraph ì—ì´ì „íŠ¸ íŒ¨í‚¤ì§€ í•„ìš”: pip install langgraph")
    LANGGRAPH_AGENT_AVAILABLE = False

# Z.AI SDK
try:
    from zai import ZaiClient
    ZAI_AVAILABLE = True
except ImportError:
    ZAI_AVAILABLE = False
    print(f"âš ï¸ Z.AI SDK í•„ìš”: pip install zai-sdk")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë„êµ¬ ì •ì˜ (Tools)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì „ì—­ ë³€ìˆ˜ (ì´ˆê¸°í™” ì‹œ ì„¤ì •)
_vector_store = None
_graph_store = None


def init_agent_tools(vector_store_module, graph_store_instance):
    """ì—ì´ì „íŠ¸ ë„êµ¬ ì´ˆê¸°í™”"""
    global _vector_store, _graph_store
    _vector_store = vector_store_module
    _graph_store = graph_store_instance
    print("âœ… ì—ì´ì „íŠ¸ ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ")


@tool
def search_sop_documents(query: str) -> str:
    """
    SOP ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì¼ë°˜ì ì¸ ì§ˆë¬¸, ì ˆì°¨ í™•ì¸, ì •ì˜ ê²€ìƒ‰ ë“±ì— ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        query: ê²€ìƒ‰í•  ë‚´ìš© (ì˜ˆ: "í’ˆì§ˆê´€ë¦¬ì±…ì„ìì˜ ì—­í• ", "ë¬¸ì„œ ë³€ê²½ ì ˆì°¨")
    
    Returns:
        ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ê³¼ ì¶œì²˜
    """
    if not _vector_store:
        return "âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        results = _vector_store.search(
            query=query,
            collection_name="documents",
            model_name="intfloat/multilingual-e5-small",
            n_results=10,
            similarity_threshold=0.3
        )
        
        if not results:
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        output = []
        for i, r in enumerate(results, 1):
            meta = r.get("metadata", {})
            sop_id = meta.get("sop_id", "N/A")
            section = meta.get("section_path", meta.get("section", ""))
            page = meta.get("page", "")
            similarity = r.get("similarity", 0)
            text = r.get("text", "")[:500]
            
            source = f"[{sop_id}]"
            if section:
                source += f" > {section}"
            if page:
                source += f" (p.{page})"
            
            output.append(f"ğŸ“„ {source} (ìœ ì‚¬ë„: {similarity:.0%})\n{text}...")
        
        return "\n\n---\n\n".join(output)
    
    except Exception as e:
        return f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"


@tool
def get_document_references(sop_id: str) -> str:
    """
    íŠ¹ì • SOP ë¬¸ì„œê°€ ì°¸ì¡°í•˜ëŠ” ë‹¤ë¥¸ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    ë¬¸ì„œ ê°„ ê´€ê³„ë‚˜ ì—°ê´€ ê·œì •ì„ ì°¾ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        sop_id: SOP ë¬¸ì„œ ID (ì˜ˆ: "EQ-SOP-00001")
    
    Returns:
        ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œ ëª©ë¡ê³¼ ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œ ëª©ë¡
    """
    if not _graph_store:
        return "âŒ ê·¸ë˜í”„ ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        # sop_id ì •ê·œí™”
        sop_id = sop_id.upper().strip()
        if not sop_id.startswith("EQ-"):
            sop_id = "EQ-" + sop_id
        
        refs = _graph_store.get_document_references(sop_id)
        
        if not refs:
            return f"'{sop_id}' ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì°¸ì¡° ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        doc = refs.get("document", {})
        references = refs.get("references", [])
        cited_by = refs.get("cited_by", [])
        
        output = [f"ğŸ“„ ë¬¸ì„œ: {doc.get('sop_id', sop_id)} - {doc.get('title', '')}"]
        
        if references:
            output.append(f"\nğŸ”— ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œ ({len(references)}ê°œ):")
            for ref in references:
                output.append(f"  â†’ {ref}")
        else:
            output.append("\nğŸ”— ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œ: ì—†ìŒ")
        
        if cited_by:
            output.append(f"\nğŸ“¥ ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œ ({len(cited_by)}ê°œ):")
            for ref in cited_by:
                output.append(f"  â† {ref}")
        else:
            output.append("\nğŸ“¥ ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œ: ì—†ìŒ")
        
        return "\n".join(output)
    
    except Exception as e:
        return f"âŒ ì°¸ì¡° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"


@tool
def search_sections_by_keyword(keyword: str, sop_id: str = None) -> str:
    """
    í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ì„¹ì…˜ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    íŠ¹ì • ìš©ì–´ë‚˜ ê°œë…ì´ ì–´ëŠ ì„¹ì…˜ì— ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ ì°¾ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: "ì±…ì„", "ì ˆì°¨", "ê¸°ë¡")
        sop_id: íŠ¹ì • ë¬¸ì„œë¡œ í•œì •í•  ê²½ìš° SOP ID (ì„ íƒì‚¬í•­)
    
    Returns:
        í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì„¹ì…˜ ëª©ë¡
    """
    if not _graph_store:
        return "âŒ ê·¸ë˜í”„ ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        results = _graph_store.search_sections(keyword, sop_id=sop_id, limit=5)
        
        if not results:
            scope = f" ({sop_id} ë‚´)" if sop_id else ""
            return f"'{keyword}' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤{scope}."
        
        output = [f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):"]
        
        for sec in results:
            sop = sec.get("doc_sop_id", "N/A")
            name = sec.get("name", "")
            path = sec.get("section_path", "")
            page = sec.get("page", "")
            
            location = f"[{sop}] {name}"
            if path:
                location += f"\n   ğŸ“ {path}"
            if page:
                location += f" (p.{page})"
            
            output.append(f"\nğŸ“„ {location}")
        
        return "\n".join(output)
    
    except Exception as e:
        return f"âŒ ì„¹ì…˜ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"


@tool
def get_document_structure(sop_id: str) -> str:
    """
    íŠ¹ì • SOP ë¬¸ì„œì˜ ì„¹ì…˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    ë¬¸ì„œì˜ ëª©ì°¨ë‚˜ êµ¬ì„±ì„ íŒŒì•…í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        sop_id: SOP ë¬¸ì„œ ID (ì˜ˆ: "EQ-SOP-00001")
    
    Returns:
        ë¬¸ì„œì˜ ì„¹ì…˜ ê³„ì¸µ êµ¬ì¡°
    """
    if not _graph_store:
        return "âŒ ê·¸ë˜í”„ ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        sop_id = sop_id.upper().strip()
        if not sop_id.startswith("EQ-"):
            sop_id = "EQ-" + sop_id
        
        hierarchy = _graph_store.get_section_hierarchy(sop_id)
        
        if not hierarchy:
            return f"'{sop_id}' ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        output = [f"ğŸ“‹ {sop_id} ë¬¸ì„œ êµ¬ì¡°:"]
        
        for item in hierarchy[:15]:  # ìƒìœ„ 15ê°œë§Œ
            sec = item.get("section", {})
            name = sec.get("name", "")
            sec_type = sec.get("section_type", "")
            children = item.get("children", [])
            
            # ë“¤ì—¬ì“°ê¸°
            indent = ""
            if sec_type == "subsection":
                indent = "  "
            elif sec_type == "subsubsection":
                indent = "    "
            
            child_info = f" ({len(children)}ê°œ í•˜ìœ„)" if children else ""
            output.append(f"{indent}â€¢ {name}{child_info}")
        
        if len(hierarchy) > 15:
            output.append(f"  ... ì™¸ {len(hierarchy) - 15}ê°œ ì„¹ì…˜")
        
        return "\n".join(output)
    
    except Exception as e:
        return f"âŒ êµ¬ì¡° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"


@tool
def list_all_documents() -> str:
    """
    ì‹œìŠ¤í…œì— ë“±ë¡ëœ ëª¨ë“  SOP ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    ì–´ë–¤ ë¬¸ì„œê°€ ìˆëŠ”ì§€ íŒŒì•…í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Returns:
        ë“±ë¡ëœ SOP ë¬¸ì„œ ëª©ë¡
    """
    if not _graph_store:
        return "âŒ ê·¸ë˜í”„ ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        docs = _graph_store.get_all_documents()
        
        if not docs:
            return "ë“±ë¡ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        output = [f"ğŸ“š ë“±ë¡ëœ SOP ë¬¸ì„œ ({len(docs)}ê°œ):"]
        
        for doc in docs:
            sop_id = doc.get("sop_id", "N/A")
            title = doc.get("title", "")
            sections = doc.get("section_count", 0)
            output.append(f"  â€¢ {sop_id}: {title} ({sections}ê°œ ì„¹ì…˜)")
        
        return "\n".join(output)
    
    except Exception as e:
        return f"âŒ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì—ì´ì „íŠ¸ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ë„êµ¬ ë¦¬ìŠ¤íŠ¸
AGENT_TOOLS = [
    search_sop_documents,
    get_document_references,
    search_sections_by_keyword,
    get_document_structure,
    list_all_documents,
]


# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
AGENT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ GMP(ì˜ì•½í’ˆ ì œì¡° ë° í’ˆì§ˆê´€ë¦¬) ê·œì • ì „ë¬¸ê°€ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ì˜ **êµ¬ì¡°í™”ëœ ë‹µë³€ í˜•ì‹**ì„ ì¤€ìˆ˜í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

## ë‹µë³€ í˜•ì‹ (í•„ìˆ˜)
1. **ê²€ì¦ ì˜ê²¬**: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ ê³¼ ì „ë¬¸ì ì¸ ë¶„ì„ ë‚´ìš©ì„ ìƒì„¸íˆ ì„œìˆ í•©ë‹ˆë‹¤.
2. **ê²€ì¦ ê·¼ê±° ë° ì¶œì²˜**: 
   - ê° ê·¼ê±°ë³„ë¡œ ìˆ«ìë¥¼ ë§¤ê²¨ ì œëª©(`**1. ì œëª©**`)ì„ ì‘ì„±í•©ë‹ˆë‹¤.
   - í•´ë‹¹ ê·¼ê±°ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…ì„ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
   - ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ `**[ì¶œì²˜]** [SOP ID] > [ì¥/ì ˆ ì œëª©] > [ìƒì„¸ ë¬¸êµ¬ ì¸ìš©] (p.í˜ì´ì§€)` í˜•ì‹ì„ ì§€í‚µë‹ˆë‹¤.

## í•µì‹¬ ê·œì¹™
1. **ìƒì„¸ì„±**: ë‹¨ìˆœíˆ ì§§ê²Œ ëŒ€ë‹µí•˜ì§€ ë§ê³ , ê·œì •ì˜ ë§¥ë½ì„ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì„¸ìš”.
2. **ê·¼ê±° ì¤‘ì‹¬**: ëª¨ë“  ì£¼ì¥ì€ ë°˜ë“œì‹œ ê²€ìƒ‰ëœ SOPì˜ êµ¬ì²´ì ì¸ ì¡°í•­ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.
3. **ê°ê´€ì„±**: ì¶”ì¸¡ì„ ë°°ì œí•˜ê³  ë¬¸ì„œì— ëª…ì‹œëœ ì‚¬ì‹¤ë§Œì„ ì „ë‹¬í•˜ì„¸ìš”.
"""


class AgentState(TypedDict):
    """ë©€í‹° ì—ì´ì „íŠ¸ ê³µìœ  ìƒíƒœ"""
    messages: Annotated[List[Any], operator.add]
    query: str
    next_node: str
    search_results: List[Dict]
    verification: str
    answer: str
    tool_calls: List[Dict]
    session_id: str
    model_name: str
    

# ë©”ëª¨ë¦¬ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€)
_memory_saver = None
_agent = None


def create_agent(model_name: str = "glm-4.7-flash"):
    """ì—ì´ì „íŠ¸ ìƒì„± (Z.AI ê¸°ë°˜)"""
    global _agent, _memory_saver
    
    if not ZAI_AVAILABLE:
        raise ImportError("Z.AI SDKê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install zai-sdk")
    
    # Z.AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    api_key = os.getenv("ZAI_API_KEY", "")
    if not api_key:
        raise ValueError("ZAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    _agent = {
        "model": model_name,
        "api_key": api_key,
        "client": ZaiClient(api_key=api_key)
    }
    
    _memory_saver = MemorySaver() if LANGGRAPH_AGENT_AVAILABLE else {}
    
    print(f"âœ… Z.AI ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ: {model_name}")
    return _agent


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©€í‹° ì—ì´ì „íŠ¸ ë…¸ë“œ êµ¬í˜„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def orchestrator_node(state: AgentState):
    """ì˜ë„ ë¶„ì„ ë° ì‘ì—… ë¶„ë°° ë…¸ë“œ"""
    query = state["query"]
    model_name = state["model_name"]
    client = _agent["client"]
    
    print(f"ğŸ¯ [Orchestrator] ì˜ë„ ë¶„ì„ ì¤‘: {query}")
    
    prompt = f"""ë‹¹ì‹ ì€ GMP ê·œì • ì‹œìŠ¤í…œì˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„(next_node)ë¥¼ ê²°ì •í•˜ì„¸ìš”.
    - search_agent: ê·œì • ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
    - verifier_agent: íŠ¹ì • ìƒí™©ì´ë‚˜ í–‰ìœ„ê°€ ê·œì •ì— ë§ëŠ”ì§€ ê²€ì°©/ê²€ì¦ì´ í•„ìš”í•œ ê²½ìš° (ê²€ìƒ‰ ê²°ê³¼ê°€ ì´ë¯¸ ìˆë‹¤ë©´)
    - list_agent: ë¬¸ì„œ ëª©ë¡ ì¡°íšŒê°€ í•„ìš”í•œ ê²½ìš°
    - writer_agent: ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆì–´ ë‹µë³€ì„ ìƒì„±í•˜ë©´ ë˜ëŠ” ê²½ìš°

    í˜„ì¬ ì§ˆë¬¸: {query}
    ì‘ë‹µ í˜•ì‹: [ë…¸ë“œì´ë¦„]
    ì˜ˆ: search_agent"""

    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=50,
        temperature=0.1
    )
    
    next_node = response.choices[0].message.content.strip().lower()
    if "search" in next_node: next_node = "search_agent"
    elif "verify" in next_node or "ê²€ì¦" in next_node: next_node = "verifier_agent"
    elif "list" in next_node: next_node = "list_agent"
    else: next_node = "search_agent" # ê¸°ë³¸ê°’

    return {"next_node": next_node}


def search_agent_node(state: AgentState):
    """ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    query = state["query"]
    print(f"ğŸ” [SearchAgent] ê·œì • ê²€ìƒ‰ ì‹œë„: {query}")
    
    # ë²¡í„° ê²€ìƒ‰ ë° ê·¸ë˜í”„ ê²€ìƒ‰ í†µí•© ì‚¬ìš©
    results = search_sop_documents.invoke(query)
    
    # ì§ˆë¬¸ì— 'ë§ëŠ”ì§€', 'ì ì ˆí•œì§€' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ì¦ ë…¸ë“œë¡œ ìœ ë„
    should_verify = any(kw in query for kw in ["ë§ëŠ”ì§€", "ì ì ˆí•œì§€", "ê°€ëŠ¥í•œì§€", "ìœ„ë°˜", "ê²€ì¦", "ì˜ê²¬"])
    
    return {
        "search_results": [{"content": results}],
        "next_node": "verifier_agent" if should_verify else "writer_agent"
    }


def verifier_agent_node(state: AgentState):
    """ê·œì • ê²€ì¦ ë° ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ (í†µí•©)"""
    query = state["query"]
    search_results = state.get("search_results", [])
    context = "\n".join([r["content"] for r in search_results]) if search_results else "ì°¸ì¡°í•  ê·œì • ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    model_name = state["model_name"]
    client = _agent["client"]
    
    print(f"âš–ï¸ [VerifierAgent] ê·œì • ê²€ì¦ ë° ë‹µë³€ ìƒì„± ì¤‘")
    
    prompt = f"""{AGENT_SYSTEM_PROMPT}

[ê·œì • ì»¨í…ìŠ¤íŠ¸]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ê·œì •ì„ ë©´ë°€íˆ ë¶„ì„í•˜ì—¬, ì´ë¯¸ì§€ì™€ ê°™ì´ **ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ê²€ì¦ ê²°ê³¼**ë¥¼ ì‘ì„±í•˜ì„¸ìš”. 
ê·œì • ì¡°í•­ì˜ ë¬¸êµ¬ë¥¼ ì§ì ‘ ì¸ìš©í•˜ë©° ì„¤ë“ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
"""

    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2500,
        temperature=0.3
    )
    
    msg_obj = response.choices[0].message
    content = getattr(msg_obj, 'content', "") or ""
    reasoning = getattr(msg_obj, 'reasoning_content', "") or ""
    
    final_answer = content if content else (f"[ë¶„ì„ ë‚´ìš©]\n{reasoning}" if reasoning else "[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    return {"answer": final_answer, "reasoning": reasoning, "next_node": "end"}


def list_agent_node(state: AgentState):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì „ë¬¸ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    print(f"ğŸ“š [ListAgent] ì „ì²´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘")
    docs_info = list_all_documents.invoke({})
    return {"search_results": [{"content": docs_info}], "next_node": "writer_agent"}


def writer_agent_node(state: AgentState):
    """ì¼ë°˜ ë‹µë³€ ìƒì„± ë…¸ë“œ (í†µí•©)"""
    query = state["query"]
    search_results = state.get("search_results", [])
    context = "\n".join([r["content"] for r in search_results]) if search_results else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
    model_name = state["model_name"]
    client = _agent["client"]
    
    print(f"âœï¸ [WriterAgent] ì¼ë°˜ ë‹µë³€ ì‘ì„± ì¤‘")
    
    prompt = f"""{AGENT_SYSTEM_PROMPT}

[ì°¸ê³  ê·œì •]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ **ì¶©ë¶„í•œ ì„¤ëª…ê³¼ êµ¬ì²´ì ì¸ ì¶œì²˜**ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ì´ë¯¸ì§€ì˜ í˜•ì‹(`**ê²€ì¦ ì˜ê²¬**`, `**ê²€ì¦ ê·¼ê±° ë° ì¶œì²˜**`)ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì„¸ìš”.
"""

    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2500,
        temperature=0.7
    )
    
    msg_obj = response.choices[0].message
    content = getattr(msg_obj, 'content', "") or ""
    reasoning = getattr(msg_obj, 'reasoning_content', "") or ""
    
    final_answer = content if content else (f"[ë¶„ì„ ë‚´ìš©]\n{reasoning}" if reasoning else "[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return {"answer": final_answer, "reasoning": reasoning, "next_node": "end"}


def run_agent(
    query: str,
    session_id: str = "default",
    model_name: str = "glm-4.7-flash"
) -> Dict[str, Any]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ë…¸ë“œ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)"""
    global _agent
    
    # ì—ì´ì „íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if _agent is None:
        create_agent(model_name)
    
    # ì´ˆê¸° ìƒíƒœ
    state: AgentState = {
        "messages": [],
        "query": query,
        "next_node": "orchestrator",
        "search_results": [],
        "verification": "",
        "answer": "",
        "tool_calls": [],
        "session_id": session_id,
        "model_name": model_name
    }
    
    try:
        # 1. Orchestrator
        res = orchestrator_node(state)
        state.update(res)
        
        # 2. Search (í•„ìš” ì‹œ)
        if state["next_node"] == "search_agent":
            res = search_agent_node(state)
            state.update(res)
            state["tool_calls"].append({"tool": "search_sop_documents", "input": query})
        
        # 3. List (í•„ìš” ì‹œ)
        elif state["next_node"] == "list_agent":
            res = list_agent_node(state)
            state.update(res)
            state["tool_calls"].append({"tool": "list_all_documents", "input": ""})
            # ListAgentì—ì„œ ë°”ë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë¯€ë¡œ Writer í˜¸ì¶œ ë¶ˆí•„ìš” (í˜„ì¬ êµ¬í˜„ìƒ)
            state["answer"] = res.get("search_results", [{}])[0].get("content", "")
            state["next_node"] = "end"

        # 4. Verifier / Writer (ë¶„ê¸°í•˜ì—¬ í•˜ë‚˜ë§Œ ì‹¤í–‰)
        if state["next_node"] == "verifier_agent":
            res = verifier_agent_node(state)
            state.update(res)
            state["tool_calls"].append({"tool": "verifier_agent", "input": "compliance_check"})
        elif state["next_node"] == "writer_agent":
            res = writer_agent_node(state)
            state.update(res)
        
        return {
            "answer": state["answer"],
            "tool_calls": state["tool_calls"],
            "session_id": session_id,
            "success": True,
            "reasoning": state.get("reasoning", "")
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "answer": f"âŒ ë©€í‹° ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "tool_calls": state.get("tool_calls", []),
            "session_id": session_id,
            "success": False
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš©ë²•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ¤– ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (v11.0)")
    print("="*60)
    
    # ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë° ë…¸ë“œ ë¡œë“œ ìƒíƒœ í™•ì¸
    print("\nâœ… ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë° ë…¸ë“œ ë¡œë“œ ì™„ë£Œ!")
    print(f"   - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: {orchestrator_node.__name__}")
    print(f"   - ê²€ìƒ‰ ì—ì´ì „íŠ¸: {search_agent_node.__name__}")
    print(f"   - ê²€ì¦ ì—ì´ì „íŠ¸: {verifier_agent_node.__name__}")
    print(f"   - ë¼ì´í„° ì—ì´ì „íŠ¸: {writer_agent_node.__name__}")
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    print("\nğŸ’¡ ì‚¬ìš©ë²•:")
    print("   from rag.agent import run_agent, init_agent_tools")
    print("   init_agent_tools(vector_store, graph_store)")
    print("   # ì˜ˆì‹œ í˜¸ì¶œ:")
    print("   # result = run_agent(\"í’ˆì§ˆê´€ë¦¬ì±…ì„ìì˜ ì—­í• ì´ ë­ì•¼? ê·œì •ì— ë§ëŠ”ì§€ ê²€ì¦í•´ì¤˜.\")")
    print("   # print(result['answer'])")
    print("="*60)
