"""
ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ v8.0

ğŸ”¥ ìƒˆë¡œìš´ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸:
1. ë³€í™˜: PDF, DOCX, HTML â†’ Markdown
2. ë¶„í• : MarkdownHeaderTextSplitterë¡œ í—¤ë” ê¸°ì¤€ 1ì°¨ ë¶„í• 
3. ìµœì í™”: RecursiveCharacterTextSplitterë¡œ ê¸´ ì„¹ì…˜ ì¬ë¶„í• 
4. ë©”íƒ€ë°ì´í„°: ìƒìœ„ ì œëª©, í˜ì´ì§€ ë²ˆí˜¸ ë“± ì €ì¥
"""

import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from io import BytesIO


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„°"""
    text: str
    index: int = 0
    metadata: Dict = field(default_factory=dict)


@dataclass
class ProcessedDocument:
    """ì²˜ë¦¬ëœ ë¬¸ì„œ"""
    markdown: str
    chunks: List[Chunk]
    metadata: Dict
    

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ë‹¨ê³„: ë¬¸ì„œ â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def convert_to_markdown(filename: str, content: bytes) -> Tuple[str, Dict]:
    """
    ë¬¸ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    
    Returns:
        (markdown_text, metadata)
    """
    # í™•ì¥ì ê°ì§€ (ë§ˆì§€ë§‰ í™•ì¥ì ê¸°ì¤€)
    ext = filename.split('.')[-1].lower()
    if ext == 'docx':
        return _docx_to_markdown(filename, content)
    elif ext == 'pdf':
        return _pdf_to_markdown(filename, content)
    elif ext in ['html', 'htm']:
        return _html_to_markdown(filename, content)
    elif ext == 'md':
        text = content.decode('utf-8', errors='ignore')
        return text, {"file_name": filename, "file_type": ".md"}
    elif ext == 'txt':
        text = content.decode('utf-8', errors='ignore')
        return _text_to_markdown(text), {"file_name": filename, "file_type": ".txt"}
    else:
        # ê¸°ë³¸: í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        text = content.decode('utf-8', errors='ignore')
        return text, {"file_name": filename, "file_type": "unknown"}


def _docx_to_markdown(filename: str, content: bytes) -> Tuple[str, Dict]:
    """DOCX â†’ Markdown ë³€í™˜"""
    from docx import Document
    from docx.shared import Pt
    
    doc = Document(BytesIO(content))
    md_lines = []
    metadata = {"file_name": filename, "file_type": ".docx"}
    
    # SOP ID ì¶”ì¶œ
    sop_pattern = re.compile(r'((?:EQ-)?SOP[-_]?\d{4,5})', re.IGNORECASE)
    
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            md_lines.append("")
            continue
        
        # SOP ID ì¶”ì¶œ
        sop_match = sop_pattern.search(text)
        if sop_match and "sop_id" not in metadata:
            sop_id = sop_match.group(1).upper().replace('_', '-')
            if not sop_id.startswith('EQ-'):
                sop_id = 'EQ-' + sop_id
            metadata["sop_id"] = sop_id
        
        # í—¤ë” ë ˆë²¨ ê²°ì •
        header_level = _detect_header_level(para, text)
        
        if header_level:
            md_lines.append(f"{'#' * header_level} {text}")
        else:
            md_lines.append(text)
    
    # í…Œì´ë¸” ì²˜ë¦¬
    for table in doc.tables:
        md_lines.append("")
        md_lines.append(_table_to_markdown(table))
    
    return '\n'.join(md_lines), metadata


def _detect_header_level(para, text: str) -> Optional[int]:
    """
    í—¤ë” ë ˆë²¨ ê°ì§€
    
    Word ìŠ¤íƒ€ì¼ + íŒ¨í„´ ê¸°ë°˜ ê°ì§€
    """
    # 1. Word ìŠ¤íƒ€ì¼ ê¸°ë°˜
    style_name = para.style.name.lower() if para.style else ""
    if 'heading 1' in style_name or 'title' in style_name:
        return 1
    if 'heading 2' in style_name:
        return 2
    if 'heading 3' in style_name:
        return 3
    if 'heading 4' in style_name:
        return 4
    
    # 2. íŒ¨í„´ ê¸°ë°˜ ê°ì§€
    # ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ (H2)
    main_sections = [
        'ëª©ì ', 'Purpose', 'ì ìš© ë²”ìœ„', 'Scope', 'ì •ì˜', 'Definitions',
        'ì±…ì„', 'Responsibilities', 'ì ˆì°¨', 'Procedure', 
        'ì°¸ê³ ë¬¸í—Œ', 'Reference', 'ì²¨ë¶€', 'Attachments'
    ]
    
    for section in main_sections:
        if text.startswith(section) or re.match(rf'^\d+\s+{section}', text):
            return 2
    
    # ìˆ«ìí˜• í—¤ë”
    # 5.1.1 xxx â†’ H4
    if re.match(r'^\d+\.\d+\.\d+\s+', text):
        return 4
    # 5.1 xxx â†’ H3
    if re.match(r'^\d+\.\d+\s+', text):
        return 3
    # 5. xxx ë˜ëŠ” 5 xxx â†’ H2 (ì  ìˆê±°ë‚˜ ì—†ê±°ë‚˜, ë’¤ì— ì˜ë¬¸ ë™ë°˜ ê°€ëŠ¥)
    if re.match(r'^\d+\.?\s+([ê°€-í£A-Za-z].+)', text):
        return 2
    
    # ì†Œì œëª© íŒ¨í„´ (í•œê¸€ + ì˜ë¬¸ ê´„í˜¸) â†’ H3
    if re.match(r'^[ê°€-í£A-Z][ê°€-í£\s\(\)/Â·\-]+\s*\([A-Za-z\s&/\-:]+\)\s*$', text):
        return 3
    
    return None


def _table_to_markdown(table) -> str:
    """Word í…Œì´ë¸” â†’ Markdown í…Œì´ë¸”"""
    rows = []
    for row in table.rows:
        cells = [cell.text.strip().replace('\n', ' ') for cell in row.cells]
        rows.append(cells)
    
    if not rows:
        return ""
    
    md_lines = []
    # í—¤ë”
    md_lines.append("| " + " | ".join(rows[0]) + " |")
    md_lines.append("| " + " | ".join(["---"] * len(rows[0])) + " |")
    # ë³¸ë¬¸
    for row in rows[1:]:
        while len(row) < len(rows[0]):
            row.append("")
        md_lines.append("| " + " | ".join(row[:len(rows[0])]) + " |")
    
    return '\n'.join(md_lines)


def _pdf_to_markdown(filename: str, content: bytes) -> Tuple[str, Dict]:
    """PDF â†’ Markdown ë³€í™˜"""
    metadata = {"file_name": filename, "file_type": ".pdf"}
    
    try:
        # Docling ì‚¬ìš© (ìµœê³  í’ˆì§ˆ)
        from docling.document_converter import DocumentConverter
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as f:
            f.write(content)
            temp_path = f.name
        
        try:
            converter = DocumentConverter()
            result = converter.convert(temp_path)
            markdown = result.document.export_to_markdown()
            metadata["parser"] = "docling"
        finally:
            os.unlink(temp_path)
        
        return markdown, metadata
        
    except ImportError:
        pass
    
    try:
        # PyMuPDF (fitz) ì‚¬ìš©
        import fitz
        
        pdf = fitz.open(stream=content, filetype="pdf")
        md_lines = []
        
        for page_num, page in enumerate(pdf):
            text = page.get_text()
            if text.strip():
                md_lines.append(f"<!-- Page {page_num + 1} -->")
                md_lines.append(text)
        
        metadata["parser"] = "pymupdf"
        return '\n'.join(md_lines), metadata
        
    except ImportError:
        pass
    
    try:
        # PyPDF2 fallback
        from PyPDF2 import PdfReader
        
        reader = PdfReader(BytesIO(content))
        md_lines = []
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text() or ''
            if text.strip():
                md_lines.append(f"<!-- Page {i + 1} -->")
                md_lines.append(text)
        
        metadata["parser"] = "pypdf2"
        return '\n'.join(md_lines), metadata
        
    except ImportError:
        pass
    
    try:
        # OCR fallback (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)
        md_lines, ocr_success = _pdf_to_markdown_ocr(content)
        if ocr_success:
            metadata["parser"] = "ocr"
            return '\n'.join(md_lines), metadata
        
    except ImportError:
        pass
    
    return "[PDF íŒŒì‹± ì‹¤íŒ¨: ëª¨ë“  ë°©ë²• ì‹œë„í–ˆìœ¼ë‚˜ ì‹¤íŒ¨]", metadata


def _pdf_to_markdown_ocr(content: bytes) -> Tuple[List[str], bool]:
    """
    PDF OCR ì²˜ë¦¬ (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)
    
    Returns:
        (md_lines, success)
    """
    try:
        import pytesseract
        from pdf2image import convert_from_bytes
        from PIL import Image
        
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        # í™˜ê²½ë³€ìˆ˜ PATHì— popplerê°€ ë“±ë¡ë˜ì–´ ìˆì–´ì•¼ í•¨
        pages = convert_from_bytes(content, dpi=300)
        
        md_lines = []
        for i, page in enumerate(pages):
            # OCR ì‹¤í–‰ (í•œê¸€+ì˜ì–´)
            text = pytesseract.image_to_string(page, lang='kor+eng')
            
            if text.strip():
                md_lines.append(f"<!-- Page {i + 1} (OCR) -->")
                md_lines.append(text)
        
        return md_lines, len(md_lines) > 0
        
    except Exception as e:
        print(f"OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return [], False


def _html_to_markdown(filename: str, content: bytes) -> Tuple[str, Dict]:
    """HTML â†’ Markdown ë³€í™˜"""
    from bs4 import BeautifulSoup
    
    html = content.decode('utf-8', errors='ignore')
    soup = BeautifulSoup(html, 'html.parser')
    
    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    md_lines = []
    
    # ì œëª©
    title = soup.title.string if soup.title else filename
    md_lines.append(f"# {title}")
    md_lines.append("")
    
    # í—¤ë” íƒœê·¸ â†’ ë§ˆí¬ë‹¤ìš´ í—¤ë”
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
        if tag.name.startswith('h'):
            level = int(tag.name[1])
            md_lines.append(f"{'#' * level} {tag.get_text(strip=True)}")
        elif tag.name == 'li':
            md_lines.append(f"- {tag.get_text(strip=True)}")
        else:
            text = tag.get_text(strip=True)
            if text:
                md_lines.append(text)
        md_lines.append("")
    
    metadata = {"file_name": filename, "file_type": ".html", "title": title}
    return '\n'.join(md_lines), metadata


def _text_to_markdown(text: str) -> str:
    """
    ì¼ë°˜ í…ìŠ¤íŠ¸ â†’ ë§ˆí¬ë‹¤ìš´ (í—¤ë” ì¶”ë¡ )
    """
    lines = text.split('\n')
    md_lines = []
    
    for line in lines:
        stripped = line.strip()
        if not stripped:
            md_lines.append("")
            continue
        
        # í—¤ë” íŒ¨í„´ ê°ì§€
        # ì£¼ìš” ì„¹ì…˜
        main_sections = ['ëª©ì ', 'ì ìš© ë²”ìœ„', 'ì •ì˜', 'ì±…ì„', 'ì ˆì°¨', 'ì°¸ê³ ë¬¸í—Œ', 'ì²¨ë¶€']
        is_header = False
        
        for section in main_sections:
            if stripped.startswith(section):
                md_lines.append(f"## {stripped}")
                is_header = True
                break
        
        if not is_header:
            # ìˆ«ì í—¤ë”
            if re.match(r'^\d+\.\d+\.\d+\s+', stripped):
                md_lines.append(f"#### {stripped}")
            elif re.match(r'^\d+\.\d+\s+', stripped):
                md_lines.append(f"### {stripped}")
            elif re.match(r'^\d+\.?\s+[ê°€-í£A-Za-z]', stripped):
                md_lines.append(f"## {stripped}")
            else:
                md_lines.append(stripped)
    
    return '\n'.join(md_lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ ë¶„í• 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_by_headers(markdown: str) -> List[Dict]:
    """
    ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    
    Returns:
        [
            {
                "content": "ì„¹ì…˜ ë‚´ìš©",
                "headers": {"H1": "ë¬¸ì„œ ì œëª©", "H2": "ì ˆì°¨", "H3": "5.1 xxx"},
                "header_path": "ì ˆì°¨ > 5.1 xxx"
            },
            ...
        ]
    """
    lines = markdown.split('\n')
    sections = []
    
    # í˜„ì¬ í—¤ë” ìŠ¤íƒ
    current_headers = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}
    current_content = []
    
    def flush_section():
        nonlocal current_content
        if current_content:
            content = '\n'.join(current_content).strip()
            if content:
                # í—¤ë” ê²½ë¡œ ìƒì„±
                header_path_parts = []
                headers_dict = {}
                for level in range(1, 7):
                    if current_headers[level]:
                        headers_dict[f"H{level}"] = current_headers[level]
                        if level >= 2:  # H2ë¶€í„° ê²½ë¡œì— í¬í•¨
                            header_path_parts.append(current_headers[level])
                
                sections.append({
                    "content": content,
                    "headers": headers_dict,
                    "header_path": " > ".join(header_path_parts) if header_path_parts else None
                })
        current_content = []
    
    for line in lines:
        # í—¤ë” ê°ì§€
        header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        
        if header_match:
            flush_section()
            
            level = len(header_match.group(1))
            header_text = header_match.group(2).strip()
            
            # í—¤ë” ìŠ¤íƒ ì—…ë°ì´íŠ¸
            current_headers[level] = header_text
            # í•˜ìœ„ ë ˆë²¨ ì´ˆê¸°í™”
            for l in range(level + 1, 7):
                current_headers[l] = None
            
            # í—¤ë”ë„ ì»¨í…ì¸ ì— í¬í•¨
            current_content.append(line)
        else:
            current_content.append(line)
    
    flush_section()
    return sections


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ë‹¨ê³„: ê¸´ ì„¹ì…˜ ì¬ë¶„í•  (RecursiveCharacterTextSplitter ìŠ¤íƒ€ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_recursive(
    text: str,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    separators: List[str] = None
) -> List[str]:
    """
    RecursiveCharacterTextSplitter ìŠ¤íƒ€ì¼ ë¶„í• 
    
    ğŸ”¥ v8.1 ê°œì„ :
    - í…Œì´ë¸” í–‰ ë‹¨ìœ„ ë¶„í•  ì§€ì› ("\n| ")
    - í…Œì´ë¸” êµ¬ì¡° ë³´ì¡´
    - í…Œì´ë¸” ë‚´ë¶€ì—ì„œëŠ” overlap ë¹„í™œì„±í™”
    """
    if separators is None:
        # ğŸ”¥ í…Œì´ë¸” í–‰ êµ¬ë¶„ì ì¶”ê°€ (í‘œ í•œë³µíŒ ë¶„í•  ë°©ì§€)
        separators = [
            "\n\n",      # ë¬¸ë‹¨ êµ¬ë¶„
            "\n| ",      # ğŸ”¥ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í–‰ (í‘œ ë³´ì¡´)
            "\n",        # ì¤„ë°”ê¿ˆ
            ". ",        # ë¬¸ì¥
            "ã€‚",        # í•œêµ­ì–´/ì¼ë³¸ì–´ ë¬¸ì¥
            " ",         # ë‹¨ì–´
            ""           # ë¬¸ì
        ]
    
    if len(text) <= chunk_size:
        return [text]
    
    # ğŸ”¥ í…Œì´ë¸”ì¸ì§€ í™•ì¸ (í…Œì´ë¸”ì´ë©´ overlap ë¹„í™œì„±í™”)
    is_table = text.strip().startswith('|') or '\n|' in text
    effective_overlap = 0 if is_table else chunk_overlap
    
    chunks = []
    
    # ì ì ˆí•œ êµ¬ë¶„ì ì°¾ê¸°
    for sep in separators:
        if sep in text:
            parts = text.split(sep)
            current_chunk = ""
            
            for part in parts:
                if len(current_chunk) + len(part) + len(sep) <= chunk_size:
                    if current_chunk:
                        current_chunk += sep + part
                    else:
                        current_chunk = part
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    
                    # íŒŒíŠ¸ê°€ ë„ˆë¬´ í¬ë©´ ì¬ê·€ ë¶„í• 
                    if len(part) > chunk_size:
                        sub_chunks = split_recursive(
                            part, chunk_size, chunk_overlap,
                            separators[separators.index(sep) + 1:]
                        )
                        chunks.extend(sub_chunks)
                        current_chunk = ""
                    else:
                        current_chunk = part
            
            if current_chunk:
                chunks.append(current_chunk)
            
            # ì˜¤ë²„ë© ì ìš© (í…Œì´ë¸”ì´ ì•„ë‹ ë•Œë§Œ)
            if effective_overlap > 0 and len(chunks) > 1:
                chunks = _apply_overlap(chunks, effective_overlap)
            
            return chunks
    
    # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ê°•ì œ ë¶„í• 
    step = chunk_size - effective_overlap if effective_overlap > 0 else chunk_size
    return [text[i:i+chunk_size] for i in range(0, len(text), step)]


def _apply_overlap(chunks: List[str], overlap: int) -> List[str]:
    """ì²­í¬ ê°„ ì˜¤ë²„ë© ì ìš©"""
    if len(chunks) <= 1:
        return chunks
    
    result = [chunks[0]]
    for i in range(1, len(chunks)):
        prev_end = chunks[i-1][-overlap:] if len(chunks[i-1]) > overlap else chunks[i-1]
        result.append(prev_end + chunks[i])
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ë‹¨ê³„: ì²­í¬ ìƒì„± ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_chunks(
    sections: List[Dict],
    metadata: Dict,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    add_context_prefix: bool = True  # ğŸ”¥ ì»¨í…ìŠ¤íŠ¸ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€ ì˜µì…˜
) -> List[Chunk]:
    """
    ì„¹ì…˜ì„ ì²­í¬ë¡œ ë³€í™˜í•˜ê³  ë©”íƒ€ë°ì´í„° ì¶”ê°€
    
    ğŸ”¥ v8.1 ê°œì„ :
    - add_context_prefix: ì¬ë¶„í• ëœ ì²­í¬ì— í—¤ë” ê²½ë¡œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‚½ì…
      â†’ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ, í™˜ê° ê°ì†Œ
    """
    chunks = []
    idx = 0
    
    sop_id = metadata.get("sop_id")
    doc_name = metadata.get("file_name")
    
    for section in sections:
        content = section["content"]
        headers = section.get("headers", {})
        header_path = section.get("header_path")
        
        # ì„¹ì…˜ì´ ë„ˆë¬´ ê¸¸ë©´ ì¬ë¶„í• 
        if len(content) > chunk_size:
            text_chunks = split_recursive(content, chunk_size, chunk_overlap)
            is_split = len(text_chunks) > 1
        else:
            text_chunks = [content]
            is_split = False
        
        for i, text in enumerate(text_chunks):
            if not text.strip():
                continue
            
            # ğŸ”¥ ì¬ë¶„í• ëœ ì²­í¬(2ë²ˆì§¸ë¶€í„°)ì— ì»¨í…ìŠ¤íŠ¸ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
            if add_context_prefix and is_split and i > 0 and header_path:
                context_prefix = f"[Context: {header_path}]\n\n"
                text = context_prefix + text
            
            # ì„¹ì…˜ íƒ€ì… ê²°ì •
            section_type = "text"
            section_num = None
            
            if headers.get("H4"):
                section_type = "subsubsection"
                match = re.match(r'^(\d+\.\d+\.\d+)', headers["H4"])
                if match:
                    section_num = match.group(1)
            elif headers.get("H3"):
                section_type = "subsection"
                match = re.match(r'^(\d+\.\d+)', headers["H3"])
                if match:
                    section_num = match.group(1)
            elif headers.get("H2"):
                section_type = "section"
                match = re.match(r'^(\d+)', headers["H2"])
                if match:
                    section_num = match.group(1)
            
            # ê°€ì¥ ë‚®ì€ ë ˆë²¨ í—¤ë”ë¥¼ sectionìœ¼ë¡œ ì‚¬ìš©
            section_display = headers.get("H4") or headers.get("H3") or headers.get("H2") or headers.get("H1")
            
            chunks.append(Chunk(
                text=text.strip(),
                index=idx,
                metadata={
                    "doc_name": doc_name,
                    "doc_title": sop_id or doc_name,
                    "sop_id": sop_id,
                    "article_num": section_num,
                    "article_type": section_type,
                    "section": section_display,
                    "section_path": header_path,
                    "section_path_readable": header_path,
                    "H1": headers.get("H1"),
                    "H2": headers.get("H2"),
                    "H3": headers.get("H3"),
                    "H4": headers.get("H4"),
                    "chunk_part": i + 1 if is_split else None,  # ğŸ”¥ ë¶„í• ëœ ì²­í¬ ë²ˆí˜¸
                    "total_parts": len(text_chunks) if is_split else None,
                }
            ))
            idx += 1
    
    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_document(
    filename: str,
    content: bytes,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    debug: bool = False
) -> ProcessedDocument:
    """
    ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
    
    1. ë³€í™˜: ë¬¸ì„œ â†’ ë§ˆí¬ë‹¤ìš´
    2. ë¶„í• : í—¤ë” ê¸°ì¤€ 1ì°¨ ë¶„í• 
    3. ìµœì í™”: ê¸´ ì„¹ì…˜ ì¬ë¶„í• 
    4. ë©”íƒ€ë°ì´í„°: ì²­í¬ ìƒì„±
    """
    # 1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    markdown, metadata = convert_to_markdown(filename, content)
    
    if debug:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ 1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³€í™˜")
        print(f"   íŒŒì¼: {filename}")
        print(f"   ë§ˆí¬ë‹¤ìš´ ê¸¸ì´: {len(markdown)}")
        print(f"   ë©”íƒ€ë°ì´í„°: {metadata}")
        print(f"\n   ë§ˆí¬ë‹¤ìš´ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):")
        print(f"   {'-'*50}")
        for line in markdown[:500].split('\n'):
            print(f"   {line}")
    
    # 2ë‹¨ê³„: í—¤ë” ê¸°ì¤€ ë¶„í• 
    sections = split_by_headers(markdown)
    
    if debug:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ 2ë‹¨ê³„: í—¤ë” ê¸°ì¤€ ë¶„í• ")
        print(f"   ì„¹ì…˜ ìˆ˜: {len(sections)}")
        print(f"\n   ì„¹ì…˜ ëª©ë¡:")
        for i, sec in enumerate(sections[:10]):
            path = sec.get('header_path', 'N/A')
            content_preview = sec['content'][:50].replace('\n', ' ')
            print(f"   [{i}] ğŸ“ {path}")
            print(f"       {content_preview}...")
    
    # 3ë‹¨ê³„ & 4ë‹¨ê³„: ì²­í¬ ìƒì„±
    chunks = create_chunks(sections, metadata, chunk_size, chunk_overlap)
    
    if debug:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ 3-4ë‹¨ê³„: ì²­í¬ ìƒì„±")
        print(f"   ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"\n   ì²­í¬ ìƒ˜í”Œ:")
        for chunk in chunks[:5]:
            path = chunk.metadata.get('section_path_readable', 'N/A')
            print(f"   [{chunk.index}] ğŸ“ {path}")
            print(f"       {chunk.text[:60]}...")
    
    return ProcessedDocument(
        markdown=markdown,
        chunks=chunks,
        metadata=metadata
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´
    test_md = """# EQ-SOP-00010 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œ

## 1 ëª©ì  Purpose

ë³¸ ê¸°ì¤€ì„œëŠ” í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ ì‘ì„±, ê²€í† , ìŠ¹ì¸ì— ê´€í•œ ê¸°ì¤€ì„ ì •í•œë‹¤.

## 2 ì ìš© ë²”ìœ„ Scope

ë³¸ ê¸°ì¤€ì„œëŠ” íšŒì‚¬ ë‚´ í’ˆì§ˆê´€ë¦¬ í™œë™ ì „ë°˜ì— ì ìš©ëœë‹¤.

## 5 ì ˆì°¨ Procedure

### 5.1 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ êµ¬ì„± ë° ê´€ë¦¬

í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œëŠ” ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œë‹¤.

#### 5.1.1 ë¬¸ì„œë²ˆí˜¸ ì²´ê³„

ë¬¸ì„œë²ˆí˜¸ëŠ” EQ-SOP-XXXXX í˜•ì‹ì„ ë”°ë¥¸ë‹¤.

#### 5.1.2 ê°œì • ê´€ë¦¬

ê°œì • ì‹œ ë³€ê²½ ì´ë ¥ì„ ê¸°ë¡í•œë‹¤.

### 5.2 ì‹œí—˜ë°©ë²•ì„œì™€ì˜ ì—°ê³„

ì‹œí—˜ë°©ë²•ì„œëŠ” í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì™€ ì—°ê³„ë˜ì–´ì•¼ í•œë‹¤.
"""
    
    # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    result = process_document(
        "test.md",
        test_md.encode('utf-8'),
        chunk_size=300,
        debug=True
    )
    
    print(f"\n{'='*60}")
    print(f"âœ… ìµœì¢… ê²°ê³¼")
    print(f"   ì´ ì²­í¬: {len(result.chunks)}")
