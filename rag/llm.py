"""
LLM ëª¨ë“ˆ v7.0 - Z.AI + Ollama + HuggingFace
"""

import os
import re
import torch
import requests
import time
import random
from typing import Dict, List, Optional, Any


device = "cuda" if torch.cuda.is_available() else "cpu"
_loaded_llm: Dict[str, Any] = {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Z.AI ë°±ì—”ë“œ (GLM-4.7-Flash)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ZaiLLM:
    """Z.AI GLM-4.7 API"""

    def __init__(self, model: str = "glm-4.7-flash", api_key: str = None):
        self.model = model
        self.api_key = api_key or os.getenv("ZAI_API_KEY", "")
        self._client = None
    
    def _get_client(self):
        """ZaiClient ì§€ì—° ë¡œë”©"""
        if self._client is None:
            try:
                from zai import ZaiClient
                self._client = ZaiClient(api_key=self.api_key)
            except ImportError:
                raise ImportError("zai-sdk íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install zai-sdk")
        return self._client

    def generate(
        self,
        prompt: str,
        system: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048  # ğŸ”¥ ê¸°ë³¸ í† í° ìƒí–¥
    ) -> str:
        """Z.AI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        # API í‚¤ í™•ì¸
        if not self.api_key or "your-api-key" in self.api_key:
            print("âš ï¸ ZAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return "âŒ ì˜¤ë¥˜: ZAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        client = self._get_client()
        
        max_retries = 3
        base_delay = 2  # ì´ˆ
        
        for attempt in range(max_retries):
            try:
                messages = []
                if system:
                    messages.append({"role": "system", "content": system})
                messages.append({"role": "user", "content": prompt})

                print(f"ğŸš€ Z.AI API í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.model}, MaxTokens: {max_tokens}, ì‹œë„: {attempt+1})")
                
                response = client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if not response.choices:
                    print(f"âš ï¸ Z.AI ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤: {response}")
                    return "âŒ ì˜¤ë¥˜: Z.AIë¡œë¶€í„° ì ì ˆí•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    
                msg_obj = response.choices[0].message
                content = getattr(msg_obj, 'content', "") or ""
                reasoning = getattr(msg_obj, 'reasoning_content', "") or ""
                
                # ë³¸ë¬¸(content)ì´ ë¹„ì–´ìˆëŠ”ë° reasoning_contentë§Œ ìˆëŠ” ê²½ìš°
                if not content and reasoning:
                    # ğŸ”¥ ë°©ë²• 1: ì™„ì „í•œ JSON ë¸”ë¡ ì°¾ê¸° ({ ... })
                    json_match = re.search(r'(\{.*\})', reasoning, re.DOTALL)
                    if json_match:
                        content = json_match.group(1)
                        print(f"âœ… [Recall] Reasoningì—ì„œ ì™„ì „í•œ JSON ë³µêµ¬ ({len(content)}ì)")
                    else:
                        # ğŸ”¥ ë°©ë²• 2: ì˜ë¦° JSONì´ë¼ë„ ì‹œì‘ ë¶€ë¶„ì´ë¼ë„ ì°¾ê¸°
                        start_idx = reasoning.find('{')
                        if start_idx != -1:
                            content = reasoning[start_idx:]
                            if not content.strip().endswith('}'):
                                content = content.strip() + '"}'
                            print(f"âš ï¸ [Recall] ì˜ë¦° JSON ê°•ì œ ë³µêµ¬ ì‹œë„")
                        else:
                            content = "âŒ ë‹µë³€ ìƒì„± ì¤‘ í† í° í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤."
                
                return content

            except Exception as e:
                error_msg = str(e)
                # í• ë‹¹ëŸ‰ ì´ˆê³¼(429) ë˜ëŠ” ë†’ì€ ë™ì‹œì„± ì˜¤ë¥˜(1302) ì‹œ ì¬ì‹œë„
                if "429" in error_msg or "1302" in error_msg or "Rate limit" in error_msg or "too many" in error_msg.lower():
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° (ì„œë²„ ë¶€í•˜ ë¶„ì‚°)
                    delay = (base_delay * (2 ** attempt)) + random.uniform(1, 3)
                    print(f"âš ï¸ API í•œë„ ì´ˆê³¼/ê³¼ë¶€í•˜ ê°ì§€. {delay:.1f}ì´ˆ í›„ ìë™ ì¬ì‹œë„... ({attempt+1}/{max_retries})")
                    time.sleep(delay)
                    continue
                else:
                    print(f"âŒ Z.AI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                    return f"âŒ AI í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}"
        
        return "âŒ ì˜¤ë¥˜: ì—¬ëŸ¬ ë²ˆì˜ ì¬ì‹œë„ í›„ì—ë„ AI ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (API í• ë‹¹ëŸ‰ ì´ˆê³¼)"

    def generate_stream(
        self,
        prompt: str,
        system: str = None,
        temperature: float = 0.7,
        max_tokens: int = 512
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        client = self._get_client()
        
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        response = client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=True,
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    @staticmethod
    def is_available() -> bool:
        """API í‚¤ ì„¤ì • ì—¬ë¶€"""
        return bool(os.getenv("ZAI_API_KEY"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ollama ë°±ì—”ë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OllamaLLM:
    """Ollama ë„¤ì´í‹°ë¸Œ API"""

    def __init__(self, model: str = "qwen2.5:3b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url

    def generate(
        self,
        prompt: str,
        system: str = None,
        temperature: float = 0.1,
        max_tokens: int = 256
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            return self._call_chat_api(prompt, system, temperature, max_tokens)
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return self._call_generate_api(prompt, system, temperature, max_tokens)
            raise
        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨. 'ollama serve' ì‹¤í–‰ í•„ìš”")

    def _call_chat_api(
        self,
        prompt: str,
        system: str,
        temperature: float,
        max_tokens: int
    ) -> str:
        """/api/chat ì—”ë“œí¬ì¸íŠ¸"""
        messages = []
        if system:
            messages.append({"role": "system", "content": system})

        # Qwen3 thinking ëª¨ë“œ ë¹„í™œì„±í™”
        final_prompt = f"/no_think {prompt}" if "qwen3" in self.model.lower() else prompt
        messages.append({"role": "user", "content": final_prompt})

        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {"temperature": temperature, "num_predict": max_tokens},
                "think": False,
            },
            timeout=120
        )
        response.raise_for_status()
        data = response.json()

        message = data.get("message", {})
        content = message.get("content", "") if isinstance(message, dict) else str(message)

        # thinking fallback
        if not content and isinstance(message, dict) and message.get("thinking"):
            content = message.get("thinking", "")

        return content

    def _call_generate_api(
        self,
        prompt: str,
        system: str,
        temperature: float,
        max_tokens: int
    ) -> str:
        """/api/generate ì—”ë“œí¬ì¸íŠ¸ (êµ¬ë²„ì „)"""
        full_prompt = f"{system}\n\n{prompt}" if system else prompt

        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": full_prompt,
                "stream": False,
                "options": {"temperature": temperature, "num_predict": max_tokens},
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("response", "")

    @staticmethod
    def list_models(base_url: str = "http://localhost:11434") -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.ok:
                return [m["name"] for m in response.json().get("models", [])]
        except Exception:
            pass
        return []

    @staticmethod
    def is_available(base_url: str = "http://localhost:11434") -> bool:
        """ì„œë²„ ì‹¤í–‰ ì—¬ë¶€"""
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=3)
            return response.ok
        except Exception:
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HuggingFace ë°±ì—”ë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_llm(model_name: str):
    """HuggingFace LLM ë¡œë“œ"""
    if model_name in _loaded_llm:
        return _loaded_llm[model_name]

    from transformers import AutoTokenizer, AutoModelForCausalLM

    print(f"ğŸ¤– Loading LLM: {model_name}...")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    dtype = torch.float16 if device == "cuda" else torch.float32

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map="auto" if device == "cuda" else None
    )

    if device == "cpu":
        model = model.to(device)
    model.eval()

    _loaded_llm[model_name] = (tokenizer, model)
    print(f"âœ… Loaded: {model_name}")
    return tokenizer, model


def generate_with_hf(
    prompt: str,
    model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
    max_new_tokens: int = 256,
    temperature: float = 0.1
) -> str:
    """HuggingFace ìƒì„±"""
    tokenizer, model = load_llm(model_name)

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            repetition_penalty=1.15,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )

    return tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):],
        skip_special_tokens=True
    ).strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© ì¸í„°í˜ì´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_llm_response(
    prompt: str,
    llm_model: str = "glm-4.7-flash",
    llm_backend: str = "zai",  # ğŸ”¥ ê¸°ë³¸ê°’ zaië¡œ ë³€ê²½
    max_tokens: int = 512,
    temperature: float = 0.7
) -> str:
    """í†µí•© LLM ì‘ë‹µ"""
    if llm_backend == "zai":
        llm = ZaiLLM(llm_model)
        return llm.generate(prompt, temperature=temperature, max_tokens=max_tokens)
    elif llm_backend == "ollama":
        llm = OllamaLLM(llm_model)
        return llm.generate(prompt, temperature=temperature, max_tokens=max_tokens)
    else:
        return generate_with_hf(prompt, llm_model, max_tokens, temperature)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ (ì—ì´ì „íŠ¸ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_search_results(results: List[Dict]) -> Dict:
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ - ë˜ë¬»ê¸° ì—¬ë¶€ íŒë‹¨"""
    if not results:
        return {'needs_clarification': False, 'options': [], 'unique_documents': []}

    doc_groups = {}

    for r in results:
        meta = r.get('metadata', {})
        doc_name = meta.get('doc_name', 'unknown')
        doc_title = meta.get('doc_title', doc_name)
        article_num = meta.get('article_num')
        article_type = meta.get('article_type', 'article')
        section = meta.get('section')  # ì œNì¡° í˜•ì‹
        score = r.get('similarity', 0)

        # ì„¹ì…˜ í‘œì‹œ
        section_display = section or ""
        if not section_display and article_num:
            if article_type == 'article':
                section_display = f"ì œ{article_num}ì¡°"
            elif article_type == 'chapter':
                section_display = f"ì œ{article_num}ì¥"
            elif article_type == 'section':
                section_display = f"ì œ{article_num}ì ˆ"
            else:
                section_display = str(article_num)

        if doc_name not in doc_groups:
            doc_groups[doc_name] = {
                'title': doc_title,
                'max_score': score,
                'sections': {section_display} if section_display else set(),
                'count': 1
            }
        else:
            doc_groups[doc_name]['max_score'] = max(doc_groups[doc_name]['max_score'], score)
            if section_display:
                doc_groups[doc_name]['sections'].add(section_display)
            doc_groups[doc_name]['count'] += 1

    unique_docs = list(doc_groups.keys())

    # ë˜ë¬»ê¸° íŒë³„: ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ë¹„ìŠ·í•œ ì ìˆ˜
    needs_clarification = False
    if len(unique_docs) > 1:
        scores = sorted([info['max_score'] for info in doc_groups.values()], reverse=True)
        if len(scores) >= 2 and (scores[0] - scores[1]) < 0.15:
            needs_clarification = True

    # ì„ íƒì§€
    options = []
    for d_name in unique_docs:
        info = doc_groups[d_name]
        sections_list = sorted(list(info['sections']))[:3]
        sections_str = f" ({', '.join(sections_list)})" if sections_list else ""

        options.append({
            "doc_name": d_name,
            "doc_title": info['title'],
            "display_text": f"{info['title']}{sections_str}",
            "score": info['max_score'],
            "sections": sections_list,
        })

    return {
        'needs_clarification': needs_clarification,
        'options': options,
        'unique_documents': unique_docs
    }


def generate_clarification_question(
    query: str,
    options: List[Dict],
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama"
) -> str:
    """ë˜ë¬»ê¸° ì§ˆë¬¸ ìƒì„±"""
    options_text = "\n".join([
        f"- {opt['display_text']} (ê´€ë ¨ë„: {opt['score']:.0%})"
        for opt in options
    ])

    prompt = f"""ì‚¬ìš©ìê°€ "{query}"ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
ê´€ë ¨ ë¬¸ì„œë“¤:
{options_text}

ì–´ë–¤ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í• ì§€ ì •ì¤‘í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.
í•œêµ­ì–´ë¡œ ì§§ê²Œ ì‘ë‹µí•˜ì„¸ìš”."""

    try:
        return get_llm_response(prompt, llm_model, llm_backend, max_tokens=200)
    except Exception:
        return f"'{query}'ì— ëŒ€í•´ ì—¬ëŸ¬ ê·œì •(SOP)ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì„œë¥¼ í™•ì¸í• ê¹Œìš”?\n\n{options_text}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¨ë¸ í”„ë¦¬ì…‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OLLAMA_MODELS = [
    {"key": "qwen2.5:0.5b", "name": "Qwen2.5-0.5B", "desc": "ì´ˆê²½ëŸ‰ (1GB)", "vram": "1GB"},
    {"key": "qwen2.5:1.5b", "name": "Qwen2.5-1.5B", "desc": "ê²½ëŸ‰ (2GB)", "vram": "2GB"},
    {"key": "qwen2.5:3b", "name": "Qwen2.5-3B", "desc": "ì¶”ì²œ (3GB)", "vram": "3GB"},
    {"key": "qwen2.5:7b", "name": "Qwen2.5-7B", "desc": "ê³ ì„±ëŠ¥ (5GB)", "vram": "5GB"},
    {"key": "qwen3:4b", "name": "Qwen3-4B", "desc": "ìµœì‹  (4GB)", "vram": "4GB"},
    {"key": "llama3.2:3b", "name": "Llama3.2-3B", "desc": "ê²½ëŸ‰ (3GB)", "vram": "3GB"},
    {"key": "gemma2:2b", "name": "Gemma2-2B", "desc": "ê²½ëŸ‰ (2GB)", "vram": "2GB"},
    {"key": "mistral:7b", "name": "Mistral-7B", "desc": "ì˜ì–´ íŠ¹í™” (5GB)", "vram": "5GB"},
]

HUGGINGFACE_MODELS = [
    {"key": "Qwen/Qwen2.5-0.5B-Instruct", "name": "Qwen2.5-0.5B", "desc": "ì´ˆê²½ëŸ‰"},
    {"key": "Qwen/Qwen2.5-1.5B-Instruct", "name": "Qwen2.5-1.5B", "desc": "ê²½ëŸ‰"},
    {"key": "Qwen/Qwen2.5-3B-Instruct", "name": "Qwen2.5-3B", "desc": "VRAM 6GB+"},
    {"key": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "name": "TinyLlama", "desc": "ì˜ì–´"},
]