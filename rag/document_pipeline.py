"""
LangGraph ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ v9.1

ğŸ”¥ v9.1 ê°œì„ :
- í˜ì´ì§€ ë²ˆí˜¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
- Parent-Child ê³„ì¸µ êµ¬ì¡° ë„ì…
- ë¬¸ì„œ í—¤ë” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (SOP ID, Version, Date, Department)
- ëª©ì°¨ ì¤‘ë³µ ì œê±°
- ë©”íƒ€ë°ì´í„° ìµœì í™” (ë¶ˆí•„ìš” í•„ë“œ ì œê±°)

ë…¸ë“œ íë¦„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load   â”‚â”€â”€â”€â–¶â”‚ Convert â”‚â”€â”€â”€â–¶â”‚ Validate â”‚â”€â”€â”€â–¶â”‚  Split  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚                â”‚
                    â–¼              â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Fallback â”‚  â”‚  Repair  â”‚    â”‚ Optimize â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Finalize â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

from typing import TypedDict, List, Dict, Optional, Literal, Annotated
from dataclasses import dataclass, field
import re
from io import BytesIO
import operator


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìƒíƒœ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PipelineState(TypedDict):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""
    # ì…ë ¥
    filename: str
    content: bytes
    
    # ì„¤ì •
    chunk_size: int
    chunk_overlap: int
    
    # ì¤‘ê°„ ê²°ê³¼
    file_type: str
    markdown: str
    metadata: Dict
    sections: List[Dict]
    chunks: List[Dict]
    
    # í’ˆì§ˆ ì§€í‘œ
    quality_score: float
    conversion_method: str
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: Annotated[List[str], operator.add]
    warnings: Annotated[List[str], operator.add]
    retry_count: int
    
    # ìµœì¢… ê²°ê³¼
    success: bool


@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„°"""
    text: str
    index: int = 0
    metadata: Dict = field(default_factory=dict)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_document_metadata(text: str, filename: str) -> Dict:
    """
    ë¬¸ì„œ í—¤ë”ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    
    Returns:
        {
            "sop_id": "EQ-SOP-00010",
            "version": "1.0",
            "effective_date": "2025-01-21",
            "title": "í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œ",
            "department": "í’ˆì§ˆê²½ì˜ì‹¤",
            "file_name": "xxx.pdf"
        }
    """
    metadata = {"file_name": filename}
    
    # SOP ID
    sop_match = re.search(r'Number:\s*(EQ-SOP-\d+)', text)
    if not sop_match:
        sop_match = re.search(r'(EQ-SOP-\d+)', text)
    if sop_match:
        metadata["sop_id"] = sop_match.group(1)
    
    # Version
    ver_match = re.search(r'Version:\s*(\d+\.\d+)', text)
    if not ver_match:
        ver_match = re.search(r'Version\s*(\d+\.\d+)', text)
    if ver_match:
        metadata["version"] = ver_match.group(1)
    
    # Effective Date
    date_match = re.search(r'Effective Date:\s*(\d{4}-\d{2}-\d{2})', text)
    if date_match:
        metadata["effective_date"] = date_match.group(1)
    
    # Title (í•œê¸€ ì œëª© ìš°ì„ )
    title_match = re.search(r'Title\s+([ê°€-í£]+)', text)
    if title_match:
        metadata["title"] = title_match.group(1)
    
    # Department
    dept_match = re.search(r'Owning Department\s+([ê°€-í£]+)', text)
    if dept_match:
        metadata["department"] = dept_match.group(1)
    
    return metadata


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def node_load(state: PipelineState) -> PipelineState:
    """
    1ë‹¨ê³„: íŒŒì¼ ë¡œë“œ ë° íƒ€ì… ê°ì§€
    """
    filename = state["filename"]
    content = state["content"]
    
    # ğŸ”¥ ì‹¤ì œ í™•ì¥ì ì¶”ì¶œ (ë§ˆì§€ë§‰ . ì´í›„)
    filename_lower = filename.lower()
    if '.' in filename_lower:
        actual_ext = filename_lower.rsplit('.', 1)[-1]
    else:
        actual_ext = ''
    
    # í™•ì¥ì ê¸°ë°˜ íƒ€ì… ê²°ì •
    if actual_ext == 'pdf':
        file_type = 'pdf'
    elif actual_ext == 'docx':
        file_type = 'docx'
    elif actual_ext == 'doc':
        file_type = 'doc'
    elif actual_ext in ['html', 'htm']:
        file_type = 'html'
    elif actual_ext == 'md':
        file_type = 'markdown'
    elif actual_ext == 'txt':
        file_type = 'text'
    else:
        file_type = 'unknown'
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = len(content)
    if file_size == 0:
        state["errors"] = ["íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
        state["success"] = False
        return state
    
    state["file_type"] = file_type
    state["metadata"] = {
        "file_name": filename,
        "file_type": file_type,
        "file_size": file_size,
    }
    
    return state


def node_convert(state: PipelineState) -> PipelineState:
    """
    2ë‹¨ê³„: ë¬¸ì„œ â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    """
    file_type = state["file_type"]
    content = state["content"]
    filename = state["filename"]
    
    try:
        if file_type == 'docx':
            markdown, metadata = _convert_docx(filename, content)
            state["conversion_method"] = "python-docx"
            
        elif file_type == 'pdf':
            markdown, metadata, method = _convert_pdf_with_fallback(filename, content)
            state["conversion_method"] = method
            # ğŸ”¥ PDFëŠ” í—¤ë” ì¶”ë¡  í•„ìˆ˜!
            markdown = _infer_headers(markdown)
            state["conversion_method"] += "+infer-headers"
            
        elif file_type == 'html':
            markdown, metadata = _convert_html(filename, content)
            state["conversion_method"] = "beautifulsoup"
            
        elif file_type == 'markdown':
            markdown = content.decode('utf-8', errors='ignore')
            metadata = {}
            state["conversion_method"] = "passthrough"
            
        elif file_type == 'text':
            markdown = _convert_text_to_markdown(content.decode('utf-8', errors='ignore'))
            metadata = {}
            state["conversion_method"] = "text-inference"
            
        else:
            markdown = content.decode('utf-8', errors='ignore')
            metadata = {}
            state["conversion_method"] = "fallback-text"
            state["warnings"] = [f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ íƒ€ì…: {file_type}, í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬"]
        
        # ğŸ”¥ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        doc_meta = extract_document_metadata(markdown, filename)
        metadata.update(doc_meta)
        
        state["markdown"] = markdown
        state["metadata"].update(metadata)
        
    except Exception as e:
        state["errors"] = [f"ë³€í™˜ ì‹¤íŒ¨: {str(e)}"]
        state["markdown"] = ""
    
    return state


def node_convert_fallback(state: PipelineState) -> PipelineState:
    """
    2-1ë‹¨ê³„: ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ
    """
    content = state["content"]
    file_type = state["file_type"]
    
    state["warnings"] = [f"ê¸°ë³¸ ë³€í™˜ ì‹¤íŒ¨, í´ë°± ì „ëµ ì‹œë„ ì¤‘..."]
    
    try:
        if file_type == 'pdf':
            markdown = _pdf_fallback_extract(content)
            state["conversion_method"] = "pdf-fallback"
            
        elif file_type == 'docx':
            markdown = _docx_fallback_extract(content)
            state["conversion_method"] = "docx-fallback"
            
        else:
            markdown = content.decode('utf-8', errors='ignore')
            state["conversion_method"] = "binary-text"
        
        state["markdown"] = markdown
        state["errors"] = []
        
    except Exception as e:
        state["errors"] = [f"í´ë°± ë³€í™˜ë„ ì‹¤íŒ¨: {str(e)}"]
        state["success"] = False
    
    return state


def node_validate(state: PipelineState) -> PipelineState:
    """
    3ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í’ˆì§ˆ ê²€ì¦
    """
    markdown = state.get("markdown", "")
    
    if not markdown:
        state["quality_score"] = 0.0
        state["errors"] = ["ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
        return state
    
    score = 0.0
    issues = []
    
    # 1. ê¸¸ì´ ì²´í¬ (ìµœì†Œ 100ì)
    if len(markdown) >= 100:
        score += 0.2
    else:
        issues.append("í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
    
    # 2. í—¤ë” ì¡´ì¬ ì—¬ë¶€
    header_count = len(re.findall(r'^#{1,6}\s+', markdown, re.MULTILINE))
    if header_count >= 3:
        score += 0.3
    elif header_count >= 1:
        score += 0.15
        issues.append("í—¤ë”ê°€ ë¶€ì¡±í•¨")
    else:
        issues.append("í—¤ë”ê°€ ì—†ìŒ")
    
    # 3. ë¬¸ë‹¨ êµ¬ì¡°
    paragraphs = [p for p in markdown.split('\n\n') if p.strip()]
    if len(paragraphs) >= 5:
        score += 0.2
    elif len(paragraphs) >= 2:
        score += 0.1
        issues.append("ë¬¸ë‹¨ êµ¬ì¡°ê°€ ë¶€ì‹¤í•¨")
    
    # 4. í•œê¸€ ë¹„ìœ¨
    korean_chars = len(re.findall(r'[ê°€-í£]', markdown))
    total_chars = len(markdown)
    korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
    
    if korean_ratio >= 0.1:
        score += 0.2
    else:
        issues.append("í•œê¸€ ë¹„ìœ¨ì´ ë‚®ìŒ")
    
    # 5. íŠ¹ìˆ˜ë¬¸ì ì˜¤ì—¼ ì²´í¬
    garbage_ratio = len(re.findall(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', markdown)) / len(markdown) if markdown else 0
    if garbage_ratio < 0.01:
        score += 0.1
    else:
        issues.append("íŠ¹ìˆ˜ë¬¸ì ì˜¤ì—¼ ê°ì§€")
    
    state["quality_score"] = min(score, 1.0)
    
    if issues:
        state["warnings"] = issues
    
    return state


def node_repair(state: PipelineState) -> PipelineState:
    """
    3-1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í’ˆì§ˆ ë³´ì •
    """
    markdown = state.get("markdown", "")
    
    state["warnings"] = ["í’ˆì§ˆ ë³´ì • ìˆ˜í–‰ ì¤‘..."]
    state["retry_count"] = state.get("retry_count", 0) + 1
    
    # 1. íŠ¹ìˆ˜ë¬¸ì ì œê±°
    markdown = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', markdown)
    
    # 2. ì—°ì† ë¹ˆ ì¤„ ì •ë¦¬
    markdown = re.sub(r'\n{3,}', '\n\n', markdown)
    
    # 3. í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ë¡ í•´ì„œ ì¶”ê°€
    if not re.search(r'^#{1,6}\s+', markdown, re.MULTILINE):
        markdown = _infer_headers(markdown)
    
    # 4. ê¹¨ì§„ í…Œì´ë¸” ë³µêµ¬ ì‹œë„
    markdown = _repair_tables(markdown)
    
    state["markdown"] = markdown
    state["conversion_method"] += "+repaired"
    
    # í’ˆì§ˆ ì¬ì¸¡ì •
    state = node_validate(state)
    
    return state


def node_split(state: PipelineState) -> PipelineState:
    """
    4ë‹¨ê³„: í—¤ë” ê¸°ì¤€ ë¶„í•  + ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•
    """
    markdown = state.get("markdown", "")
    
    if not markdown:
        state["sections"] = []
        return state
    
    lines = markdown.split('\n')
    sections = []
    
    current_headers = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}
    current_content = []
    current_page = 1
    in_toc = False
    
    def flush_section():
        nonlocal current_content
        if current_content:
            content = '\n'.join(current_content).strip()
            if content:
                # ğŸ”¥ ê³„ì¸µ ê²½ë¡œ ìƒì„±
                header_path_parts = []
                headers_dict = {}
                for level in range(1, 7):
                    if current_headers[level]:
                        headers_dict[f"H{level}"] = current_headers[level]
                        header_path_parts.append(current_headers[level])
                
                # ğŸ”¥ Parent-Child ê´€ê³„
                parent = None
                for level in range(6, 0, -1):
                    if current_headers[level]:
                        # í˜„ì¬ ë ˆë²¨ë³´ë‹¤ í•œ ë‹¨ê³„ ìœ„ ì°¾ê¸°
                        for p_level in range(level - 1, 0, -1):
                            if current_headers[p_level]:
                                parent = current_headers[p_level]
                                break
                        break
                
                sections.append({
                    "content": content,
                    "headers": headers_dict,
                    "header_path": " > ".join(header_path_parts) if header_path_parts else None,
                    "page": current_page,
                    "parent": parent,
                })
        current_content = []
    
    for line in lines:
        # ğŸ”¥ í˜ì´ì§€ ë§ˆì»¤ ê°ì§€
        page_match = re.match(r'<!-- PAGE:(\d+) -->', line)
        if page_match:
            current_page = int(page_match.group(1))
            continue
        
        # ğŸ”¥ ëª©ì°¨ ê°ì§€ ë° ìŠ¤í‚µ
        if re.match(r'^#{1,2}\s+ëª©ì°¨|^#{1,2}\s+Table of Contents', line, re.IGNORECASE):
            in_toc = True
            continue
        
        # ëª©ì°¨ ì¢…ë£Œ ê°ì§€ (ë‹¤ìŒ ì£¼ìš” ì„¹ì…˜ ì‹œì‘)
        if in_toc:
            if re.match(r'^##\s+\d+\s+ëª©ì |^##\s+1\s+', line):
                in_toc = False
            else:
                continue  # ëª©ì°¨ ë‚´ìš© ìŠ¤í‚µ
        
        # í—¤ë” ê°ì§€
        header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        
        if header_match:
            flush_section()
            level = len(header_match.group(1))
            header_text = header_match.group(2).strip()
            
            current_headers[level] = header_text
            for l in range(level + 1, 7):
                current_headers[l] = None
            
            current_content.append(line)
        else:
            current_content.append(line)
    
    flush_section()
    
    state["sections"] = sections
    return state


def node_optimize(state: PipelineState) -> PipelineState:
    """
    5ë‹¨ê³„: ê¸´ ì„¹ì…˜ ì¬ë¶„í•  + ìµœì í™”ëœ ë©”íƒ€ë°ì´í„°
    """
    sections = state.get("sections", [])
    chunk_size = state.get("chunk_size", 500)
    chunk_overlap = state.get("chunk_overlap", 50)
    metadata = state.get("metadata", {})
    
    chunks = []
    idx = 0
    
    # ğŸ”¥ ë¬¸ì„œ ë ˆë²¨ ë©”íƒ€ë°ì´í„°
    sop_id = metadata.get("sop_id")
    doc_name = metadata.get("file_name")
    version = metadata.get("version")
    effective_date = metadata.get("effective_date")
    
    for section in sections:
        content = section["content"]
        headers = section.get("headers", {})
        header_path = section.get("header_path")
        page = section.get("page", 1)
        parent = section.get("parent")
        
        # ê¸´ ì„¹ì…˜ ì¬ë¶„í• 
        if len(content) > chunk_size:
            text_chunks = _split_recursive(content, chunk_size, chunk_overlap)
            is_split = len(text_chunks) > 1
        else:
            text_chunks = [content]
            is_split = False
        
        for i, text in enumerate(text_chunks):
            if not text.strip():
                continue
            
            # ì¬ë¶„í• ëœ ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
            if is_split and i > 0 and header_path:
                text = f"[Context: {header_path}]\n\n{text}"
            
            # ğŸ”¥ ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ ê°œì„ 
            section_num = None
            current_section = headers.get("H4") or headers.get("H3") or headers.get("H2")
            
            if current_section:
                # 5.1.2.1, 5.1.1, 5.1, 5 íŒ¨í„´
                num_match = re.match(r'^(\d+(?:\.\d+)*)', current_section)
                if num_match:
                    section_num = num_match.group(1)
            
            # ğŸ”¥ ìµœì í™”ëœ ë©”íƒ€ë°ì´í„° (í•„ìˆ˜ + ìœ ìš©í•œ ê²ƒë§Œ)
            chunks.append({
                "text": text.strip(),
                "index": idx,
                "metadata": {
                    # ğŸ”¥ í•„ìˆ˜ (ê²€ìƒ‰/í•„í„°ë§)
                    "doc_name": doc_name,
                    "sop_id": sop_id,
                    "section_path": header_path,
                    "section": current_section,
                    
                    # ğŸ”¥ ìœ ìš© (ì¶œì²˜/ë¶„ì„)
                    "article_num": section_num,
                    "page": page,
                    "parent": parent,
                    
                    # ğŸ”¥ ë¶„í•  ì²­í¬ ì¶”ì 
                    "chunk_part": i + 1 if is_split else None,
                    "total_parts": len(text_chunks) if is_split else None,
                    
                    # ğŸ”¥ ë¬¸ì„œ ë²„ì „ ì •ë³´ (ì„ íƒì )
                    "version": version,
                    "effective_date": effective_date,
                }
            })
            idx += 1
    
    state["chunks"] = chunks
    return state


def node_finalize(state: PipelineState) -> PipelineState:
    """
    6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì •ë¦¬
    """
    chunks = state.get("chunks", [])
    
    if not chunks:
        state["success"] = False
        state["errors"] = ["ì²­í¬ ìƒì„± ì‹¤íŒ¨: ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
    else:
        state["success"] = True
    
    # í†µê³„ ì¶”ê°€
    state["metadata"]["total_chunks"] = len(chunks)
    state["metadata"]["quality_score"] = state.get("quality_score", 0)
    state["metadata"]["conversion_method"] = state.get("conversion_method", "unknown")
    
    return state


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def should_fallback(state: PipelineState) -> Literal["fallback", "validate"]:
    """ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°±ìœ¼ë¡œ ë¼ìš°íŒ…"""
    if state.get("errors") or not state.get("markdown"):
        return "fallback"
    return "validate"


def should_repair(state: PipelineState) -> Literal["repair", "split"]:
    """í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë³´ì •ìœ¼ë¡œ ë¼ìš°íŒ…"""
    quality_score = state.get("quality_score", 0)
    retry_count = state.get("retry_count", 0)
    
    if quality_score < 0.5 and retry_count < 2:
        return "repair"
    return "split"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _convert_docx(filename: str, content: bytes) -> tuple:
    """DOCX â†’ Markdown"""
    from docx import Document
    
    doc = Document(BytesIO(content))
    md_lines = []
    metadata = {}
    
    sop_pattern = re.compile(r'((?:EQ-)?SOP[-_]?\d{4,5})', re.IGNORECASE)
    
    main_sections = ['ëª©ì ', 'Purpose', 'ì ìš© ë²”ìœ„', 'Scope', 'ì •ì˜', 'Definitions',
                     'ì±…ì„', 'Responsibilities', 'ì ˆì°¨', 'Procedure', 
                     'ì°¸ê³ ë¬¸í—Œ', 'Reference', 'ì²¨ë¶€', 'Attachments']
    
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            md_lines.append("")
            continue
        
        # SOP ID ì¶”ì¶œ
        sop_match = sop_pattern.search(text)
        if sop_match and "sop_id" not in metadata:
            sop_id = sop_match.group(1).upper().replace('_', '-')
            if not sop_id.startswith('EQ-'):
                sop_id = 'EQ-' + sop_id
            metadata["sop_id"] = sop_id
        
        # í—¤ë” ë ˆë²¨ ê²°ì •
        header_level = None
        
        style_name = para.style.name.lower() if para.style else ""
        if 'heading 1' in style_name or 'title' in style_name:
            header_level = 1
        elif 'heading 2' in style_name:
            header_level = 2
        elif 'heading 3' in style_name:
            header_level = 3
        elif 'heading 4' in style_name:
            header_level = 4
        
        if not header_level:
            for section in main_sections:
                if text.startswith(section) or re.match(rf'^\d+\s+{section}', text):
                    header_level = 2
                    break
            
            if not header_level:
                if re.match(r'^\d+\.\d+\.\d+\.\d+\s*', text):
                    header_level = 5
                elif re.match(r'^\d+\.\d+\.\d+\s+', text):
                    header_level = 4
                elif re.match(r'^\d+\.\d+\s+', text):
                    header_level = 3
                elif re.match(r'^\d+\.?\s+[ê°€-í£A-Za-z]', text):
                    header_level = 2
                elif re.match(r'^[ê°€-í£A-Z][ê°€-í£\s\(\)/Â·\-]+\s*\([A-Za-z\s&/\-:]+\)\s*$', text):
                    header_level = 3
        
        if header_level:
            md_lines.append(f"{'#' * header_level} {text}")
        else:
            md_lines.append(text)
    
    # í…Œì´ë¸” ì²˜ë¦¬
    for table in doc.tables:
        md_lines.append("")
        md_lines.append(_table_to_markdown(table))
    
    return '\n'.join(md_lines), metadata


def _convert_pdf_with_fallback(filename: str, content: bytes) -> tuple:
    """PDF ë³€í™˜ (ë‹¤ì¤‘ í´ë°±) + í˜ì´ì§€ ë§ˆì»¤"""
    
    # 1ìˆœìœ„: pdfplumber (ê°€ì¥ ì•ˆì •ì )
    try:
        import pdfplumber
        md_lines = []
        with pdfplumber.open(BytesIO(content)) as pdf:
            for i, page in enumerate(pdf.pages):
                text = page.extract_text() or ''
                if text.strip():
                    # ğŸ”¥ í˜ì´ì§€ ë§ˆì»¤ ì‚½ì…
                    md_lines.append(f"<!-- PAGE:{i + 1} -->")
                    md_lines.append(text)
        if md_lines:
            return '\n'.join(md_lines), {"parser": "pdfplumber", "total_pages": len(pdf.pages)}, "pdfplumber"
    except Exception as e:
        print(f"   pdfplumber ì‹¤íŒ¨: {e}")
    
    # 2ìˆœìœ„: Docling
    try:
        from docling.document_converter import DocumentConverter
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as f:
            f.write(content)
            temp_path = f.name
        
        try:
            converter = DocumentConverter()
            result = converter.convert(temp_path)
            markdown = result.document.export_to_markdown()
            return markdown, {"parser": "docling"}, "docling"
        finally:
            os.unlink(temp_path)
    except Exception as e:
        print(f"   Docling ì‹¤íŒ¨: {e}")
    
    # 3ìˆœìœ„: PyMuPDF
    try:
        import fitz
        pdf = fitz.open(stream=content, filetype="pdf")
        md_lines = []
        for page_num, page in enumerate(pdf):
            text = page.get_text()
            if text.strip():
                md_lines.append(f"<!-- PAGE:{page_num + 1} -->")
                md_lines.append(text)
        return '\n'.join(md_lines), {"parser": "pymupdf", "total_pages": len(pdf)}, "pymupdf"
    except Exception as e:
        print(f"   PyMuPDF ì‹¤íŒ¨: {e}")
    
    # 4ìˆœìœ„: PyPDF2
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(BytesIO(content))
        md_lines = []
        for i, page in enumerate(reader.pages):
            text = page.extract_text() or ''
            if text.strip():
                md_lines.append(f"<!-- PAGE:{i + 1} -->")
                md_lines.append(text)
        return '\n'.join(md_lines), {"parser": "pypdf2", "total_pages": len(reader.pages)}, "pypdf2"
    except Exception as e:
        print(f"   PyPDF2 ì‹¤íŒ¨: {e}")
    
    raise Exception("ëª¨ë“  PDF íŒŒì„œ ì‹¤íŒ¨")


def _convert_html(filename: str, content: bytes) -> tuple:
    """HTML â†’ Markdown"""
    from bs4 import BeautifulSoup
    
    html = content.decode('utf-8', errors='ignore')
    soup = BeautifulSoup(html, 'html.parser')
    
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    md_lines = []
    title = soup.title.string if soup.title else filename
    md_lines.append(f"# {title}")
    md_lines.append("")
    
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
        if tag.name.startswith('h'):
            level = int(tag.name[1])
            md_lines.append(f"{'#' * level} {tag.get_text(strip=True)}")
        elif tag.name == 'li':
            md_lines.append(f"- {tag.get_text(strip=True)}")
        else:
            text = tag.get_text(strip=True)
            if text:
                md_lines.append(text)
        md_lines.append("")
    
    return '\n'.join(md_lines), {"title": title}


def _convert_text_to_markdown(text: str) -> str:
    """í…ìŠ¤íŠ¸ â†’ ë§ˆí¬ë‹¤ìš´ (í—¤ë” ì¶”ë¡ )"""
    return _infer_headers(text)


def _pdf_fallback_extract(content: bytes) -> str:
    """PDF í´ë°± ì¶”ì¶œ"""
    try:
        import pdfplumber
        with pdfplumber.open(BytesIO(content)) as pdf:
            texts = []
            for i, page in enumerate(pdf.pages):
                texts.append(f"<!-- PAGE:{i+1} -->")
                texts.append(page.extract_text() or '')
            return '\n\n'.join(texts)
    except:
        pass
    return content.decode('latin-1', errors='ignore')


def _docx_fallback_extract(content: bytes) -> str:
    """DOCX í´ë°±: XML ì§ì ‘ íŒŒì‹±"""
    import zipfile
    from xml.etree import ElementTree
    
    try:
        with zipfile.ZipFile(BytesIO(content)) as zf:
            xml_content = zf.read('word/document.xml')
            tree = ElementTree.fromstring(xml_content)
            
            texts = []
            for t in tree.iter('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'):
                if t.text:
                    texts.append(t.text)
            
            return '\n'.join(texts)
    except:
        return ""


def _table_to_markdown(table) -> str:
    """Word í…Œì´ë¸” â†’ Markdown"""
    rows = []
    for row in table.rows:
        cells = [cell.text.strip().replace('\n', ' ') for cell in row.cells]
        rows.append(cells)
    
    if not rows:
        return ""
    
    md_lines = []
    md_lines.append("| " + " | ".join(rows[0]) + " |")
    md_lines.append("| " + " | ".join(["---"] * len(rows[0])) + " |")
    for row in rows[1:]:
        while len(row) < len(rows[0]):
            row.append("")
        md_lines.append("| " + " | ".join(row[:len(rows[0])]) + " |")
    
    return '\n'.join(md_lines)


def _infer_headers(markdown: str) -> str:
    """í—¤ë” ì¶”ë¡  ì‚½ì… (PDFìš© ê°•í™”)"""
    lines = markdown.split('\n')
    result = []
    
    main_sections = ['ëª©ì ', 'ì ìš© ë²”ìœ„', 'ì •ì˜', 'ì±…ì„', 'ì ˆì°¨', 'ì°¸ê³ ë¬¸í—Œ', 'ì²¨ë¶€',
                     'Purpose', 'Scope', 'Definitions', 'Responsibilities', 'Procedure', 
                     'Reference', 'Attachments']
    
    # ğŸ”¥ ë¬´ì‹œí•  íŒ¨í„´ (í˜ì´ì§€ ë²ˆí˜¸ ë“±) - í…ìŠ¤íŠ¸ëŠ” ìœ ì§€í•˜ë˜ í—¤ë”ë¡œ ì•ˆ ë§Œë“¦
    ignore_patterns = [
        r'^\d+\s+of\s+\d+$',
        r'^Page\s+\d+',
        r'^-\s*\d+\s*-$',
        r'^Number:\s*',
        r'^<!--\s*PAGE',
    ]
    
    for line in lines:
        stripped = line.strip()
        
        if not stripped:
            result.append(line)
            continue
        
        # ë¬´ì‹œ íŒ¨í„´ ì²´í¬ (í—¤ë”ë¡œ ì•ˆ ë§Œë“¤ê³  í…ìŠ¤íŠ¸ ìœ ì§€)
        should_ignore = False
        for pattern in ignore_patterns:
            if re.match(pattern, stripped, re.IGNORECASE):
                should_ignore = True
                break
        
        if should_ignore:
            result.append(line)
            continue
        
        # 1. ìˆ«ìí˜• í—¤ë” íŒ¨í„´
        # 5.1.2.1 xxx â†’ H5
        if re.match(r'^(\d+\.\d+\.\d+\.\d+)\s*(.+)', stripped):
            result.append(f"##### {stripped}")
            continue
        
        # 5.1.1 xxx â†’ H4
        if re.match(r'^(\d+\.\d+\.\d+)\s+(.+)', stripped):
            result.append(f"#### {stripped}")
            continue
        
        # 5.1 xxx â†’ H3
        if re.match(r'^(\d+\.\d+)\s+(.+)', stripped):
            result.append(f"### {stripped}")
            continue
        
        # 5 xxx â†’ H2
        match = re.match(r'^(\d+)\s+([ê°€-í£A-Za-z].+)', stripped)
        if match:
            num = match.group(1)
            text = match.group(2)
            if not re.match(r'^of\s+\d+', text, re.IGNORECASE):
                result.append(f"## {stripped}")
                continue
        
        # 2. ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ â†’ H2
        matched = False
        for section in main_sections:
            if stripped.startswith(section) and len(stripped) < 50:
                result.append(f"## {stripped}")
                matched = True
                break
        
        if not matched:
            # 3. ì†Œì œëª© íŒ¨í„´ â†’ H3
            if re.match(r'^[ê°€-í£][ê°€-í£\s\(\)/Â·\-]+\s*\([A-Za-z\s&/\-:]+\)\s*$', stripped):
                result.append(f"### {stripped}")
            else:
                result.append(line)
    
    return '\n'.join(result)


def _repair_tables(markdown: str) -> str:
    """ê¹¨ì§„ í…Œì´ë¸” ë³µêµ¬"""
    lines = markdown.split('\n')
    result = []
    in_table = False
    table_cols = 0
    
    for line in lines:
        if line.strip().startswith('|') and line.strip().endswith('|'):
            cols = line.count('|') - 1
            
            if not in_table:
                in_table = True
                table_cols = cols
                result.append(line)
            else:
                while line.count('|') - 1 < table_cols:
                    line = line.rstrip('|') + ' |'
                result.append(line)
        else:
            in_table = False
            result.append(line)
    
    return '\n'.join(result)


def _split_recursive(text: str, chunk_size: int, overlap: int) -> List[str]:
    """ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• """
    if len(text) <= chunk_size:
        return [text]
    
    separators = ["\n\n", "\n| ", "\n", ". ", "ã€‚", " ", ""]
    is_table = text.strip().startswith('|') or '\n|' in text
    effective_overlap = 0 if is_table else overlap
    
    for sep in separators:
        if sep in text:
            parts = text.split(sep)
            chunks = []
            current = ""
            
            for part in parts:
                if len(current) + len(part) + len(sep) <= chunk_size:
                    current = current + sep + part if current else part
                else:
                    if current:
                        chunks.append(current)
                    if len(part) > chunk_size:
                        chunks.extend(_split_recursive(part, chunk_size, overlap))
                        current = ""
                    else:
                        current = part
            
            if current:
                chunks.append(current)
            
            return chunks
    
    step = chunk_size - effective_overlap if effective_overlap > 0 else chunk_size
    return [text[i:i+chunk_size] for i in range(0, len(text), step)]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LangGraph íŒŒì´í”„ë¼ì¸ ë¹Œë”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_pipeline():
    """LangGraph íŒŒì´í”„ë¼ì¸ êµ¬ì„±"""
    try:
        from langgraph.graph import StateGraph, END
    except ImportError:
        raise ImportError("langgraph íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install langgraph")
    
    workflow = StateGraph(PipelineState)
    
    workflow.add_node("load", node_load)
    workflow.add_node("convert", node_convert)
    workflow.add_node("fallback", node_convert_fallback)
    workflow.add_node("validate", node_validate)
    workflow.add_node("repair", node_repair)
    workflow.add_node("split", node_split)
    workflow.add_node("optimize", node_optimize)
    workflow.add_node("finalize", node_finalize)
    
    workflow.set_entry_point("load")
    
    workflow.add_edge("load", "convert")
    
    workflow.add_conditional_edges(
        "convert",
        should_fallback,
        {"fallback": "fallback", "validate": "validate"}
    )
    
    workflow.add_edge("fallback", "validate")
    
    workflow.add_conditional_edges(
        "validate",
        should_repair,
        {"repair": "repair", "split": "split"}
    )
    
    workflow.add_edge("repair", "split")
    workflow.add_edge("split", "optimize")
    workflow.add_edge("optimize", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_document(
    filename: str,
    content: bytes,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    debug: bool = False
) -> dict:
    """ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    
    initial_state: PipelineState = {
        "filename": filename,
        "content": content,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_type": "",
        "markdown": "",
        "metadata": {},
        "sections": [],
        "chunks": [],
        "quality_score": 0.0,
        "conversion_method": "",
        "errors": [],
        "warnings": [],
        "retry_count": 0,
        "success": False,
    }
    
    try:
        pipeline = build_pipeline()
        result = pipeline.invoke(initial_state)
        
        if debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
            print(f"{'='*60}")
            print(f"   íŒŒì¼: {filename}")
            print(f"   ë³€í™˜ ë°©ë²•: {result.get('conversion_method')}")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0%}")
            print(f"   ì´ ì²­í¬: {len(result.get('chunks', []))}")
            if result.get('warnings'):
                print(f"   âš ï¸ ê²½ê³ : {result['warnings']}")
            if result.get('errors'):
                print(f"   âŒ ì—ëŸ¬: {result['errors']}")
        
        return result
        
    except ImportError:
        if debug:
            print("âš ï¸ LangGraph ì—†ìŒ, ì‹¬í”Œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
        return _simple_pipeline(initial_state, debug)


def _simple_pipeline(state: PipelineState, debug: bool = False) -> dict:
    """LangGraph ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ì‹¬í”Œ íŒŒì´í”„ë¼ì¸"""
    state = node_load(state)
    if state.get("errors"):
        return state
    
    state = node_convert(state)
    if not state.get("markdown"):
        state = node_convert_fallback(state)
    
    state = node_validate(state)
    
    if state.get("quality_score", 0) < 0.5:
        state = node_repair(state)
    
    state = node_split(state)
    state = node_optimize(state)
    state = node_finalize(state)
    
    if debug:
        print(f"\nğŸ“Š ì‹¬í”Œ íŒŒì´í”„ë¼ì¸ ê²°ê³¼: {len(state.get('chunks', []))} ì²­í¬")
    
    return state


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²°ê³¼ ë³€í™˜ í—¬í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def state_to_chunks(state: dict) -> List[Chunk]:
    """ìƒíƒœë¥¼ Chunk ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        Chunk(
            text=c["text"],
            index=c["index"],
            metadata=c["metadata"]
        )
        for c in state.get("chunks", [])
    ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("ğŸ”¥ document_pipeline v9.1 í…ŒìŠ¤íŠ¸")