"""
LangGraph ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ v9.0

ğŸ”¥ ìƒíƒœ ë¨¸ì‹ (State Machine) ê¸°ë°˜ ìœ ì—°í•œ ì›Œí¬í”Œë¡œìš°:
- ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬
- ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ
- í’ˆì§ˆ ê²€ì¦ ë° ë³´ì • ë‹¨ê³„
- ì¡°ê±´ë¶€ ì¬ì²˜ë¦¬

ë…¸ë“œ íë¦„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load   â”‚â”€â”€â”€â–¶â”‚ Convert â”‚â”€â”€â”€â–¶â”‚ Validate â”‚â”€â”€â”€â–¶â”‚  Split  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚                â”‚
                    â–¼              â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Fallback â”‚  â”‚  Repair  â”‚    â”‚ Optimize â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Finalize â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

from typing import TypedDict, List, Dict, Optional, Literal, Annotated
from dataclasses import dataclass, field
import re
from io import BytesIO
import operator


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìƒíƒœ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PipelineState(TypedDict):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""
    # ì…ë ¥
    filename: str
    content: bytes
    
    # ì„¤ì •
    chunk_size: int
    chunk_overlap: int
    
    # ì¤‘ê°„ ê²°ê³¼
    file_type: str
    markdown: str
    metadata: Dict
    sections: List[Dict]
    chunks: List[Dict]
    
    # í’ˆì§ˆ ì§€í‘œ
    quality_score: float
    conversion_method: str
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: Annotated[List[str], operator.add]
    warnings: Annotated[List[str], operator.add]
    retry_count: int
    
    # ìµœì¢… ê²°ê³¼
    success: bool


@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„°"""
    text: str
    index: int = 0
    metadata: Dict = field(default_factory=dict)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def node_load(state: PipelineState) -> PipelineState:
    """
    1ë‹¨ê³„: íŒŒì¼ ë¡œë“œ ë° íƒ€ì… ê°ì§€
    """
    filename = state["filename"]
    content = state["content"]
    
    # íŒŒì¼ íƒ€ì… ê°ì§€ (í™•ì¥ì ë²„ê·¸ ìˆ˜ì • í¬í•¨)
    filename_lower = filename.lower()
    
    if '.docx' in filename_lower:
        file_type = 'docx'
    elif '.doc' in filename_lower and '.docx' not in filename_lower:
        file_type = 'doc'
    elif '.pdf' in filename_lower:
        file_type = 'pdf'
    elif '.html' in filename_lower or '.htm' in filename_lower:
        file_type = 'html'
    elif '.md' in filename_lower:
        file_type = 'markdown'
    elif '.txt' in filename_lower:
        file_type = 'text'
    else:
        file_type = 'unknown'
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = len(content)
    if file_size == 0:
        state["errors"] = ["íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
        state["success"] = False
        return state
    
    state["file_type"] = file_type
    state["metadata"] = {
        "file_name": filename,
        "file_type": file_type,
        "file_size": file_size,
    }
    
    return state


def node_convert(state: PipelineState) -> PipelineState:
    """
    2ë‹¨ê³„: ë¬¸ì„œ â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    """
    file_type = state["file_type"]
    content = state["content"]
    filename = state["filename"]
    
    try:
        if file_type == 'docx':
            markdown, metadata = _convert_docx(filename, content)
            state["conversion_method"] = "python-docx"
            
        elif file_type == 'pdf':
            markdown, metadata, method = _convert_pdf_with_fallback(filename, content)
            state["conversion_method"] = method
            
        elif file_type == 'html':
            markdown, metadata = _convert_html(filename, content)
            state["conversion_method"] = "beautifulsoup"
            
        elif file_type == 'markdown':
            markdown = content.decode('utf-8', errors='ignore')
            metadata = {}
            state["conversion_method"] = "passthrough"
            
        elif file_type == 'text':
            markdown = _convert_text_to_markdown(content.decode('utf-8', errors='ignore'))
            metadata = {}
            state["conversion_method"] = "text-inference"
            
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… â†’ í…ìŠ¤íŠ¸ë¡œ ì‹œë„
            markdown = content.decode('utf-8', errors='ignore')
            metadata = {}
            state["conversion_method"] = "fallback-text"
            state["warnings"] = [f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ íƒ€ì…: {file_type}, í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬"]
        
        state["markdown"] = markdown
        state["metadata"].update(metadata)
        
    except Exception as e:
        state["errors"] = [f"ë³€í™˜ ì‹¤íŒ¨: {str(e)}"]
        state["markdown"] = ""
    
    return state


def node_convert_fallback(state: PipelineState) -> PipelineState:
    """
    2-1ë‹¨ê³„: ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ
    """
    content = state["content"]
    file_type = state["file_type"]
    
    state["warnings"] = [f"ê¸°ë³¸ ë³€í™˜ ì‹¤íŒ¨, í´ë°± ì „ëµ ì‹œë„ ì¤‘..."]
    
    try:
        if file_type == 'pdf':
            # PDF í´ë°±: PyPDF2 â†’ pdfplumber â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            markdown = _pdf_fallback_extract(content)
            state["conversion_method"] = "pdf-fallback"
            
        elif file_type == 'docx':
            # DOCX í´ë°±: XML ì§ì ‘ íŒŒì‹±
            markdown = _docx_fallback_extract(content)
            state["conversion_method"] = "docx-fallback"
            
        else:
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ë°”ì´ë„ˆë¦¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            markdown = content.decode('utf-8', errors='ignore')
            state["conversion_method"] = "binary-text"
        
        state["markdown"] = markdown
        state["errors"] = []  # ì—ëŸ¬ í´ë¦¬ì–´
        
    except Exception as e:
        state["errors"] = [f"í´ë°± ë³€í™˜ë„ ì‹¤íŒ¨: {str(e)}"]
        state["success"] = False
    
    return state


def node_validate(state: PipelineState) -> PipelineState:
    """
    3ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í’ˆì§ˆ ê²€ì¦
    """
    markdown = state.get("markdown", "")
    
    if not markdown:
        state["quality_score"] = 0.0
        state["errors"] = ["ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
        return state
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    score = 0.0
    issues = []
    
    # 1. ê¸¸ì´ ì²´í¬ (ìµœì†Œ 100ì)
    if len(markdown) >= 100:
        score += 0.2
    else:
        issues.append("í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
    
    # 2. í—¤ë” ì¡´ì¬ ì—¬ë¶€
    header_count = len(re.findall(r'^#{1,6}\s+', markdown, re.MULTILINE))
    if header_count >= 3:
        score += 0.3
    elif header_count >= 1:
        score += 0.15
        issues.append("í—¤ë”ê°€ ë¶€ì¡±í•¨")
    else:
        issues.append("í—¤ë”ê°€ ì—†ìŒ")
    
    # 3. ë¬¸ë‹¨ êµ¬ì¡° (ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ëœ ë¬¸ë‹¨)
    paragraphs = [p for p in markdown.split('\n\n') if p.strip()]
    if len(paragraphs) >= 5:
        score += 0.2
    elif len(paragraphs) >= 2:
        score += 0.1
        issues.append("ë¬¸ë‹¨ êµ¬ì¡°ê°€ ë¶€ì‹¤í•¨")
    
    # 4. í•œê¸€ ë¹„ìœ¨ (SOP ë¬¸ì„œ íŠ¹ì„±ìƒ í•œê¸€ì´ ìˆì–´ì•¼ í•¨)
    korean_chars = len(re.findall(r'[ê°€-í£]', markdown))
    total_chars = len(markdown)
    korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
    
    if korean_ratio >= 0.1:
        score += 0.2
    else:
        issues.append("í•œê¸€ ë¹„ìœ¨ì´ ë‚®ìŒ")
    
    # 5. íŠ¹ìˆ˜ë¬¸ì ì˜¤ì—¼ ì²´í¬
    garbage_ratio = len(re.findall(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', markdown)) / len(markdown) if markdown else 0
    if garbage_ratio < 0.01:
        score += 0.1
    else:
        issues.append("íŠ¹ìˆ˜ë¬¸ì ì˜¤ì—¼ ê°ì§€")
    
    state["quality_score"] = min(score, 1.0)
    
    if issues:
        state["warnings"] = issues
    
    return state


def node_repair(state: PipelineState) -> PipelineState:
    """
    3-1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í’ˆì§ˆ ë³´ì •
    """
    markdown = state.get("markdown", "")
    
    state["warnings"] = ["í’ˆì§ˆ ë³´ì • ìˆ˜í–‰ ì¤‘..."]
    
    # 1. íŠ¹ìˆ˜ë¬¸ì ì œê±°
    markdown = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', markdown)
    
    # 2. ì—°ì† ë¹ˆ ì¤„ ì •ë¦¬
    markdown = re.sub(r'\n{3,}', '\n\n', markdown)
    
    # 3. í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ë¡ í•´ì„œ ì¶”ê°€
    if not re.search(r'^#{1,6}\s+', markdown, re.MULTILINE):
        markdown = _infer_headers(markdown)
    
    # 4. ê¹¨ì§„ í…Œì´ë¸” ë³µêµ¬ ì‹œë„
    markdown = _repair_tables(markdown)
    
    state["markdown"] = markdown
    state["conversion_method"] += "+repaired"
    
    # í’ˆì§ˆ ì¬ì¸¡ì •
    state = node_validate(state)
    
    return state


def node_split(state: PipelineState) -> PipelineState:
    """
    4ë‹¨ê³„: í—¤ë” ê¸°ì¤€ ë¶„í• 
    """
    markdown = state.get("markdown", "")
    
    if not markdown:
        state["sections"] = []
        return state
    
    lines = markdown.split('\n')
    sections = []
    
    current_headers = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}
    current_content = []
    
    def flush_section():
        nonlocal current_content
        if current_content:
            content = '\n'.join(current_content).strip()
            if content:
                header_path_parts = []
                headers_dict = {}
                for level in range(1, 7):
                    if current_headers[level]:
                        headers_dict[f"H{level}"] = current_headers[level]
                        if level >= 2:
                            header_path_parts.append(current_headers[level])
                
                sections.append({
                    "content": content,
                    "headers": headers_dict,
                    "header_path": " > ".join(header_path_parts) if header_path_parts else None
                })
        current_content = []
    
    for line in lines:
        header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        
        if header_match:
            flush_section()
            level = len(header_match.group(1))
            header_text = header_match.group(2).strip()
            
            current_headers[level] = header_text
            for l in range(level + 1, 7):
                current_headers[l] = None
            
            current_content.append(line)
        else:
            current_content.append(line)
    
    flush_section()
    
    state["sections"] = sections
    return state


def node_optimize(state: PipelineState) -> PipelineState:
    """
    5ë‹¨ê³„: ê¸´ ì„¹ì…˜ ì¬ë¶„í•  + ì»¨í…ìŠ¤íŠ¸ í”„ë¦¬í”½ìŠ¤
    """
    sections = state.get("sections", [])
    chunk_size = state.get("chunk_size", 500)
    chunk_overlap = state.get("chunk_overlap", 50)
    metadata = state.get("metadata", {})
    
    chunks = []
    idx = 0
    
    sop_id = metadata.get("sop_id")
    doc_name = metadata.get("file_name")
    
    for section in sections:
        content = section["content"]
        headers = section.get("headers", {})
        header_path = section.get("header_path")
        
        # ê¸´ ì„¹ì…˜ ì¬ë¶„í• 
        if len(content) > chunk_size:
            text_chunks = _split_recursive(content, chunk_size, chunk_overlap)
            is_split = len(text_chunks) > 1
        else:
            text_chunks = [content]
            is_split = False
        
        for i, text in enumerate(text_chunks):
            if not text.strip():
                continue
            
            # ì¬ë¶„í• ëœ ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
            if is_split and i > 0 and header_path:
                text = f"[Context: {header_path}]\n\n{text}"
            
            # ì„¹ì…˜ íƒ€ì… ê²°ì •
            section_type, section_num = _determine_section_type(headers)
            section_display = headers.get("H4") or headers.get("H3") or headers.get("H2") or headers.get("H1")
            
            chunks.append({
                "text": text.strip(),
                "index": idx,
                "metadata": {
                    "doc_name": doc_name,
                    "doc_title": sop_id or doc_name,
                    "sop_id": sop_id,
                    "article_num": section_num,
                    "article_type": section_type,
                    "section": section_display,
                    "section_path": header_path,
                    "section_path_readable": header_path,
                    "H1": headers.get("H1"),
                    "H2": headers.get("H2"),
                    "H3": headers.get("H3"),
                    "H4": headers.get("H4"),
                    "chunk_part": i + 1 if is_split else None,
                    "total_parts": len(text_chunks) if is_split else None,
                }
            })
            idx += 1
    
    state["chunks"] = chunks
    return state


def node_finalize(state: PipelineState) -> PipelineState:
    """
    6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì •ë¦¬
    """
    chunks = state.get("chunks", [])
    
    if not chunks:
        state["success"] = False
        state["errors"] = ["ì²­í¬ ìƒì„± ì‹¤íŒ¨: ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."]
    else:
        state["success"] = True
    
    # í†µê³„ ì¶”ê°€
    state["metadata"]["total_chunks"] = len(chunks)
    state["metadata"]["quality_score"] = state.get("quality_score", 0)
    state["metadata"]["conversion_method"] = state.get("conversion_method", "unknown")
    
    return state


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def should_fallback(state: PipelineState) -> Literal["fallback", "validate"]:
    """ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°±ìœ¼ë¡œ ë¼ìš°íŒ…"""
    if state.get("errors") or not state.get("markdown"):
        return "fallback"
    return "validate"


def should_repair(state: PipelineState) -> Literal["repair", "split"]:
    """í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë³´ì •ìœ¼ë¡œ ë¼ìš°íŒ…"""
    quality_score = state.get("quality_score", 0)
    retry_count = state.get("retry_count", 0)
    
    # í’ˆì§ˆì´ ë‚®ê³  ì¬ì‹œë„ íšŸìˆ˜ê°€ 2íšŒ ë¯¸ë§Œì´ë©´ ë³´ì •
    if quality_score < 0.5 and retry_count < 2:
        return "repair"
    return "split"


def is_failed(state: PipelineState) -> Literal["end", "continue"]:
    """ì‹¤íŒ¨ ìƒíƒœë©´ ì¢…ë£Œ"""
    if state.get("errors") and state.get("success") == False:
        return "end"
    return "continue"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _convert_docx(filename: str, content: bytes) -> tuple:
    """DOCX â†’ Markdown"""
    from docx import Document
    
    doc = Document(BytesIO(content))
    md_lines = []
    metadata = {}
    
    sop_pattern = re.compile(r'((?:EQ-)?SOP[-_]?\d{4,5})', re.IGNORECASE)
    
    # ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ
    main_sections = ['ëª©ì ', 'Purpose', 'ì ìš© ë²”ìœ„', 'Scope', 'ì •ì˜', 'Definitions',
                     'ì±…ì„', 'Responsibilities', 'ì ˆì°¨', 'Procedure', 
                     'ì°¸ê³ ë¬¸í—Œ', 'Reference', 'ì²¨ë¶€', 'Attachments']
    
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            md_lines.append("")
            continue
        
        # SOP ID ì¶”ì¶œ
        sop_match = sop_pattern.search(text)
        if sop_match and "sop_id" not in metadata:
            sop_id = sop_match.group(1).upper().replace('_', '-')
            if not sop_id.startswith('EQ-'):
                sop_id = 'EQ-' + sop_id
            metadata["sop_id"] = sop_id
        
        # í—¤ë” ë ˆë²¨ ê²°ì •
        header_level = None
        
        # Word ìŠ¤íƒ€ì¼ ê¸°ë°˜
        style_name = para.style.name.lower() if para.style else ""
        if 'heading 1' in style_name or 'title' in style_name:
            header_level = 1
        elif 'heading 2' in style_name:
            header_level = 2
        elif 'heading 3' in style_name:
            header_level = 3
        elif 'heading 4' in style_name:
            header_level = 4
        
        # íŒ¨í„´ ê¸°ë°˜ ê°ì§€
        if not header_level:
            for section in main_sections:
                if text.startswith(section) or re.match(rf'^\d+\s+{section}', text):
                    header_level = 2
                    break
            
            if not header_level:
                if re.match(r'^\d+\.\d+\.\d+\s+', text):
                    header_level = 4
                elif re.match(r'^\d+\.\d+\s+', text):
                    header_level = 3
                elif re.match(r'^\d+\.?\s+[ê°€-í£A-Za-z]', text):
                    header_level = 2
                elif re.match(r'^[ê°€-í£A-Z][ê°€-í£\s\(\)/Â·\-]+\s*\([A-Za-z\s&/\-:]+\)\s*$', text):
                    header_level = 3
        
        if header_level:
            md_lines.append(f"{'#' * header_level} {text}")
        else:
            md_lines.append(text)
    
    # í…Œì´ë¸” ì²˜ë¦¬
    for table in doc.tables:
        md_lines.append("")
        md_lines.append(_table_to_markdown(table))
    
    return '\n'.join(md_lines), metadata


def _convert_pdf_with_fallback(filename: str, content: bytes) -> tuple:
    """PDF ë³€í™˜ (ë‹¤ì¤‘ í´ë°±)"""
    
    # 1ìˆœìœ„: Docling
    try:
        from docling.document_converter import DocumentConverter
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as f:
            f.write(content)
            temp_path = f.name
        
        try:
            converter = DocumentConverter()
            result = converter.convert(temp_path)
            markdown = result.document.export_to_markdown()
            return markdown, {"parser": "docling"}, "docling"
        finally:
            os.unlink(temp_path)
    except:
        pass
    
    # 2ìˆœìœ„: PyMuPDF
    try:
        import fitz
        pdf = fitz.open(stream=content, filetype="pdf")
        md_lines = []
        for page_num, page in enumerate(pdf):
            text = page.get_text()
            if text.strip():
                md_lines.append(f"<!-- Page {page_num + 1} -->")
                md_lines.append(text)
        return '\n'.join(md_lines), {"parser": "pymupdf"}, "pymupdf"
    except:
        pass
    
    # 3ìˆœìœ„: PyPDF2
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(BytesIO(content))
        md_lines = []
        for i, page in enumerate(reader.pages):
            text = page.extract_text() or ''
            if text.strip():
                md_lines.append(f"<!-- Page {i + 1} -->")
                md_lines.append(text)
        return '\n'.join(md_lines), {"parser": "pypdf2"}, "pypdf2"
    except:
        pass
    
    raise Exception("ëª¨ë“  PDF íŒŒì„œ ì‹¤íŒ¨")


def _convert_html(filename: str, content: bytes) -> tuple:
    """HTML â†’ Markdown"""
    from bs4 import BeautifulSoup
    
    html = content.decode('utf-8', errors='ignore')
    soup = BeautifulSoup(html, 'html.parser')
    
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    md_lines = []
    title = soup.title.string if soup.title else filename
    md_lines.append(f"# {title}")
    md_lines.append("")
    
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
        if tag.name.startswith('h'):
            level = int(tag.name[1])
            md_lines.append(f"{'#' * level} {tag.get_text(strip=True)}")
        elif tag.name == 'li':
            md_lines.append(f"- {tag.get_text(strip=True)}")
        else:
            text = tag.get_text(strip=True)
            if text:
                md_lines.append(text)
        md_lines.append("")
    
    return '\n'.join(md_lines), {"title": title}


def _convert_text_to_markdown(text: str) -> str:
    """í…ìŠ¤íŠ¸ â†’ ë§ˆí¬ë‹¤ìš´ (í—¤ë” ì¶”ë¡ )"""
    lines = text.split('\n')
    md_lines = []
    
    main_sections = ['ëª©ì ', 'ì ìš© ë²”ìœ„', 'ì •ì˜', 'ì±…ì„', 'ì ˆì°¨', 'ì°¸ê³ ë¬¸í—Œ', 'ì²¨ë¶€']
    
    for line in lines:
        stripped = line.strip()
        if not stripped:
            md_lines.append("")
            continue
        
        is_header = False
        for section in main_sections:
            if stripped.startswith(section):
                md_lines.append(f"## {stripped}")
                is_header = True
                break
        
        if not is_header:
            if re.match(r'^\d+\.\d+\.\d+\s+', stripped):
                md_lines.append(f"#### {stripped}")
            elif re.match(r'^\d+\.\d+\s+', stripped):
                md_lines.append(f"### {stripped}")
            elif re.match(r'^\d+\.?\s+[ê°€-í£A-Za-z]', stripped):
                md_lines.append(f"## {stripped}")
            else:
                md_lines.append(stripped)
    
    return '\n'.join(md_lines)


def _pdf_fallback_extract(content: bytes) -> str:
    """PDF í´ë°± ì¶”ì¶œ"""
    try:
        import pdfplumber
        with pdfplumber.open(BytesIO(content)) as pdf:
            texts = [page.extract_text() or '' for page in pdf.pages]
            return '\n\n'.join(texts)
    except:
        pass
    
    # ìµœí›„ì˜ ìˆ˜ë‹¨
    return content.decode('latin-1', errors='ignore')


def _docx_fallback_extract(content: bytes) -> str:
    """DOCX í´ë°±: XML ì§ì ‘ íŒŒì‹±"""
    import zipfile
    from xml.etree import ElementTree
    
    try:
        with zipfile.ZipFile(BytesIO(content)) as zf:
            xml_content = zf.read('word/document.xml')
            tree = ElementTree.fromstring(xml_content)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
            texts = []
            for t in tree.iter('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'):
                if t.text:
                    texts.append(t.text)
            
            return '\n'.join(texts)
    except:
        return ""


def _table_to_markdown(table) -> str:
    """Word í…Œì´ë¸” â†’ Markdown"""
    rows = []
    for row in table.rows:
        cells = [cell.text.strip().replace('\n', ' ') for cell in row.cells]
        rows.append(cells)
    
    if not rows:
        return ""
    
    md_lines = []
    md_lines.append("| " + " | ".join(rows[0]) + " |")
    md_lines.append("| " + " | ".join(["---"] * len(rows[0])) + " |")
    for row in rows[1:]:
        while len(row) < len(rows[0]):
            row.append("")
        md_lines.append("| " + " | ".join(row[:len(rows[0])]) + " |")
    
    return '\n'.join(md_lines)


def _infer_headers(markdown: str) -> str:
    """í—¤ë” ì¶”ë¡  ì‚½ì…"""
    lines = markdown.split('\n')
    result = []
    
    main_sections = ['ëª©ì ', 'ì ìš© ë²”ìœ„', 'ì •ì˜', 'ì±…ì„', 'ì ˆì°¨', 'ì°¸ê³ ë¬¸í—Œ', 'ì²¨ë¶€']
    
    for line in lines:
        stripped = line.strip()
        
        # ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ë©´ H2
        matched = False
        for section in main_sections:
            if stripped.startswith(section):
                result.append(f"## {stripped}")
                matched = True
                break
        
        if not matched:
            # ìˆ«ì íŒ¨í„´
            if re.match(r'^\d+\.\d+\.\d+\s+', stripped):
                result.append(f"#### {stripped}")
            elif re.match(r'^\d+\.\d+\s+', stripped):
                result.append(f"### {stripped}")
            elif re.match(r'^\d+\.?\s+[ê°€-í£A-Za-z]', stripped):
                result.append(f"## {stripped}")
            else:
                result.append(line)
    
    return '\n'.join(result)


def _repair_tables(markdown: str) -> str:
    """ê¹¨ì§„ í…Œì´ë¸” ë³µêµ¬"""
    lines = markdown.split('\n')
    result = []
    in_table = False
    table_cols = 0
    
    for line in lines:
        if line.strip().startswith('|') and line.strip().endswith('|'):
            cols = line.count('|') - 1
            
            if not in_table:
                in_table = True
                table_cols = cols
                result.append(line)
                # êµ¬ë¶„ì„ ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                if len(result) >= 1:
                    next_idx = len(result)
            else:
                # ì—´ ìˆ˜ ë§ì¶”ê¸°
                while line.count('|') - 1 < table_cols:
                    line = line.rstrip('|') + ' |'
                result.append(line)
        else:
            in_table = False
            result.append(line)
    
    return '\n'.join(result)


def _split_recursive(text: str, chunk_size: int, overlap: int) -> List[str]:
    """ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• """
    if len(text) <= chunk_size:
        return [text]
    
    separators = ["\n\n", "\n| ", "\n", ". ", "ã€‚", " ", ""]
    is_table = text.strip().startswith('|') or '\n|' in text
    effective_overlap = 0 if is_table else overlap
    
    for sep in separators:
        if sep in text:
            parts = text.split(sep)
            chunks = []
            current = ""
            
            for part in parts:
                if len(current) + len(part) + len(sep) <= chunk_size:
                    current = current + sep + part if current else part
                else:
                    if current:
                        chunks.append(current)
                    if len(part) > chunk_size:
                        chunks.extend(_split_recursive(part, chunk_size, overlap))
                        current = ""
                    else:
                        current = part
            
            if current:
                chunks.append(current)
            
            return chunks
    
    # ê°•ì œ ë¶„í• 
    step = chunk_size - effective_overlap if effective_overlap > 0 else chunk_size
    return [text[i:i+chunk_size] for i in range(0, len(text), step)]


def _determine_section_type(headers: Dict) -> tuple:
    """ì„¹ì…˜ íƒ€ì… ê²°ì •"""
    section_type = "text"
    section_num = None
    
    if headers.get("H4"):
        section_type = "subsubsection"
        match = re.match(r'^(\d+\.\d+\.\d+)', headers["H4"])
        if match:
            section_num = match.group(1)
    elif headers.get("H3"):
        section_type = "subsection"
        match = re.match(r'^(\d+\.\d+)', headers["H3"])
        if match:
            section_num = match.group(1)
    elif headers.get("H2"):
        section_type = "section"
        match = re.match(r'^(\d+)', headers["H2"])
        if match:
            section_num = match.group(1)
    
    return section_type, section_num


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LangGraph íŒŒì´í”„ë¼ì¸ ë¹Œë”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_pipeline():
    """
    LangGraph íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    """
    try:
        from langgraph.graph import StateGraph, END
    except ImportError:
        raise ImportError("langgraph íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install langgraph")
    
    # ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(PipelineState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("load", node_load)
    workflow.add_node("convert", node_convert)
    workflow.add_node("fallback", node_convert_fallback)
    workflow.add_node("validate", node_validate)
    workflow.add_node("repair", node_repair)
    workflow.add_node("split", node_split)
    workflow.add_node("optimize", node_optimize)
    workflow.add_node("finalize", node_finalize)
    
    # ì—£ì§€ ì •ì˜ (íë¦„)
    workflow.set_entry_point("load")
    
    workflow.add_edge("load", "convert")
    
    workflow.add_conditional_edges(
        "convert",
        should_fallback,
        {
            "fallback": "fallback",
            "validate": "validate"
        }
    )
    
    workflow.add_edge("fallback", "validate")
    
    workflow.add_conditional_edges(
        "validate",
        should_repair,
        {
            "repair": "repair",
            "split": "split"
        }
    )
    
    workflow.add_edge("repair", "split")
    workflow.add_edge("split", "optimize")
    workflow.add_edge("optimize", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_document(
    filename: str,
    content: bytes,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    debug: bool = False
) -> dict:
    """
    ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
    
    LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    """
    # ì´ˆê¸° ìƒíƒœ
    initial_state: PipelineState = {
        "filename": filename,
        "content": content,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_type": "",
        "markdown": "",
        "metadata": {},
        "sections": [],
        "chunks": [],
        "quality_score": 0.0,
        "conversion_method": "",
        "errors": [],
        "warnings": [],
        "retry_count": 0,
        "success": False,
    }
    
    try:
        # LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = build_pipeline()
        result = pipeline.invoke(initial_state)
        
        if debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
            print(f"{'='*60}")
            print(f"   íŒŒì¼: {filename}")
            print(f"   ë³€í™˜ ë°©ë²•: {result.get('conversion_method')}")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0%}")
            print(f"   ì´ ì²­í¬: {len(result.get('chunks', []))}")
            if result.get('warnings'):
                print(f"   âš ï¸ ê²½ê³ : {result['warnings']}")
            if result.get('errors'):
                print(f"   âŒ ì—ëŸ¬: {result['errors']}")
        
        return result
        
    except ImportError:
        # LangGraph ì—†ìœ¼ë©´ ì‹¬í”Œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        if debug:
            print("âš ï¸ LangGraph ì—†ìŒ, ì‹¬í”Œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
        return _simple_pipeline(initial_state, debug)


def _simple_pipeline(state: PipelineState, debug: bool = False) -> dict:
    """
    LangGraph ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ì‹¬í”Œ íŒŒì´í”„ë¼ì¸
    """
    state = node_load(state)
    if state.get("errors"):
        return state
    
    state = node_convert(state)
    if not state.get("markdown"):
        state = node_convert_fallback(state)
    
    state = node_validate(state)
    
    if state.get("quality_score", 0) < 0.5:
        state = node_repair(state)
    
    state = node_split(state)
    state = node_optimize(state)
    state = node_finalize(state)
    
    if debug:
        print(f"\nğŸ“Š ì‹¬í”Œ íŒŒì´í”„ë¼ì¸ ê²°ê³¼: {len(state.get('chunks', []))} ì²­í¬")
    
    return state


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²°ê³¼ ë³€í™˜ í—¬í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def state_to_chunks(state: dict) -> List[Chunk]:
    """ìƒíƒœë¥¼ Chunk ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        Chunk(
            text=c["text"],
            index=c["index"],
            metadata=c["metadata"]
        )
        for c in state.get("chunks", [])
    ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´
    test_md = """# EQ-SOP-00010 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œ

## ëª©ì  Purpose

ë³¸ ê¸°ì¤€ì„œëŠ” í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ ì‘ì„±, ê²€í† , ìŠ¹ì¸ì— ê´€í•œ ê¸°ì¤€ì„ ì •í•œë‹¤.

## ì ìš© ë²”ìœ„ Scope

ë³¸ ê¸°ì¤€ì„œëŠ” íšŒì‚¬ ë‚´ í’ˆì§ˆê´€ë¦¬ í™œë™ ì „ë°˜ì— ì ìš©ëœë‹¤.

## ì ˆì°¨ Procedure

í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œëŠ” ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œë‹¤.
"""
    
    result = process_document("test.md", test_md.encode(), chunk_size=300, debug=True)
    
    print(f"\nâœ… ì„±ê³µ: {result.get('success')}")
    print(f"ğŸ“Š ì²­í¬ ìˆ˜: {len(result.get('chunks', []))}")
    
    for chunk in result.get('chunks', [])[:3]:
        print(f"\nğŸ“ {chunk['metadata'].get('section_path_readable', 'N/A')}")
        print(f"   {chunk['text'][:60]}...")
