"""
RAG ì±—ë´‡ API v11.0 + Agent (Z.AI)

ğŸ”¥ v11.0 ë³€ê²½ì‚¬í•­:
- LLM ë°±ì—”ë“œ ë³€ê²½: Ollama â†’ Z.AI GLM-4.7-Flash
- ì—ì´ì „íŠ¸ ë„êµ¬ ì„±ëŠ¥ ê°•í™”
- LangSmith ì¶”ì  ì§€ì› ë° ìµœì í™”
- ë˜ë¬»ê¸° ë¡œì§ ì œê±° ë° ê²€ìƒ‰ ê²°ê³¼ ì§ì ‘ ì¶œë ¥
"""

# ğŸ”¥ .env íŒŒì¼ ìë™ ë¡œë“œ (ë‹¤ë¥¸ importë³´ë‹¤ ë¨¼ì €!)
from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import torch
import time
import uuid

from rag.sql_store import SQLStore
sql_store = SQLStore()
sql_store.init_db()

# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©)
from rag import (
    load_document,
    get_supported_extensions,
    create_chunks,
    create_chunks_from_blocks,
    get_available_methods,
    CHUNK_METHODS,
)
from rag import vector_store
from rag.prompt import build_rag_prompt, build_chunk_prompt
from rag.llm import (
    get_llm_response,
    ZaiLLM,
    OllamaLLM,
    analyze_search_results,
    HUGGINGFACE_MODELS,
)

# ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ (v9.2)
try:
    from rag.document_pipeline import process_document, state_to_chunks, Chunk
    LANGGRAPH_AVAILABLE = True
    print("âœ… LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    print(f"âš ï¸ LangGraph ì‚¬ìš© ë¶ˆê°€, ë ˆê±°ì‹œ ëª¨ë“œ: {e}")


app = FastAPI(title="RAG Chatbot API", version="9.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 500
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 7  # ğŸ”¥ 5 -> 7 ìƒí–¥
DEFAULT_SIMILARITY_THRESHOLD = 0.30  # ğŸ”¥ 0.35 -> 0.30 (ë” ë§ì€ ë§¥ë½ í™•ë³´)
USE_LANGGRAPH = True  # ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì—¬ë¶€

PRESET_MODELS = {
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5-large": "intfloat/multilingual-e5-large",
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
    "bge-m3": "BAAI/bge-m3",
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
}

device = "cuda" if torch.cuda.is_available() else "cpu"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë©”ëª¨ë¦¬)
chat_histories: Dict[str, List[Dict]] = {}

# Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ (ì‹±ê¸€í†¤)
_graph_store = None


def get_graph_store():
    """Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ ì‹±ê¸€í†¤"""
    global _graph_store
    if _graph_store is None:
        from rag.graph_store import Neo4jGraphStore
        _graph_store = Neo4jGraphStore()
        _graph_store.connect()
    return _graph_store


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "glm-4.7-flash"
    llm_backend: str = "zai"  # ğŸ”¥ ê¸°ë³¸ê°’ zaië¡œ ë³€ê²½
    temperature: float = 0.7
    filter_doc: Optional[str] = None
    language: str = "ko"
    max_tokens: int = 512
    similarity_threshold: Optional[float] = None
    include_sources: bool = True


class LLMRequest(BaseModel):
    prompt: str
    model: str = "qwen2.5:3b"
    backend: str = "ollama"
    max_tokens: int = 256
    temperature: float = 0.1


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"
    delete_from_neo4j: bool = True  # ğŸ”¥ Neo4jì—ì„œë„ ì‚­ì œ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model: str) -> str:
    """ëª¨ë¸ í”„ë¦¬ì…‹ â†’ ì „ì²´ ê²½ë¡œ"""
    return PRESET_MODELS.get(model, model)


def format_context(results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ â†’ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    context_parts = []
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        text = r.get("text", "")
        similarity = r.get("similarity", 0)
        
        # ğŸ”¥ v9.2: ê°œì„ ëœ ì¶œì²˜ í‘œì‹œ
        sop_id = meta.get("sop_id", "")
        section_path = meta.get("section_path", "")
        page = meta.get("page", "")
        article_num = meta.get("article_num", "")
        
        # ì¶œì²˜ í—¤ë” êµ¬ì„±
        source_parts = []
        if sop_id:
            source_parts.append(f"[{sop_id}]")
        if section_path:
            source_parts.append(f"> {section_path}")
        if page:
            source_parts.append(f"(p.{page})")
        if similarity:
            source_parts.append(f"ê´€ë ¨ë„: {similarity:.0%}")
        
        source_header = " ".join(source_parts) if source_parts else f"[ë¬¸ì„œ {i}]"
        
        context_parts.append(f"ğŸ“„ {source_header}\n{text}")
    
    return "\n\n---\n\n".join(context_parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG Chatbot API v9.2",
        "features": [
            "LangGraph íŒŒì´í”„ë¼ì¸",
            "í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì ",
            "Parent-Child ê³„ì¸µ",
            "Question ì¶”ì  (Neo4j)",
            "ChromaDB + Neo4j ë™ê¸°í™” ì‚­ì œ"
        ],
        "endpoints": {
            "upload": "/rag/upload",
            "search": "/rag/search",
            "chat": "/chat",
            "ask": "/rag/ask",
            "graph": "/graph/*"
        },
        "langgraph_enabled": LANGGRAPH_AVAILABLE and USE_LANGGRAPH
    }


@app.get("/health")
def health():
    return {
        "status": "healthy",
        "cuda": torch.cuda.is_available(),
        "device": device,
        "ollama": OllamaLLM.is_available(),
        "langgraph": LANGGRAPH_AVAILABLE
    }


@app.get("/models/embedding")
def list_embedding_models():
    return {
        "presets": PRESET_MODELS,
        "specs": vector_store.EMBEDDING_MODEL_SPECS,
        "compatible": vector_store.filter_compatible_models()
    }


@app.get("/models/llm")
def list_llm_models():
    available_ollama = []
    if OllamaLLM.is_available():
        available_ollama = OllamaLLM.list_models()
    return {
        "ollama": {"presets": OLLAMA_MODELS, "available": available_ollama},
        "huggingface": HUGGINGFACE_MODELS
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ API ì—”ë“œí¬ì¸íŠ¸ - ì—…ë¡œë“œ (LangGraph v9.2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    use_langgraph: bool = Form(True),  # ğŸ”¥ LangGraph ì‚¬ìš© ì—¬ë¶€
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ (LangGraph v9.2 íŒŒì´í”„ë¼ì¸)
    
    - ChromaDBì— ë²¡í„° ì €ì¥
    - Neo4jì— ê·¸ë˜í”„ ì €ì¥
    - í˜ì´ì§€ ë²ˆí˜¸, Parent-Child ê³„ì¸µ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    start_time = time.time()
    
    try:
        content = await file.read()
        filename = file.filename
        
        print(f"\n{'='*70}")
        print(f"ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ: {filename}")
        print(f"{'='*70}")
        
        # ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ vs ë ˆê±°ì‹œ ì„ íƒ
        if use_langgraph and LANGGRAPH_AVAILABLE and USE_LANGGRAPH:
            # === LangGraph íŒŒì´í”„ë¼ì¸ (v9.2) ===
            print(f"   ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
            
            result = process_document(
                filename=filename,
                content=content,
                chunk_size=chunk_size,
                chunk_overlap=overlap,
                debug=True
            )
            
            if not result.get("success"):
                errors = result.get("errors", ["ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"])
                raise HTTPException(400, f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {errors}")
            
            chunks = state_to_chunks(result)
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            metadata_base = result.get("metadata", {})
            sop_id = metadata_base.get("doc_id") or metadata_base.get("sop_id")
            
            # ğŸ”¥ IDê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ëìë¦¬ ìˆ«ìë¡œë¼ë„ ìœ ì¶” ì‹œë„
            if not sop_id:
                import re
                id_match = re.search(r'([A-Z0-9]+-[A-Z0-9]+-\d+)', filename)
                if id_match:
                    sop_id = id_match.group(1)
                else:
                    sop_id = filename.split('.')[0] # ìµœí›„ì˜ ìˆ˜ë‹¨: íŒŒì¼ëª…
            
            # ì œëª© ì„¤ì •: ì›ë³¸ íŒŒì¼ëª… ìœ ì§€ (ì‚¬ìš©ì ìš”ì²­)
            doc_title = filename 
            extracted_title = metadata_base.get("title")
            if extracted_title and extracted_title not in filename:
                doc_title = f"{filename} ({extracted_title})"
            
            print(f"   DOC ID: {sop_id}")
            print(f"   ì œëª©: {doc_title}")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0%}")
            print(f"   ë³€í™˜ ë°©ë²•: {result.get('conversion_method')}")
            print(f"   ì´ ì²­í¬: {len(chunks)}")
            
            pipeline_version = "langgraph-v9.2"
            quality_score = result.get("quality_score", 0)
            conversion_method = result.get("conversion_method", "unknown")
            
        else:
            # === ë ˆê±°ì‹œ íŒŒì´í”„ë¼ì¸ ===
            print(f"   ğŸ“¦ ë ˆê±°ì‹œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
            
            parsed_doc = load_document(filename, content)
            sop_id = parsed_doc.metadata.get("sop_id")
            doc_title = parsed_doc.metadata.get("title", filename)
            
            print(f"   SOP ID: {sop_id}")
            print(f"   ì œëª©: {doc_title}")
            print(f"   ì´ ë¸”ë¡ ìˆ˜: {len(parsed_doc.blocks)}")
            
            if chunk_method == "article" and parsed_doc.blocks:
                chunks = create_chunks_from_blocks(
                    parsed_doc,
                    chunk_size=chunk_size,
                    overlap=overlap,
                    method="recursive",
                    exclude_intro=True,
                )
            else:
                chunks = create_chunks(
                    parsed_doc.text,
                    chunk_size=chunk_size,
                    overlap=overlap,
                    method=chunk_method
                )
                for chunk in chunks:
                    chunk.metadata.update({
                        "doc_name": filename,
                        "doc_title": doc_title,
                        "sop_id": sop_id,
                    })
            
            # ë¹ˆ ì²­í¬ ì²´í¬
            if not chunks:
                chunks = create_chunks_from_blocks(
                    parsed_doc,
                    chunk_size=chunk_size,
                    overlap=overlap,
                    method="recursive",
                    exclude_intro=False,
                )
            
            pipeline_version = "legacy-v6.3"
            quality_score = None
            conversion_method = "legacy"
            metadata_base = parsed_doc.metadata
        
        if not chunks:
            raise HTTPException(400, "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # === ChromaDB ì €ì¥ ===
        model_path = resolve_model_path(model)
        texts = [c.text for c in chunks]
        metadatas = [
            {
                **c.metadata,
                "chunk_method": chunk_method,
                "model": model,
                "pipeline_version": pipeline_version,
            }
            for c in chunks
        ]
        
        vector_store.add_documents(
            texts=texts,
            metadatas=metadatas,
            collection_name=collection,
            model_name=model_path
        )
        print(f"   âœ… ChromaDB ì €ì¥ ì™„ë£Œ: {len(chunks)} ì²­í¬")
        
        # === PostgreSQL ì €ì¥ ===
        try:
            # ì›ë³¸ ë§ˆí¬ë‹¤ìš´ ê²°ì • (LangGraph ê²°ê³¼ ìš°ì„ , ì—†ìœ¼ë©´ ì²­í¬ í•©ì‚°)
            full_markdown = ""
            if use_langgraph and 'result' in locals() and result.get("markdown"):
                full_markdown = result.get("markdown")
            else:
                full_markdown = "\n\n".join([c.text for c in chunks])

            sql_store.save_document(
                sop_id=sop_id,
                title=doc_title or filename,
                markdown_content=full_markdown,
                pdf_binary=content if filename.lower().endswith(".pdf") else None,
                doc_metadata={
                    "doc_id": metadata_base.get("doc_id"),
                    "version": metadata_base.get("version"),
                    "effective_date": metadata_base.get("effective_date"),
                    "owning_dept": metadata_base.get("owning_dept"),
                    "filename": filename
                }
            )
        except Exception as sql_err:
            print(f"   âš ï¸ PostgreSQL ìƒì„¸ ì €ì¥ ì‹¤íŒ¨: {sql_err}")
            # í´ë°±: ê¸°ì¡´ ìœ ì € ì½”ë“œ ë°©ì‹ (í•„ìš” ì‹œ)
            # save_chunks_to_db(sop_id, filename, chunks)
        
        # === Neo4j ê·¸ë˜í”„ ì €ì¥ ===
        graph_uploaded = False
        graph_sections = 0
        
        try:
            from rag.graph_store import Neo4jGraphStore
            
            graph = get_graph_store()
            if graph.test_connection():
                # LangGraph ê²°ê³¼ì—ì„œ ê·¸ë˜í”„ ìƒì„±
                if use_langgraph and LANGGRAPH_AVAILABLE:
                    # ì§ì ‘ ì„¹ì…˜ ë°ì´í„°ë¡œ ê·¸ë˜í”„ ìƒì„±
                    _upload_to_neo4j_from_pipeline(graph, result, filename)
                else:
                    # ë ˆê±°ì‹œ: ParsedDocumentì—ì„œ ìƒì„±
                    from rag.graph_store import document_to_graph
                    document_to_graph(graph, parsed_doc, sop_id)
                
                graph_uploaded = True
                stats = graph.get_graph_stats()
                graph_sections = stats.get("sections", 0)
                print(f"   âœ… Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as graph_error:
            print(f"   âš ï¸ Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {graph_error}")
        
        elapsed = round(time.time() - start_time, 2)
        
        return {
            "success": True,
            "filename": filename,
            "sop_id": sop_id,
            "doc_title": doc_title,
            "chunks": len(chunks),
            "chunk_method": chunk_method,
            "pipeline_version": pipeline_version,
            "quality_score": quality_score,
            "conversion_method": conversion_method,
            "graph_uploaded": graph_uploaded,
            "elapsed_seconds": elapsed,
            "sample_metadata": metadatas[0] if metadatas else {},
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


def _upload_to_neo4j_from_pipeline(graph, result: dict, filename: str):
    """LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ Neo4jì— ì—…ë¡œë“œ (V22.0 ëŒ€ì‘)"""
    metadata = result.get("metadata", {})
    doc_id = metadata.get("doc_id") or "UNKNOWN"
    title = metadata.get("title") or filename
    version = metadata.get("version") or "1.0"
    effective_date = metadata.get("effective_date")
    owning_dept = metadata.get("owning_dept")
    
    # 1. Document ë…¸ë“œ ìƒì„±
    graph.create_document(
        doc_id=doc_id,
        title=title,
        version=version,
        effective_date=effective_date,
        owning_dept=owning_dept,
        metadata=metadata
    )
    
    # 2. DocumentType ì²˜ë¦¬ (ì½”ë“œ ê¸°ë°˜)
    doc_type_code = "SOP" # ê¸°ë³¸ê°’
    if "SOP" in doc_id: doc_type_code = "SOP"
    elif "WI" in doc_id: doc_type_code = "WI"
    
    graph.create_document_type(doc_type_code, "í‘œì¤€ì‘ì—…ì ˆì°¨ì„œ" if doc_type_code == "SOP" else "ì‘ì—…ì§€ì¹¨ì„œ", doc_type_code)
    graph.link_doc_to_type(doc_id, doc_type_code)
    
    # 3. Section ë…¸ë“œ ìƒì„± ë° ê´€ê³„ ì„¤ì •
    sections = result.get("sections", [])
    
    for sec in sections:
        headers = sec.get("headers", {})
        content = sec.get("content", "")
        page = sec.get("page", 1)
        parent_name = sec.get("parent")
        clause_meta = sec.get("clause_meta", {})
        
        # clause_level ë° section_id ìœ ì¶”
        clause_level = 0
        current_title = ""
        for level in range(6, 0, -1):
            if headers.get(f"H{level}"):
                clause_level = level
                current_title = headers[f"H{level}"]
                break
        
        clause_id = None
        import re
        num_match = re.match(r'^(\d+(?:\.\d+)*)', current_title)
        if num_match:
            clause_id = num_match.group(1)
        
        if not clause_id: continue
        
        section_id = f"{doc_id}:{clause_id}"
        main_section = clause_id.split('.')[0] if '.' in clause_id else clause_id
        
        # Section ë…¸ë“œ ìƒì„± (ìƒì„¸ ë©”íƒ€ë°ì´í„° í¬í•¨)
        graph.create_section(
            doc_id=doc_id,
            section_id=section_id,
            title=current_title,
            content=content,
            clause_level=clause_level,
            main_section=main_section,
            llm_meta=clause_meta,
            page=page
        )
        
        # 4. ê³„ì¸µ ê´€ê³„ (Parent-Child)
        if parent_name:
            # ë¶€ëª¨ ID ìœ ì¶” (ë‹¨ìˆœí™”: ê°™ì€ ë¬¸ì„œ ë‚´ì—ì„œ ì  í•˜ë‚˜ ëº€ íŒ¨í„´)
            if '.' in clause_id:
                parent_clause_id = '.'.join(clause_id.split('.')[:-1])
                parent_section_id = f"{doc_id}:{parent_clause_id}"
                graph.create_section_hierarchy(parent_section_id, section_id)
        
        # 5. Concept ì—°ë™ (intent_scope í™œìš©)
        intent_scope = clause_meta.get("intent_scope")
        if intent_scope:
            graph.create_concept(intent_scope, intent_scope, intent_scope)
            graph.link_section_to_concept(section_id, intent_scope)
            
        # 6. íƒ€ ë¬¸ì„œ ì–¸ê¸‰ (MENTIONS) ì¶”ì 
        mentions = re.findall(r'((?:EQ-)?SOP[-_]?\d{4,5})', content, re.IGNORECASE)
        for m in set(mentions):
            m_id = m.upper().replace('_', '-')
            if not m_id.startswith('EQ-'): m_id = 'EQ-' + m_id
            if m_id != doc_id:
                graph.link_section_to_mention_doc(section_id, m_id)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/search")
def search_documents(request: SearchRequest):
    """ë²¡í„° ê²€ìƒ‰"""
    model_path = resolve_model_path(request.model)
    threshold = request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    
    results = vector_store.search(
        query=request.query,
        collection_name=request.collection,
        model_name=model_path,
        n_results=request.n_results,
        filter_doc=request.filter_doc,
        similarity_threshold=threshold,
    )
    
    return {
        "query": request.query,
        "results": results,
        "count": len(results),
        "threshold": threshold,
    }


@app.post("/rag/search/advanced")
def search_advanced(request: SearchRequest):
    """ê³ ê¸‰ ê²€ìƒ‰ (í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨)"""
    model_path = resolve_model_path(request.model)
    threshold = request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    
    response = vector_store.search_advanced(
        query=request.query,
        collection_name=request.collection,
        model_name=model_path,
        n_results=request.n_results,
        filter_doc=request.filter_doc,
        similarity_threshold=threshold,
    )
    
    return response


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì±—ë´‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/chat")
def chat(request: ChatRequest):
    """ëŒ€í™”í˜• RAG ì±—ë´‡ (v10.1 - ë˜ë¬»ê¸°/ì¶”ì  ì œê±°, ì†ŒìŠ¤ í˜•ì‹ ìˆ˜ì •)"""
    session_id = request.session_id or str(uuid.uuid4())
    
    if session_id not in chat_histories:
        chat_histories[session_id] = []
    
    # 1. ë²¡í„° ê²€ìƒ‰
    model_path = resolve_model_path(request.embedding_model)
    threshold = request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    
    results, context = vector_store.search_with_context(
        query=request.message,
        collection_name=request.collection,
        model_name=model_path,
        n_results=request.n_results,
        filter_doc=request.filter_doc,
        similarity_threshold=threshold,
    )
    
    # ğŸ”¥ [ì¡°ì •] ì»¨í…ìŠ¤íŠ¸ê°€ ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì„ ë•Œë§Œ(ì˜ˆ: 400ì ë¯¸ë§Œ) ìµœì†Œí•œì˜ ë³´ì¶© ìˆ˜í–‰
    if len(context) < 400 and len(results) < 10:
        print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ({len(context)}ì). ìµœì†Œ ë³´ì¶© ì‹œë„...")
        extra_results, extra_context = vector_store.search_with_context(
            query=request.message,
            collection_name=request.collection,
            model_name=model_path,
            n_results=10,  # 15ì—ì„œ 10ìœ¼ë¡œ í•˜í–¥
            filter_doc=request.filter_doc,
            similarity_threshold=threshold * 0.7, # ì„ê³„ê°’ ë” ì™„í™”í•˜ì—¬ ì£¼ë³€ ë§¥ë½ í™•ë³´
        )
        if len(extra_context) > len(context):
            results, context = extra_results, extra_context
            print(f"âœ… ì»¨í…ìŠ¤íŠ¸ ë³´ì¶© ì™„ë£Œ ({len(context)}ì, {len(results)}ê°œ)")
    
    # ğŸ”¥ ë””ë²„ê·¸ ë¡œê·¸
    print(f"\n{'='*50}")
    print(f"ğŸ” ì§ˆë¬¸: {request.message}")
    print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    if results:
        for i, r in enumerate(results[:3]):
            sim = r.get('similarity', 0)
            meta = r.get('metadata', {})
            sop = meta.get('sop_id', '?')
            path = meta.get('section_path', '')[:40] if meta.get('section_path') else ''
            print(f"   [{i+1}] ìœ ì‚¬ë„: {sim:.2f} | {sop} | {path}...")
    print(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ê¸€ì")
    
    # 2. LLM ë‹µë³€ ìƒì„± (ë˜ë¬»ê¸° ë¡œì§ ì œê±°!)
    prompt = build_rag_prompt(request.message, context)
    print(f"ğŸ¤– LLM í˜¸ì¶œ ì‹œë„:")
    print(f"   - Backend: {request.llm_backend}")
    print(f"   - Model: {request.llm_model}")
    print(f"   - Prompt Prefix: {prompt[:100]}...")
    
    answer = get_llm_response(
        prompt=prompt,
        llm_model=request.llm_model,
        llm_backend=request.llm_backend,
        max_tokens=2048 # ğŸ”¥ Z.AI ë¶„ì„ê³¼ í•œêµ­ì–´ ë‹µë³€ì„ ìœ„í•´ 2048 ê¶Œì¥
    )
    
    print(f"ğŸ’¬ LLM ê²°ê³¼:")
    print(f"   - ë‹µë³€: {answer[:50]}..." if answer else "   - ë‹µë³€: (EMPTY)")
    print(f"   - ê¸¸ì´: {len(answer)} ê¸€ì")
    print(f"{'='*50}\n")
    
    # 3. íˆìŠ¤í† ë¦¬ ì €ì¥
    chat_histories[session_id].append({"role": "user", "content": request.message})
    chat_histories[session_id].append({"role": "assistant", "content": answer})
    
    # 4. ì†ŒìŠ¤ ì •ë³´ (í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì— ë§ì¶¤!)
    sources = []
    for r in results:
        meta = r.get("metadata", {})
        sources.append({
            "text": r.get("text", ""),
            "similarity": r.get("similarity", 0),
            "metadata": meta,
            "metadata_display": {
                "doc_name": meta.get("doc_name", "ë¬¸ì„œ"),
                "doc_title": meta.get("doc_title", ""),
                "sop_id": meta.get("sop_id", ""),
                "version": meta.get("version", ""),
                "section": meta.get("section", ""),
                "section_path": meta.get("section_path", ""),
                "section_path_readable": meta.get("section_path_readable", ""),
                "title": meta.get("title", ""),
                "page": meta.get("page", ""),
            }
        })
    
    return {
        "session_id": session_id,
        "answer": answer,
        "sources": sources,
        "needs_clarification": False
    }


@app.post("/rag/ask")
def ask_with_rag(request: AskRequest):
    """ë‹¨ì¼ ì§ˆë¬¸ RAG (Question ì¶”ì  í¬í•¨)"""
    model_path = resolve_model_path(request.embedding_model)
    threshold = request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    
    # ê²€ìƒ‰
    results, context = vector_store.search_with_context(
        query=request.query,
        collection_name=request.collection,
        model_name=model_path,
        n_results=request.n_results,
        filter_doc=request.filter_doc,
        similarity_threshold=threshold,
    )
    
    if not results:
        return {
            "query": request.query,
            "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "context_used": ""
        }
    
    # ğŸ”¥ ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    formatted_context = format_context(results)
    prompt = build_rag_prompt(request.query, formatted_context, request.language)
    
    answer = get_llm_response(
        prompt=prompt,
        llm_model=request.llm_model,
        llm_backend=request.llm_backend,
        max_tokens=request.max_tokens
    )
    
    # ğŸ”¥ Question ì¶”ì  (Neo4j)
    question_id = None
    try:
        from rag.graph_store import track_rag_question
        graph = get_graph_store()
        if graph.test_connection():
            question_id = track_rag_question(
                graph_store=graph,
                question_text=request.query,
                search_results=results,
                answer=answer,
                embedding_model=request.embedding_model,
                llm_model=request.llm_model
            )
    except Exception as e:
        print(f"âš ï¸ Question ì¶”ì  ì‹¤íŒ¨: {e}")
    
    sources = []
    if request.include_sources:
        sources = [
            {
                "doc_name": r.get("metadata", {}).get("doc_name"),
                "sop_id": r.get("metadata", {}).get("sop_id"),
                "section": r.get("metadata", {}).get("section"),
                "section_path": r.get("metadata", {}).get("section_path"),
                "page": r.get("metadata", {}).get("page"),
                "similarity": r.get("similarity"),
                "confidence": r.get("confidence"),
            }
            for r in results
        ]
    
    return {
        "query": request.query,
        "answer": answer,
        "sources": sources,
        "question_id": question_id,
        "context_used": formatted_context[:500] + "..." if len(formatted_context) > 500 else formatted_context
    }


@app.get("/chat/history/{session_id}")
def get_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = chat_histories.get(session_id, [])
    return {"session_id": session_id, "history": history, "count": len(history)}


@app.delete("/chat/history/{session_id}")
def clear_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    if session_id in chat_histories:
        del chat_histories[session_id]
        return {"success": True, "message": f"ì„¸ì…˜ {session_id} ì‚­ì œë¨"}
    return {"success": False, "message": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/llm/generate")
def generate_llm(request: LLMRequest):
    """LLM ì§ì ‘ í˜¸ì¶œ"""
    try:
        response = get_llm_response(
            prompt=request.prompt,
            llm_model=request.model,
            llm_backend=request.backend,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        return {"response": response, "model": request.model, "backend": request.backend}
    except Exception as e:
        raise HTTPException(500, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """
    ğŸ”¥ ë¬¸ì„œ ì‚­ì œ (ChromaDB + Neo4j ë™ì‹œ ì‚­ì œ)
    """
    result = {"chromadb": None, "neo4j": None}
    
    # 1. ChromaDB ì‚­ì œ
    chroma_result = vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )
    result["chromadb"] = chroma_result
    
    # 2. Neo4j ì‚­ì œ (ì˜µì…˜)
    if request.delete_from_neo4j:
        try:
            graph = get_graph_store()
            if graph.test_connection():
                # doc_nameì—ì„œ sop_id ì¶”ì¶œ ì‹œë„
                import re
                sop_match = re.search(r'(EQ-SOP-\d+)', request.doc_name, re.IGNORECASE)
                if sop_match:
                    sop_id = sop_match.group(1).upper()
                    neo4j_result = graph.delete_document(sop_id)
                    result["neo4j"] = {"success": True, "sop_id": sop_id, "deleted": neo4j_result}
                else:
                    result["neo4j"] = {"success": False, "message": "SOP IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"}
        except Exception as e:
            result["neo4j"] = {"success": False, "error": str(e)}
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    success = chroma_result.get("success", False)
    
    return {
        "success": success,
        "doc_name": request.doc_name,
        "details": result
    }


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    return {"collections": [vector_store.get_collection_info(name) for name in collections]}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› í¬ë§·"""
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²•"""
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - Neo4j ê·¸ë˜í”„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/status")
def graph_status():
    """Neo4j ì—°ê²° ìƒíƒœ"""
    try:
        graph = get_graph_store()
        connected = graph.test_connection()
        stats = graph.get_graph_stats() if connected else {}
        return {"connected": connected, "stats": stats}
    except Exception as e:
        return {"connected": False, "error": str(e)}


@app.post("/graph/init")
def graph_init():
    """Neo4j ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
    try:
        graph = get_graph_store()
        graph.init_schema()
        return {"success": True, "message": "ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/clear")
def graph_clear():
    """Neo4j ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        graph = get_graph_store()
        graph.clear_all()
        return {"success": True, "message": "ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/graph/upload")
async def graph_upload_document(
    file: UploadFile = File(...),
    use_langgraph: bool = Form(True)
):
    """ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ë¡œë§Œ ì—…ë¡œë“œ"""
    try:
        content = await file.read()
        filename = file.filename
        
        if use_langgraph and LANGGRAPH_AVAILABLE:
            result = process_document(filename, content, debug=True)
            if not result.get("success"):
                raise HTTPException(400, f"ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")
            
            graph = get_graph_store()
            _upload_to_neo4j_from_pipeline(graph, result, filename)
            
            return {
                "success": True,
                "filename": filename,
                "sop_id": result.get("metadata", {}).get("sop_id"),
                "sections": len(result.get("sections", [])),
                "pipeline": "langgraph"
            }
        else:
            from rag.graph_store import document_to_graph
            
            parsed_doc = load_document(filename, content)
            sop_id = parsed_doc.metadata.get("sop_id")
            
            graph = get_graph_store()
            document_to_graph(graph, parsed_doc, sop_id)
            
            return {
                "success": True,
                "filename": filename,
                "sop_id": sop_id,
                "blocks": len(parsed_doc.blocks),
                "pipeline": "legacy"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/documents")
def graph_list_documents():
    """Neo4j ë¬¸ì„œ ëª©ë¡"""
    try:
        graph = get_graph_store()
        docs = graph.get_all_documents()
        return {"documents": docs, "count": len(docs)}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}")
def graph_get_document(sop_id: str):
    """íŠ¹ì • ë¬¸ì„œ ìƒì„¸"""
    try:
        graph = get_graph_store()
        doc = graph.get_document(sop_id)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return doc
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/document/{sop_id}")
def graph_delete_document(sop_id: str):
    """Neo4jì—ì„œ ë¬¸ì„œ ì‚­ì œ"""
    try:
        graph = get_graph_store()
        result = graph.delete_document(sop_id)
        return {"success": True, "sop_id": sop_id, "result": result}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/hierarchy")
def graph_get_hierarchy(sop_id: str):
    """ë¬¸ì„œ ì„¹ì…˜ ê³„ì¸µ"""
    try:
        graph = get_graph_store()
        hierarchy = graph.get_section_hierarchy(sop_id)
        return {"sop_id": sop_id, "hierarchy": hierarchy}
    except Exception as e:
        raise HTTPException(500, f"ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/references")
def graph_get_references(sop_id: str):
    """ë¬¸ì„œ ì°¸ì¡° ê´€ê³„"""
    try:
        graph = get_graph_store()
        refs = graph.get_document_references(sop_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return refs
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì°¸ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/sections")
def graph_search_sections(keyword: str, sop_id: str = None):
    """ì„¹ì…˜ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(keyword, sop_id)
        return {"keyword": keyword, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/terms")
def graph_search_terms(term: str):
    """ìš©ì–´ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_by_term(term)
        return {"term": term, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ìš©ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ API ì—”ë“œí¬ì¸íŠ¸ - Question ì¶”ì 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/questions")
def graph_list_questions(limit: int = 50, session_id: str = None):
    """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        questions = graph.get_question_history(session_id=session_id, limit=limit)
        return {"questions": questions, "count": len(questions)}
    except Exception as e:
        raise HTTPException(500, f"ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/questions/{question_id}/sources")
def graph_get_question_sources(question_id: str):
    """ì§ˆë¬¸ì´ ì°¸ì¡°í•œ ì„¹ì…˜ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        result = graph.get_question_sources(question_id)
        if not result:
            raise HTTPException(404, f"ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {question_id}")
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/stats/section-usage")
def graph_section_usage_stats(sop_id: str = None):
    """ì„¹ì…˜ ì‚¬ìš© í†µê³„"""
    try:
        graph = get_graph_store()
        stats = graph.get_section_usage_stats(sop_id)
        return {"stats": stats, "count": len(stats)}
    except Exception as e:
        raise HTTPException(500, f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– API ì—”ë“œí¬ì¸íŠ¸ - ì—ì´ì „íŠ¸ (NEW!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from rag.agent import (
        init_agent_tools, 
        run_agent, 
        create_agent,
        AGENT_TOOLS,
        LANGCHAIN_AVAILABLE,
        LANGGRAPH_AGENT_AVAILABLE,
        ZAI_AVAILABLE
    )
    AGENT_AVAILABLE = True
    print("âœ… ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    AGENT_AVAILABLE = False
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False
    ZAI_AVAILABLE = False
    print(f"âš ï¸ ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class AgentRequest(BaseModel):
    """ì—ì´ì „íŠ¸ ìš”ì²­"""
    message: str
    session_id: Optional[str] = None
    llm_model: str = "glm-4.7-flash"
    embedding_model: str = "multilingual-e5-small" # ì¶”ê°€
    n_results: int = DEFAULT_N_RESULTS # ğŸ”¥ ì¶”ê°€
    use_langgraph: bool = True  # LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€


@app.post("/agent/chat")
def agent_chat(request: AgentRequest):
    """
    ğŸ¤– ì—ì´ì „íŠ¸ ì±„íŒ… - LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰
    
    ì¼ë°˜ RAGì™€ ë‹¤ë¥´ê²Œ ì—ì´ì „íŠ¸ê°€ ìƒí™©ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:
    - search_sop_documents: ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰
    - get_document_references: ë¬¸ì„œ ê°„ ì°¸ì¡° ê´€ê³„
    - search_sections_by_keyword: í‚¤ì›Œë“œë¡œ ì„¹ì…˜ ê²€ìƒ‰
    - get_document_structure: ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨
    - list_all_documents: ì „ì²´ ë¬¸ì„œ ëª©ë¡
    """
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    session_id = request.session_id or str(uuid.uuid4())
    
    print(f"\n{'='*50}")
    print(f"ğŸ¤– ì—ì´ì „íŠ¸ ì§ˆë¬¸: {request.message}")
    print(f"   ì„¸ì…˜: {session_id}")
    print(f"   ëª¨ë“œ: {'LangGraph' if request.use_langgraph else 'Simple'}")
    
    try:
        # ë„êµ¬ ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)
        init_agent_tools(vector_store, get_graph_store(), sql_store)
        
        # í†µí•©ëœ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = run_agent(
            query=request.message,
            session_id=session_id,
            model_name=request.llm_model,
            embedding_model=resolve_model_path(request.embedding_model)
        )
        
        reasoning = result.get("reasoning")
        answer = result.get("answer", "")

        # ë³¸ë¬¸(answer)ì´ ë¹„ì–´ìˆëŠ”ë° reasoningë§Œ ìˆëŠ” ê²½ìš° (í† í° í•œë„ ì´ˆê³¼ ë“±ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ)
        if not answer and reasoning:
            print("âš ï¸ ë³¸ë¬¸ì´ ì§ì ‘ì ìœ¼ë¡œ ìˆ˜ì‹ ë˜ì§€ ì•Šì•„ ì‚¬ê³  ê³¼ì •(Reasoning)ì„ ë‹µë³€ìœ¼ë¡œ ìµœìš°ì„  ë…¸ì¶œí•©ë‹ˆë‹¤.")
            result["answer"] = f"[AI ë¶„ì„ ë¦¬í¬íŠ¸]\n\n{reasoning}"
            answer = result["answer"]
        
        if reasoning:
            print(f"ğŸ§  ëª¨ë¸ì˜ ìƒê°(Reasoning) ì¶”ì¶œë¨ ({len(reasoning)}ì)")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì²« 100ì ì •ë„ ì¶œë ¥
            print(f"   [THINK] {reasoning[:150].replace('\n', ' ')}...")
        
        print(f"   ë„êµ¬ í˜¸ì¶œ: {len(result.get('tool_calls', []))}íšŒ")
        print(f"   ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))} ê¸€ì")
        print(f"{'='*50}\n")
        
        return {
            "session_id": session_id,
            "answer": result.get("answer", ""),
            "tool_calls": result.get("tool_calls", []),
            "success": result.get("success", False),
            "mode": "langgraph" if (request.use_langgraph and LANGGRAPH_AGENT_AVAILABLE) else "simple"
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/agent/status")
def agent_status():
    """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    return {
        "agent_available": AGENT_AVAILABLE,
        "langchain_available": LANGCHAIN_AVAILABLE if AGENT_AVAILABLE else False,
        "langgraph_agent_available": LANGGRAPH_AGENT_AVAILABLE if AGENT_AVAILABLE else False,
        "tools": [t.name for t in AGENT_TOOLS] if AGENT_AVAILABLE else [],
        "message": "ì—ì´ì „íŠ¸ ì‚¬ìš© ê°€ëŠ¥" if AGENT_AVAILABLE else "ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"
    }


@app.get("/agent/tools")
def agent_tools():
    """ì—ì´ì „íŠ¸ ë„êµ¬ ëª©ë¡"""
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    tools_info = []
    for tool in AGENT_TOOLS:
        tools_info.append({
            "name": tool.name,
            "description": tool.description
        })
    
    return {"tools": tools_info, "count": len(tools_info)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    sql_store.init_db()
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì‹œì‘")
    
    import uvicorn
    
    print("\n" + "=" * 60)
    print("ğŸ¤– RAG Chatbot API v11.0 + Z.AI Agent")
    print("=" * 60)
    print(f"ğŸ”¥ LLM ë°±ì—”ë“œ: {'âœ… Z.AI (GLM-4.7-Flash)' if ZaiLLM.is_available() else 'âŒ ZAI_API_KEY ì„¤ì • í•„ìš”'}")
    print(f"ğŸ¤– ì—ì´ì „íŠ¸: {'âœ… í™œì„±í™”' if AGENT_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    
    if AGENT_AVAILABLE:
        print(f"   - LangChain: {'âœ…' if LANGCHAIN_AVAILABLE else 'âŒ'}")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("ì£¼ìš” ê¸°ëŠ¥:")
    print("  - LangGraph ë¬¸ì„œ íŒŒì´í”„ë¼ì¸")
    print("  - ğŸ¤– ReAct ì—ì´ì „íŠ¸ (/agent/chat)")
    print("  - ChromaDB + Neo4j + PostgreSQL")
    print("  - LangSmith ì¶”ì  ì§€ì›")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)


if __name__ == "__main__":
    main()