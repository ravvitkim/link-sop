"""
RAG ì±—ë´‡ API v9.0

ğŸ”¥ LangGraph ìƒíƒœ ë¨¸ì‹  ê¸°ë°˜ íŒŒì´í”„ë¼ì¸:
- ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬
- ë³€í™˜ ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ  
- í’ˆì§ˆ ê²€ì¦ ë° ìë™ ë³´ì •
- ì¡°ê±´ë¶€ ì¬ì²˜ë¦¬

ë…¸ë“œ íë¦„:
Load â†’ Convert â†’ Validate â†’ Split â†’ Optimize â†’ Finalize
         â†“           â†“
      Fallback    Repair
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import torch
import time
import uuid

# RAG ëª¨ë“ˆ
from rag import (
    load_document,
    get_supported_extensions,
    create_chunks,
    create_chunks_from_blocks,
    get_available_methods,
    CHUNK_METHODS,
)
from rag.document_processor import process_document, convert_to_markdown  # ğŸ”¥ v8.0 ìƒˆ íŒŒì´í”„ë¼ì¸
from rag.document_pipeline import process_document as process_document_v9, state_to_chunks  # ğŸ”¥ v9.0 LangGraph
from rag import vector_store
from rag.prompt import build_rag_prompt, build_chunk_prompt
from rag.llm import (
    get_llm_response,
    OllamaLLM,
    analyze_search_results,
    generate_clarification_question,
    OLLAMA_MODELS,
    HUGGINGFACE_MODELS,
)


app = FastAPI(title="RAG Chatbot API", version="9.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 200
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 3  # ê¸°ë³¸ ì°¸ê³  ë¬¸ì„œ ìˆ˜
DEFAULT_SIMILARITY_THRESHOLD = 0.35

PRESET_MODELS = {
    "ko-sroberta": "jhgan/ko-sroberta-multitask",
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5-large": "intfloat/multilingual-e5-large",
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
    "bge-m3": "BAAI/bge-m3",
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
}

device = "cuda" if torch.cuda.is_available() else "cpu"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë©”ëª¨ë¦¬)
chat_histories: Dict[str, List[Dict]] = {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None
    include_sources: bool = True


class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    check_clarification: bool = True
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model_key: str) -> str:
    return PRESET_MODELS.get(model_key, model_key)


def format_metadata_display(metadata: Dict) -> Dict:
    display = {}
    if metadata.get('doc_name'):
        display['doc_name'] = metadata['doc_name']
    if metadata.get('doc_title'):
        display['doc_title'] = metadata['doc_title']
    if metadata.get('sop_id'):
        display['sop_id'] = metadata['sop_id']
    if metadata.get('version'):
        display['version'] = f"v{metadata['version']}"
    if metadata.get('section_path'):
        display['section_path'] = metadata['section_path']
    if metadata.get('section_path_readable'):
        display['section_path_readable'] = metadata['section_path_readable']
    if metadata.get('section'):
        display['section'] = metadata['section']
    elif metadata.get('article_num'):
        article_num = metadata['article_num']
        article_type = metadata.get('article_type', 'article')
        if article_type == 'article':
            display['section'] = f"ì œ{article_num}ì¡°"
        elif article_type == 'chapter':
            display['section'] = f"ì œ{article_num}ì¥"
        else:
            display['section'] = str(article_num)
    if metadata.get('title') and metadata.get('title') != metadata.get('doc_title'):
        display['title'] = metadata['title']
    return display


def build_chat_context(history: List[Dict], max_turns: int = 5) -> str:
    recent = history[-max_turns:] if len(history) > max_turns else history
    context_parts = []
    for turn in recent:
        if turn['role'] == 'user':
            context_parts.append(f"ì‚¬ìš©ì: {turn['content']}")
        else:
            context_parts.append(f"AI: {turn['content'][:200]}...")
    return "\n".join(context_parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG Chatbot API v6.2",
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "ollama_available": OllamaLLM.is_available(),
        "features": [
            "section_path ê³„ì¸µ ì¶”ì ", 
            "intro ë¸”ë¡ RAG ì œì™¸",      # ğŸ”¥ v6.3
            "doc_title SOP ID ê¸°ë°˜",    # ğŸ”¥ v6.3
            "ì±—ë´‡ ëŒ€í™” íˆìŠ¤í† ë¦¬", 
            "similarity_threshold ê²€ìƒ‰ í•„í„°ë§"
        ],
    }


@app.get("/health")
def health():
    return {"status": "ok", "device": device}


@app.get("/models/embedding")
def get_embedding_models():
    return vector_store.get_embedding_model_info()


@app.get("/models/llm")
def get_llm_models():
    ollama_running = OllamaLLM.is_available()
    ollama_models = OllamaLLM.list_models() if ollama_running else []
    return {
        "ollama": {"server_running": ollama_running, "available_models": ollama_models, "models": [{**m, "available": m["key"] in ollama_models} for m in OLLAMA_MODELS]},
        "huggingface": {"models": HUGGINGFACE_MODELS},
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ì—…ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/debug-parse")
async def debug_parse_document(file: UploadFile = File(...)):
    """ğŸ”¥ ë””ë²„ê¹…ìš©: ë¬¸ì„œ íŒŒì‹± ê²°ê³¼ í™•ì¸"""
    content = await file.read()
    filename = file.filename
    
    parsed_doc = load_document(filename, content)
    
    # í…ìŠ¤íŠ¸ì—ì„œ ì¡°í•­ íŒ¨í„´ ì°¾ê¸°
    import re
    text = parsed_doc.text
    
    # ê° íŒ¨í„´ë³„ ë§¤ì¹­ í™•ì¸
    pattern_matches = {}
    test_patterns = [
        (r'^(\d+\.\d+\.\d+)\s+', 'subsubsection'),
        (r'^(\d+\.\d+)\s+', 'subsection'),
        (r'^(\d+)\.\s+', 'section'),
        (r'^ì œ\s*(\d+)\s*ì¡°', 'article'),
        (r'^ì œ\s*(\d+)\s*ì¥', 'chapter'),
    ]
    
    for pattern, name in test_patterns:
        matches = re.findall(pattern, text, re.MULTILINE)
        pattern_matches[name] = matches[:10]  # ì²˜ìŒ 10ê°œë§Œ
    
    # ë¸”ë¡ ì •ë³´
    blocks_info = []
    for i, block in enumerate(parsed_doc.blocks):
        blocks_info.append({
            "index": i,
            "article_type": block.metadata.get("article_type"),
            "article_num": block.metadata.get("article_num"),
            "section_path": block.metadata.get("section_path"),
            "title": block.metadata.get("title"),
            "text_preview": block.text[:100] + "..." if len(block.text) > 100 else block.text,
        })
    
    # í…ìŠ¤íŠ¸ ì²˜ìŒ 2000ì
    text_preview = text[:2000]
    
    return {
        "filename": filename,
        "total_blocks": len(parsed_doc.blocks),
        "pattern_matches": pattern_matches,
        "blocks": blocks_info[:20],  # ì²˜ìŒ 20ê°œ ë¸”ë¡
        "text_preview": text_preview,
    }


@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    exclude_intro: bool = Form(True),
    use_langgraph: bool = Form(True),  # ğŸ”¥ v9.0: LangGraph ì‚¬ìš© ì—¬ë¶€
):
    start_time = time.time()
    try:
        content = await file.read()
        filename = file.filename
        
        # ğŸ”¥ v9.0: LangGraph ìƒíƒœ ë¨¸ì‹  íŒŒì´í”„ë¼ì¸
        if use_langgraph:
            result = process_document_v9(
                filename, 
                content, 
                chunk_size=chunk_size, 
                chunk_overlap=overlap,
                debug=True
            )
            
            # LangGraph ê²°ê³¼ì—ì„œ ì²­í¬ ì¶”ì¶œ
            chunks = state_to_chunks(result)
            metadata = result.get("metadata", {})
            quality_score = result.get("quality_score", 0)
            conversion_method = result.get("conversion_method", "unknown")
            warnings = result.get("warnings", [])
            
        else:
            # v8.0 íŒŒì´í”„ë¼ì¸ (í´ë°±)
            result = process_document(
                filename, 
                content, 
                chunk_size=chunk_size, 
                chunk_overlap=overlap,
                debug=True
            )
            chunks = result.chunks
            metadata = result.metadata
            quality_score = 1.0
            conversion_method = "v8-linear"
            warnings = []
        
        # ëª©ì°¨/intro ì œì™¸ (ì„ íƒì )
        if exclude_intro:
            chunks = [c for c in chunks if c.metadata.get('section_path')]
        
        # ğŸ”¥ ë¹ˆ ì²­í¬ ì²´í¬
        if not chunks:
            print(f"âš ï¸ ì²­í¬ê°€ 0ê°œ! í•„í„° ì—†ì´ ì¬ì‹œë„")
            chunks = state_to_chunks(result) if use_langgraph else result.chunks
            
            if not chunks:
                raise HTTPException(400, "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"   ìµœì¢… ì²­í¬ ìˆ˜: {len(chunks)}")
        
        model_path = resolve_model_path(model)
        texts = [c.text for c in chunks]
        metadatas = [{**c.metadata, "chunk_method": chunk_method, "model": model} for c in chunks]
        
        vector_store.add_documents(texts=texts, metadatas=metadatas, collection_name=collection, model_name=model_path)
        
        # ğŸ”¥ Neo4j ê·¸ë˜í”„ì—ë„ ìë™ ì—…ë¡œë“œ
        graph_uploaded = False
        try:
            from rag.graph_store import document_to_graph, Neo4jGraphStore
            from rag.document_loader import load_document as load_doc_legacy
            
            parsed_doc_legacy = load_doc_legacy(filename, content)
            
            graph = get_graph_store()
            if graph.test_connection():
                document_to_graph(graph, parsed_doc_legacy, metadata.get("sop_id"))
                graph_uploaded = True
                print(f"   âœ… Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as graph_error:
            print(f"   âš ï¸ Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {graph_error}")
        
        return {
            "success": True,
            "filename": filename,
            "doc_title": metadata.get("sop_id") or filename,
            "sop_id": metadata.get("sop_id"),
            "chunks": len(chunks),
            "chunk_method": chunk_method,
            "elapsed_seconds": round(time.time() - start_time, 2),
            "sample_metadata": metadatas[0] if metadatas else {},
            "graph_uploaded": graph_uploaded,
            # ğŸ”¥ v9.0 ì¶”ê°€ ì •ë³´
            "pipeline_version": "v9.0-langgraph" if use_langgraph else "v8.0-linear",
            "quality_score": quality_score,
            "conversion_method": conversion_method,
            "warnings": warnings,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.post("/rag/preview-markdown")
async def preview_markdown(file: UploadFile = File(...)):
    """
    ë¬¸ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•´ì„œ ë¯¸ë¦¬ë³´ê¸°
    """
    try:
        content = await file.read()
        filename = file.filename
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        markdown_text, metadata = convert_to_markdown(filename, content)
        
        return {
            "success": True,
            "filename": filename,
            "markdown": markdown_text,
            "metadata": metadata,
            "preview_url": f"data:text/markdown;base64,{markdown_text.encode('utf-8').decode('latin-1')}"  # ê°„ë‹¨í•œ base64 ì¸ì½”ë”© (ì‹¤ì œë¡œëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”)
        }
    except Exception as e:
        raise HTTPException(500, f"ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/search")
def search_documents(request: SearchRequest):
    model_path = resolve_model_path(request.model)
    results = vector_store.search(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    for r in results:
        r["metadata_display"] = format_metadata_display(r.get("metadata", {}))
    return {"query": request.query, "results": results, "count": len(results)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì±—ë´‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/chat")
def chat(request: ChatRequest):
    session_id = request.session_id or str(uuid.uuid4())
    if session_id not in chat_histories:
        chat_histories[session_id] = []
    history = chat_histories[session_id]
    history.append({"role": "user", "content": request.message})
    
    model_path = resolve_model_path(request.embedding_model)
    results, context = vector_store.search_with_context(
        query=request.message,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    
    if not results:
        answer = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        history.append({"role": "assistant", "content": answer})
        return {"session_id": session_id, "message": request.message, "answer": answer, "sources": []}
    
    chat_context = build_chat_context(history[:-1])
    
    prompt = f"""ë‹¹ì‹ ì€ ê·œì •(SOP) ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ì™€ [ëŒ€í™” ê¸°ë¡]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§€ì¹¨:
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: 5.1.1 í•­ëª©)ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”.
- ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'í•´ë‹¹ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_context if chat_context else "(ì—†ìŒ)"}

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{request.message}

[ì±—ë´‡ ë‹µë³€]:"""
    
    try:
        answer = get_llm_response(prompt=prompt, llm_model=request.llm_model, llm_backend=request.llm_backend, max_tokens=512)
    except Exception as e:
        answer = f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    history.append({"role": "assistant", "content": answer})
    if len(history) > 40:
        chat_histories[session_id] = history[-40:]
    
    response = {"session_id": session_id, "message": request.message, "answer": answer}
    if request.include_sources:
        response["sources"] = [
            {
                "text": r.get("text", "")[:300] + "..." if len(r.get("text", "")) > 300 else r.get("text", ""),
                "similarity": r.get("similarity", 0),
                "metadata": r.get("metadata", {}),
                "metadata_display": format_metadata_display(r.get("metadata", {})),
            }
            for r in results
        ]
    return response


@app.get("/chat/history/{session_id}")
def get_chat_history(session_id: str):
    if session_id not in chat_histories:
        return {"session_id": session_id, "history": []}
    return {"session_id": session_id, "history": chat_histories[session_id]}


@app.delete("/chat/history/{session_id}")
def clear_chat_history(session_id: str):
    if session_id in chat_histories:
        del chat_histories[session_id]
    return {"success": True, "session_id": session_id}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - RAG ë‹µë³€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/ask")
def ask_with_rag(request: AskRequest):
    model_path = resolve_model_path(request.embedding_model)
    results, context = vector_store.search_with_context(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    
    if not results:
        return {"query": request.query, "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "sources": [], "needs_clarification": False}
    
    if request.check_clarification and not request.filter_doc:
        analysis = analyze_search_results(results)
        if analysis['needs_clarification']:
            clarification_text = generate_clarification_question(query=request.query, options=analysis['options'], llm_model=request.llm_model, llm_backend=request.llm_backend)
            return {
                "query": request.query,
                "answer": clarification_text,
                "needs_clarification": True,
                "clarification_options": analysis['options'],
                "sources": [{**r, "metadata_display": format_metadata_display(r.get("metadata", {}))} for r in results],
            }
    
    prompt = build_rag_prompt(request.query, context, language="ko")
    try:
        answer = get_llm_response(prompt=prompt, llm_model=request.llm_model, llm_backend=request.llm_backend, max_tokens=512)
    except Exception as e:
        answer = f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    return {
        "query": request.query,
        "answer": answer,
        "needs_clarification": False,
        "sources": [{**r, "metadata_display": format_metadata_display(r.get("metadata", {}))} for r in results],
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    return vector_store.delete_by_doc_name(doc_name=request.doc_name, collection_name=request.collection)


@app.get("/rag/collections")
def list_collections():
    collections = vector_store.list_collections()
    return {"collections": [vector_store.get_collection_info(name) for name in collections]}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - Neo4j ê·¸ë˜í”„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Neo4j ê·¸ë˜í”„ ì €ì¥ì†Œ (lazy ì´ˆê¸°í™”)
_graph_store = None

def get_graph_store():
    global _graph_store
    if _graph_store is None:
        from rag.graph_store import Neo4jGraphStore
        _graph_store = Neo4jGraphStore()
        _graph_store.connect()
    return _graph_store


@app.get("/graph/status")
def graph_status():
    """Neo4j ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        graph = get_graph_store()
        connected = graph.test_connection()
        stats = graph.get_graph_stats() if connected else {}
        return {
            "connected": connected,
            "stats": stats
        }
    except Exception as e:
        return {"connected": False, "error": str(e)}


@app.post("/graph/init")
def graph_init():
    """Neo4j ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
    try:
        graph = get_graph_store()
        graph.init_schema()
        return {"success": True, "message": "ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/clear")
def graph_clear():
    """Neo4j ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        graph = get_graph_store()
        graph.clear_all()
        return {"success": True, "message": "ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/graph/upload")
async def graph_upload_document(file: UploadFile = File(...)):
    """ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ë¡œ ì—…ë¡œë“œ"""
    try:
        from rag.graph_store import document_to_graph
        
        content = await file.read()
        filename = file.filename
        
        # ë¬¸ì„œ íŒŒì‹±
        parsed_doc = load_document(filename, content)
        sop_id = parsed_doc.metadata.get("sop_id")
        
        # ê·¸ë˜í”„ ë³€í™˜
        graph = get_graph_store()
        document_to_graph(graph, parsed_doc, sop_id)
        
        return {
            "success": True,
            "filename": filename,
            "sop_id": sop_id,
            "blocks": len(parsed_doc.blocks)
        }
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/documents")
def graph_list_documents():
    """Neo4jì˜ ëª¨ë“  ë¬¸ì„œ ëª©ë¡"""
    try:
        graph = get_graph_store()
        docs = graph.get_all_documents()
        return {"documents": docs, "count": len(docs)}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}")
def graph_get_document(sop_id: str):
    """íŠ¹ì • ë¬¸ì„œì˜ ìƒì„¸ ì •ë³´"""
    try:
        graph = get_graph_store()
        doc = graph.get_document(sop_id)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return doc
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/hierarchy")
def graph_get_hierarchy(sop_id: str):
    """ë¬¸ì„œì˜ ì„¹ì…˜ ê³„ì¸µ êµ¬ì¡°"""
    try:
        graph = get_graph_store()
        hierarchy = graph.get_section_hierarchy(sop_id)
        return {"sop_id": sop_id, "hierarchy": hierarchy}
    except Exception as e:
        raise HTTPException(500, f"ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/references")
def graph_get_references(sop_id: str):
    """ë¬¸ì„œì˜ ì°¸ì¡° ê´€ê³„"""
    try:
        graph = get_graph_store()
        refs = graph.get_document_references(sop_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return refs
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì°¸ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/sections")
def graph_search_sections(keyword: str, sop_id: str = None):
    """ì„¹ì…˜ ë‚´ìš© ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(keyword, sop_id)
        return {"keyword": keyword, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/terms")
def graph_search_terms(term: str):
    """ìš©ì–´ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_by_term(term)
        return {"term": term, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ìš©ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    print("\n" + "=" * 60)
    print("ğŸ¤– RAG Chatbot API v6.3 + Neo4j")
    print("=" * 60)
    if torch.cuda.is_available():
        print(f"âœ… CUDA: {torch.cuda.get_device_name(0)}")
    else:
        print("âŒ CUDA ë¶ˆê°€ - CPU ëª¨ë“œ")
    if OllamaLLM.is_available():
        models = OllamaLLM.list_models()
        print(f"âœ… Ollama: {len(models)}ê°œ ëª¨ë¸")
    else:
        print("âš ï¸ Ollama ë¯¸ì‹¤í–‰")
    print("=" * 60)
    print("URL: http://localhost:8000")
    print("Docs: http://localhost:8000/docs")
    print("Graph API: /graph/*")
    print("=" * 60)
    uvicorn.run(app, host="0.0.0.0", port=8000)
